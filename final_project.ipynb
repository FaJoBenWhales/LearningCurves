{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Learning Curves of Convolutional Neural Network on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tools as t\n",
    "import models as m\n",
    "import hyperband as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n"
     ]
    }
   ],
   "source": [
    "configs,lcs,Y = t.load_data(scale_configs = True)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Testing models (mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/200\n",
      "177/177 [==============================] - 0s 1ms/step - loss: 0.0570 - val_loss: 0.0263\n",
      "Epoch 2/200\n",
      "177/177 [==============================] - 0s 183us/step - loss: 0.0388 - val_loss: 0.0259\n",
      "Epoch 3/200\n",
      "177/177 [==============================] - 0s 296us/step - loss: 0.0384 - val_loss: 0.0260\n",
      "Epoch 4/200\n",
      "177/177 [==============================] - 0s 294us/step - loss: 0.0393 - val_loss: 0.0290\n",
      "Epoch 5/200\n",
      "177/177 [==============================] - 0s 289us/step - loss: 0.0373 - val_loss: 0.0248\n",
      "Epoch 6/200\n",
      "177/177 [==============================] - 0s 308us/step - loss: 0.0357 - val_loss: 0.0245\n",
      "Epoch 7/200\n",
      "177/177 [==============================] - 0s 407us/step - loss: 0.0359 - val_loss: 0.0267\n",
      "Epoch 8/200\n",
      "177/177 [==============================] - 0s 341us/step - loss: 0.0342 - val_loss: 0.0237\n",
      "Epoch 9/200\n",
      "177/177 [==============================] - 0s 389us/step - loss: 0.0344 - val_loss: 0.0235\n",
      "Epoch 10/200\n",
      "177/177 [==============================] - 0s 492us/step - loss: 0.0335 - val_loss: 0.0260\n",
      "Epoch 11/200\n",
      "177/177 [==============================] - 0s 315us/step - loss: 0.0329 - val_loss: 0.0226\n",
      "Epoch 12/200\n",
      "177/177 [==============================] - 0s 271us/step - loss: 0.0309 - val_loss: 0.0238\n",
      "Epoch 13/200\n",
      "177/177 [==============================] - 0s 387us/step - loss: 0.0314 - val_loss: 0.0220\n",
      "Epoch 14/200\n",
      "177/177 [==============================] - 0s 323us/step - loss: 0.0300 - val_loss: 0.0219\n",
      "Epoch 15/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0294 - val_loss: 0.0216\n",
      "Epoch 16/200\n",
      "177/177 [==============================] - 0s 314us/step - loss: 0.0286 - val_loss: 0.0211\n",
      "Epoch 17/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0277 - val_loss: 0.0212\n",
      "Epoch 18/200\n",
      "177/177 [==============================] - 0s 497us/step - loss: 0.0273 - val_loss: 0.0206\n",
      "Epoch 19/200\n",
      "177/177 [==============================] - 0s 405us/step - loss: 0.0266 - val_loss: 0.0199\n",
      "Epoch 20/200\n",
      "177/177 [==============================] - 0s 337us/step - loss: 0.0250 - val_loss: 0.0194\n",
      "Epoch 21/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0247 - val_loss: 0.0196\n",
      "Epoch 22/200\n",
      "177/177 [==============================] - 0s 269us/step - loss: 0.0240 - val_loss: 0.0186\n",
      "Epoch 23/200\n",
      "177/177 [==============================] - 0s 394us/step - loss: 0.0232 - val_loss: 0.0185\n",
      "Epoch 24/200\n",
      "177/177 [==============================] - 0s 282us/step - loss: 0.0226 - val_loss: 0.0184\n",
      "Epoch 25/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0222 - val_loss: 0.0176\n",
      "Epoch 26/200\n",
      "177/177 [==============================] - 0s 300us/step - loss: 0.0219 - val_loss: 0.0180\n",
      "Epoch 27/200\n",
      "177/177 [==============================] - 0s 326us/step - loss: 0.0197 - val_loss: 0.0166\n",
      "Epoch 28/200\n",
      "177/177 [==============================] - 0s 583us/step - loss: 0.0190 - val_loss: 0.0166\n",
      "Epoch 29/200\n",
      "177/177 [==============================] - 0s 254us/step - loss: 0.0187 - val_loss: 0.0173\n",
      "Epoch 30/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0194 - val_loss: 0.0167\n",
      "Epoch 31/200\n",
      "177/177 [==============================] - 0s 322us/step - loss: 0.0181 - val_loss: 0.0182\n",
      "Epoch 32/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0174 - val_loss: 0.0154\n",
      "Epoch 33/200\n",
      "177/177 [==============================] - 0s 376us/step - loss: 0.0172 - val_loss: 0.0154\n",
      "Epoch 34/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0164 - val_loss: 0.0154\n",
      "Epoch 35/200\n",
      "177/177 [==============================] - 0s 317us/step - loss: 0.0165 - val_loss: 0.0150\n",
      "Epoch 36/200\n",
      "177/177 [==============================] - 0s 265us/step - loss: 0.0165 - val_loss: 0.0159\n",
      "Epoch 37/200\n",
      "177/177 [==============================] - 0s 422us/step - loss: 0.0166 - val_loss: 0.0147\n",
      "Epoch 38/200\n",
      "177/177 [==============================] - 0s 380us/step - loss: 0.0145 - val_loss: 0.0153\n",
      "Epoch 39/200\n",
      "177/177 [==============================] - 0s 311us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 40/200\n",
      "177/177 [==============================] - 0s 363us/step - loss: 0.0130 - val_loss: 0.0145\n",
      "Epoch 41/200\n",
      "177/177 [==============================] - 0s 313us/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 42/200\n",
      "177/177 [==============================] - 0s 237us/step - loss: 0.0118 - val_loss: 0.0145\n",
      "Epoch 43/200\n",
      "177/177 [==============================] - 0s 406us/step - loss: 0.0140 - val_loss: 0.0133\n",
      "Epoch 44/200\n",
      "177/177 [==============================] - 0s 251us/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 45/200\n",
      "177/177 [==============================] - 0s 220us/step - loss: 0.0118 - val_loss: 0.0131\n",
      "Epoch 46/200\n",
      "177/177 [==============================] - 0s 427us/step - loss: 0.0105 - val_loss: 0.0153\n",
      "Epoch 47/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0109 - val_loss: 0.0145\n",
      "Epoch 48/200\n",
      "177/177 [==============================] - 0s 413us/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 49/200\n",
      "177/177 [==============================] - 0s 325us/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 50/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 51/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 53/200\n",
      "177/177 [==============================] - 0s 442us/step - loss: 0.0130 - val_loss: 0.0115\n",
      "Epoch 54/200\n",
      "177/177 [==============================] - 0s 309us/step - loss: 0.0090 - val_loss: 0.0122\n",
      "Epoch 55/200\n",
      "177/177 [==============================] - 0s 298us/step - loss: 0.0101 - val_loss: 0.0117\n",
      "Epoch 56/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 57/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 58/200\n",
      "177/177 [==============================] - 0s 395us/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 59/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0091 - val_loss: 0.0134\n",
      "Epoch 60/200\n",
      "177/177 [==============================] - 0s 443us/step - loss: 0.0097 - val_loss: 0.0122\n",
      "Epoch 61/200\n",
      "177/177 [==============================] - 0s 314us/step - loss: 0.0091 - val_loss: 0.0114\n",
      "Epoch 62/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 63/200\n",
      "177/177 [==============================] - 0s 190us/step - loss: 0.0084 - val_loss: 0.0108\n",
      "Epoch 64/200\n",
      "177/177 [==============================] - 0s 366us/step - loss: 0.0086 - val_loss: 0.0104\n",
      "Epoch 65/200\n",
      "177/177 [==============================] - 0s 330us/step - loss: 0.0086 - val_loss: 0.0128\n",
      "Epoch 66/200\n",
      "177/177 [==============================] - 0s 260us/step - loss: 0.0074 - val_loss: 0.0112\n",
      "Epoch 67/200\n",
      "177/177 [==============================] - 0s 358us/step - loss: 0.0085 - val_loss: 0.0111\n",
      "Epoch 68/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0086 - val_loss: 0.0104\n",
      "Epoch 69/200\n",
      "177/177 [==============================] - 0s 232us/step - loss: 0.0080 - val_loss: 0.0118\n",
      "Epoch 70/200\n",
      "177/177 [==============================] - 0s 363us/step - loss: 0.0075 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "177/177 [==============================] - 0s 274us/step - loss: 0.0085 - val_loss: 0.0142\n",
      "Epoch 72/200\n",
      "177/177 [==============================] - 0s 277us/step - loss: 0.0082 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0073 - val_loss: 0.0101\n",
      "Epoch 74/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.006 - 0s 324us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 75/200\n",
      "177/177 [==============================] - 0s 390us/step - loss: 0.0070 - val_loss: 0.0104\n",
      "Epoch 76/200\n",
      "177/177 [==============================] - 0s 308us/step - loss: 0.0070 - val_loss: 0.0115\n",
      "Epoch 77/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0077 - val_loss: 0.0094\n",
      "Epoch 78/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 79/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 80/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0070 - val_loss: 0.0094\n",
      "Epoch 81/200\n",
      "177/177 [==============================] - 0s 282us/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 82/200\n",
      "177/177 [==============================] - 0s 433us/step - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 83/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0059 - val_loss: 0.0099\n",
      "Epoch 84/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0061 - val_loss: 0.0096\n",
      "Epoch 85/200\n",
      "177/177 [==============================] - 0s 286us/step - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 86/200\n",
      "177/177 [==============================] - 0s 248us/step - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 87/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 88/200\n",
      "177/177 [==============================] - 0s 252us/step - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 89/200\n",
      "177/177 [==============================] - 0s 353us/step - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 90/200\n",
      "177/177 [==============================] - 0s 252us/step - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 91/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0052 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "177/177 [==============================] - 0s 289us/step - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 94/200\n",
      "177/177 [==============================] - 0s 329us/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 95/200\n",
      "177/177 [==============================] - 0s 303us/step - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 96/200\n",
      "177/177 [==============================] - 0s 357us/step - loss: 0.0054 - val_loss: 0.0087\n",
      "Epoch 97/200\n",
      "177/177 [==============================] - 0s 312us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 98/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 99/200\n",
      "177/177 [==============================] - 0s 307us/step - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 100/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0048 - val_loss: 0.0086\n",
      "Epoch 101/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.003 - 0s 411us/step - loss: 0.0050 - val_loss: 0.0091\n",
      "Epoch 102/200\n",
      "177/177 [==============================] - 0s 297us/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 103/200\n",
      "177/177 [==============================] - 0s 530us/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 104/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0047 - val_loss: 0.0092\n",
      "Epoch 105/200\n",
      "177/177 [==============================] - 0s 388us/step - loss: 0.0057 - val_loss: 0.0108\n",
      "Epoch 106/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 107/200\n",
      "177/177 [==============================] - 0s 325us/step - loss: 0.0048 - val_loss: 0.0102\n",
      "Epoch 108/200\n",
      "177/177 [==============================] - 0s 344us/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 109/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0048 - val_loss: 0.0084\n",
      "Epoch 110/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Epoch 111/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0044 - val_loss: 0.0086\n",
      "Epoch 112/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 113/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0042 - val_loss: 0.0084\n",
      "Epoch 114/200\n",
      "177/177 [==============================] - 0s 324us/step - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 115/200\n",
      "177/177 [==============================] - 0s 399us/step - loss: 0.0042 - val_loss: 0.0083\n",
      "Epoch 116/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0052 - val_loss: 0.0094\n",
      "Epoch 117/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0046 - val_loss: 0.0074\n",
      "Epoch 118/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0044 - val_loss: 0.0076\n",
      "Epoch 119/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0045 - val_loss: 0.0080\n",
      "Epoch 120/200\n",
      "177/177 [==============================] - 0s 360us/step - loss: 0.0042 - val_loss: 0.0092\n",
      "Epoch 121/200\n",
      "177/177 [==============================] - 0s 313us/step - loss: 0.0043 - val_loss: 0.0079\n",
      "Epoch 122/200\n",
      "177/177 [==============================] - 0s 240us/step - loss: 0.0045 - val_loss: 0.0082\n",
      "Epoch 123/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0039 - val_loss: 0.0089\n",
      "Epoch 124/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0044 - val_loss: 0.0078\n",
      "Epoch 125/200\n",
      "177/177 [==============================] - 0s 342us/step - loss: 0.0039 - val_loss: 0.0084\n",
      "Epoch 126/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0039 - val_loss: 0.0080\n",
      "Epoch 127/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.005 - 0s 394us/step - loss: 0.0042 - val_loss: 0.0104\n",
      "Epoch 128/200\n",
      "177/177 [==============================] - 0s 259us/step - loss: 0.0041 - val_loss: 0.0082\n",
      "Epoch 129/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0041 - val_loss: 0.0081\n",
      "Epoch 130/200\n",
      "177/177 [==============================] - 0s 225us/step - loss: 0.0041 - val_loss: 0.0077\n",
      "Epoch 131/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0041 - val_loss: 0.0080\n",
      "Epoch 132/200\n",
      "177/177 [==============================] - 0s 523us/step - loss: 0.0042 - val_loss: 0.0077\n",
      "Epoch 133/200\n",
      "177/177 [==============================] - 0s 231us/step - loss: 0.0042 - val_loss: 0.0087\n",
      "Epoch 134/200\n",
      "177/177 [==============================] - 0s 382us/step - loss: 0.0039 - val_loss: 0.0071\n",
      "Epoch 135/200\n",
      "177/177 [==============================] - 0s 263us/step - loss: 0.0040 - val_loss: 0.0078\n",
      "Epoch 136/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0040 - val_loss: 0.0075\n",
      "Epoch 137/200\n",
      "177/177 [==============================] - 0s 321us/step - loss: 0.0039 - val_loss: 0.0069\n",
      "Epoch 138/200\n",
      "177/177 [==============================] - 0s 323us/step - loss: 0.0036 - val_loss: 0.0072\n",
      "Epoch 139/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0036 - val_loss: 0.0089\n",
      "Epoch 140/200\n",
      "177/177 [==============================] - 0s 278us/step - loss: 0.0048 - val_loss: 0.0075\n",
      "Epoch 141/200\n",
      "177/177 [==============================] - 0s 366us/step - loss: 0.0035 - val_loss: 0.0072\n",
      "Epoch 142/200\n",
      "177/177 [==============================] - 0s 205us/step - loss: 0.0037 - val_loss: 0.0079\n",
      "Epoch 143/200\n",
      "177/177 [==============================] - 0s 467us/step - loss: 0.0037 - val_loss: 0.0083\n",
      "Epoch 144/200\n",
      "177/177 [==============================] - 0s 332us/step - loss: 0.0038 - val_loss: 0.0086\n",
      "Epoch 145/200\n",
      "177/177 [==============================] - 0s 326us/step - loss: 0.0035 - val_loss: 0.0076\n",
      "Epoch 146/200\n",
      "177/177 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0078\n",
      "Epoch 147/200\n",
      "177/177 [==============================] - 0s 195us/step - loss: 0.0046 - val_loss: 0.0076\n",
      "Epoch 148/200\n",
      "177/177 [==============================] - 0s 353us/step - loss: 0.0035 - val_loss: 0.0076\n",
      "Epoch 149/200\n",
      "177/177 [==============================] - 0s 266us/step - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 150/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0032 - val_loss: 0.0072\n",
      "Epoch 151/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0034 - val_loss: 0.0068\n",
      "Epoch 152/200\n",
      "177/177 [==============================] - 0s 352us/step - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 153/200\n",
      "177/177 [==============================] - 0s 295us/step - loss: 0.0042 - val_loss: 0.0069\n",
      "Epoch 154/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0031 - val_loss: 0.0071\n",
      "Epoch 155/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0035 - val_loss: 0.0082\n",
      "Epoch 156/200\n",
      "177/177 [==============================] - 0s 250us/step - loss: 0.0034 - val_loss: 0.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "177/177 [==============================] - 0s 419us/step - loss: 0.0038 - val_loss: 0.0068\n",
      "Epoch 158/200\n",
      "177/177 [==============================] - 0s 269us/step - loss: 0.0032 - val_loss: 0.0070\n",
      "Epoch 159/200\n",
      "177/177 [==============================] - 0s 428us/step - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 160/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0042 - val_loss: 0.0078\n",
      "Epoch 161/200\n",
      "177/177 [==============================] - 0s 348us/step - loss: 0.0036 - val_loss: 0.0068\n",
      "Epoch 162/200\n",
      "177/177 [==============================] - 0s 241us/step - loss: 0.0038 - val_loss: 0.0071\n",
      "Epoch 163/200\n",
      "177/177 [==============================] - 0s 394us/step - loss: 0.0032 - val_loss: 0.0078\n",
      "Epoch 164/200\n",
      "177/177 [==============================] - 0s 249us/step - loss: 0.0031 - val_loss: 0.0066\n",
      "Epoch 165/200\n",
      "177/177 [==============================] - 0s 356us/step - loss: 0.0030 - val_loss: 0.0065\n",
      "Epoch 166/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0032 - val_loss: 0.0081\n",
      "Epoch 167/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0029 - val_loss: 0.0076\n",
      "Epoch 168/200\n",
      "177/177 [==============================] - 0s 428us/step - loss: 0.0030 - val_loss: 0.0066\n",
      "Epoch 169/200\n",
      "177/177 [==============================] - 0s 218us/step - loss: 0.0032 - val_loss: 0.0067\n",
      "Epoch 170/200\n",
      "177/177 [==============================] - 0s 410us/step - loss: 0.0029 - val_loss: 0.0080\n",
      "Epoch 171/200\n",
      "177/177 [==============================] - 0s 277us/step - loss: 0.0031 - val_loss: 0.0064\n",
      "Epoch 172/200\n",
      "177/177 [==============================] - 0s 364us/step - loss: 0.0029 - val_loss: 0.0068\n",
      "Epoch 173/200\n",
      "177/177 [==============================] - 0s 296us/step - loss: 0.0028 - val_loss: 0.0067\n",
      "Epoch 174/200\n",
      "177/177 [==============================] - 0s 223us/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 175/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0037 - val_loss: 0.0071\n",
      "Epoch 176/200\n",
      "177/177 [==============================] - 0s 334us/step - loss: 0.0030 - val_loss: 0.0071\n",
      "Epoch 177/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0030 - val_loss: 0.0075\n",
      "Epoch 178/200\n",
      "177/177 [==============================] - 0s 375us/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 179/200\n",
      "177/177 [==============================] - 0s 463us/step - loss: 0.0027 - val_loss: 0.0071\n",
      "Epoch 180/200\n",
      "177/177 [==============================] - 0s 273us/step - loss: 0.0028 - val_loss: 0.0064\n",
      "Epoch 181/200\n",
      "177/177 [==============================] - 0s 390us/step - loss: 0.0027 - val_loss: 0.0078\n",
      "Epoch 182/200\n",
      "177/177 [==============================] - 0s 307us/step - loss: 0.0028 - val_loss: 0.0065\n",
      "Epoch 183/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 327us/step - loss: 0.0028 - val_loss: 0.0065\n",
      "Epoch 184/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0025 - val_loss: 0.0077\n",
      "Epoch 185/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 270us/step - loss: 0.0030 - val_loss: 0.0084\n",
      "Epoch 186/200\n",
      "177/177 [==============================] - 0s 389us/step - loss: 0.0030 - val_loss: 0.0064\n",
      "Epoch 187/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0028 - val_loss: 0.0063\n",
      "Epoch 188/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 189/200\n",
      "177/177 [==============================] - 0s 254us/step - loss: 0.0026 - val_loss: 0.0069\n",
      "Epoch 190/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 303us/step - loss: 0.0027 - val_loss: 0.0068\n",
      "Epoch 191/200\n",
      "177/177 [==============================] - 0s 257us/step - loss: 0.0026 - val_loss: 0.0118\n",
      "Epoch 192/200\n",
      "177/177 [==============================] - 0s 317us/step - loss: 0.0046 - val_loss: 0.0071\n",
      "Epoch 193/200\n",
      "177/177 [==============================] - 0s 406us/step - loss: 0.0029 - val_loss: 0.0071\n",
      "Epoch 194/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0027 - val_loss: 0.0065\n",
      "Epoch 195/200\n",
      "177/177 [==============================] - 0s 346us/step - loss: 0.0025 - val_loss: 0.0067\n",
      "Epoch 196/200\n",
      "177/177 [==============================] - 0s 322us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 197/200\n",
      "177/177 [==============================] - 0s 276us/step - loss: 0.0031 - val_loss: 0.0070\n",
      "Epoch 198/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0024 - val_loss: 0.0064\n",
      "Epoch 199/200\n",
      "177/177 [==============================] - 0s 309us/step - loss: 0.0026 - val_loss: 0.0070\n",
      "Epoch 200/200\n",
      "177/177 [==============================] - 0s 369us/step - loss: 0.0031 - val_loss: 0.0066\n",
      "65/65 [==============================] - 0s 123us/step\n",
      "mse:  0.00701043214649\n"
     ]
    }
   ],
   "source": [
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
    "model = m.mlp(cfg['lr'])\n",
    "m.train_mlp(model, configs, Y, cfg, split=177, epochs=500)\n",
    "mse = m.eval_mlp(model, configs, Y, split=177, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build lstm with input_dim: 1\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 3s 304ms/step - loss: 0.0598 - mean_squared_error: 0.0598 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 2s 170ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "evaluate lstm without consideration of configs\n",
      "65/65 [==============================] - 0s 779us/step\n",
      "mse:  0.0173839222855\n"
     ]
    }
   ],
   "source": [
    "model = m.lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, lcs, steps=(5,5), split=200, batch_size=20, epochs=20, mode = 'nextstep')\n",
    "m.train_lstm(model, lcs, steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, lcs, steps=5, split=200, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.3766 - mean_squared_error: 0.3766 - val_loss: 0.4930 - val_mean_squared_error: 0.4930\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 0.9218 - mean_squared_error: 0.9218 - val_loss: 0.2245 - val_mean_squared_error: 0.2245\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.2141 - mean_squared_error: 0.2141 - val_loss: 0.1223 - val_mean_squared_error: 0.1223\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 144ms/step - loss: 0.1177 - mean_squared_error: 0.1177 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 173ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 8.8576e-04 - val_mean_squared_error: 8.8576e-04\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 9.1999e-04 - mean_squared_error: 9.1999e-04 - val_loss: 6.1876e-04 - val_mean_squared_error: 6.1876e-04\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 4.5607e-04 - mean_squared_error: 4.5607e-04 - val_loss: 4.6909e-04 - val_mean_squared_error: 4.6909e-04\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 3.4069e-04 - mean_squared_error: 3.4069e-04 - val_loss: 4.7472e-04 - val_mean_squared_error: 4.7472e-04\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 3.0764e-04 - mean_squared_error: 3.0764e-04 - val_loss: 4.0949e-04 - val_mean_squared_error: 4.0949e-04\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 2.6481e-04 - mean_squared_error: 2.6481e-04 - val_loss: 3.9920e-04 - val_mean_squared_error: 3.9920e-04\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.6800e-04 - mean_squared_error: 2.6800e-04 - val_loss: 4.0419e-04 - val_mean_squared_error: 4.0419e-04\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.6282e-04 - mean_squared_error: 2.6282e-04 - val_loss: 3.9006e-04 - val_mean_squared_error: 3.9006e-04\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.5470e-04 - mean_squared_error: 2.5470e-04 - val_loss: 3.8480e-04 - val_mean_squared_error: 3.8480e-04\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.5354e-04 - mean_squared_error: 2.5354e-04 - val_loss: 3.8584e-04 - val_mean_squared_error: 3.8584e-04\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.5453e-04 - mean_squared_error: 2.5453e-04 - val_loss: 3.8151e-04 - val_mean_squared_error: 3.8151e-04\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.5168e-04 - mean_squared_error: 2.5168e-04 - val_loss: 3.7989e-04 - val_mean_squared_error: 3.7989e-04\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.4947e-04 - mean_squared_error: 2.4947e-04 - val_loss: 3.7850e-04 - val_mean_squared_error: 3.7850e-04\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.4922e-04 - mean_squared_error: 2.4922e-04 - val_loss: 3.7601e-04 - val_mean_squared_error: 3.7601e-04\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4832e-04 - mean_squared_error: 2.4832e-04 - val_loss: 3.7310e-04 - val_mean_squared_error: 3.7310e-04\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.4703e-04 - mean_squared_error: 2.4703e-04 - val_loss: 3.7014e-04 - val_mean_squared_error: 3.7014e-04\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.4595e-04 - mean_squared_error: 2.4595e-04 - val_loss: 3.6862e-04 - val_mean_squared_error: 3.6862e-04\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.4498e-04 - mean_squared_error: 2.4498e-04 - val_loss: 3.6712e-04 - val_mean_squared_error: 3.6712e-04\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4411e-04 - mean_squared_error: 2.4411e-04 - val_loss: 3.6492e-04 - val_mean_squared_error: 3.6492e-04\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4329e-04 - mean_squared_error: 2.4329e-04 - val_loss: 3.6287e-04 - val_mean_squared_error: 3.6287e-04\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4250e-04 - mean_squared_error: 2.4250e-04 - val_loss: 3.6116e-04 - val_mean_squared_error: 3.6116e-04\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.4170e-04 - mean_squared_error: 2.4170e-04 - val_loss: 3.5959e-04 - val_mean_squared_error: 3.5959e-04\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.4095e-04 - mean_squared_error: 2.4095e-04 - val_loss: 3.5789e-04 - val_mean_squared_error: 3.5789e-04\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 2.4029e-04 - mean_squared_error: 2.4029e-04 - val_loss: 3.5625e-04 - val_mean_squared_error: 3.5625e-04\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 2.3967e-04 - mean_squared_error: 2.3967e-04 - val_loss: 3.5478e-04 - val_mean_squared_error: 3.5478e-04\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3909e-04 - mean_squared_error: 2.3909e-04 - val_loss: 3.5337e-04 - val_mean_squared_error: 3.5337e-04\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.3854e-04 - mean_squared_error: 2.3854e-04 - val_loss: 3.5199e-04 - val_mean_squared_error: 3.5199e-04\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3802e-04 - mean_squared_error: 2.3802e-04 - val_loss: 3.5063e-04 - val_mean_squared_error: 3.5063e-04\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3755e-04 - mean_squared_error: 2.3755e-04 - val_loss: 3.4936e-04 - val_mean_squared_error: 3.4936e-04\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3713e-04 - mean_squared_error: 2.3713e-04 - val_loss: 3.4816e-04 - val_mean_squared_error: 3.4816e-04\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3674e-04 - mean_squared_error: 2.3674e-04 - val_loss: 3.4701e-04 - val_mean_squared_error: 3.4701e-04\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3640e-04 - mean_squared_error: 2.3640e-04 - val_loss: 3.4588e-04 - val_mean_squared_error: 3.4588e-04\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.3608e-04 - mean_squared_error: 2.3608e-04 - val_loss: 3.4477e-04 - val_mean_squared_error: 3.4477e-04\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3580e-04 - mean_squared_error: 2.3580e-04 - val_loss: 3.4369e-04 - val_mean_squared_error: 3.4369e-04\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3555e-04 - mean_squared_error: 2.3555e-04 - val_loss: 3.4261e-04 - val_mean_squared_error: 3.4261e-04\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3532e-04 - mean_squared_error: 2.3532e-04 - val_loss: 3.4152e-04 - val_mean_squared_error: 3.4152e-04\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 2.3512e-04 - mean_squared_error: 2.3512e-04 - val_loss: 3.4039e-04 - val_mean_squared_error: 3.4039e-04\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 2.3492e-04 - mean_squared_error: 2.3492e-04 - val_loss: 3.3919e-04 - val_mean_squared_error: 3.3919e-04\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 2.3473e-04 - mean_squared_error: 2.3473e-04 - val_loss: 3.3790e-04 - val_mean_squared_error: 3.3790e-04\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3454e-04 - mean_squared_error: 2.3454e-04 - val_loss: 3.3647e-04 - val_mean_squared_error: 3.3647e-04\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3432e-04 - mean_squared_error: 2.3432e-04 - val_loss: 3.3488e-04 - val_mean_squared_error: 3.3488e-04\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 2.3406e-04 - mean_squared_error: 2.3406e-04 - val_loss: 3.3309e-04 - val_mean_squared_error: 3.3309e-04\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.3374e-04 - mean_squared_error: 2.3374e-04 - val_loss: 3.3107e-04 - val_mean_squared_error: 3.3107e-04\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3336e-04 - mean_squared_error: 2.3336e-04 - val_loss: 3.2881e-04 - val_mean_squared_error: 3.2881e-04\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3292e-04 - mean_squared_error: 2.3292e-04 - val_loss: 3.2636e-04 - val_mean_squared_error: 3.2636e-04\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3245e-04 - mean_squared_error: 2.3245e-04 - val_loss: 3.2379e-04 - val_mean_squared_error: 3.2379e-04\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3205e-04 - mean_squared_error: 2.3205e-04 - val_loss: 3.2129e-04 - val_mean_squared_error: 3.2129e-04\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 2.3192e-04 - mean_squared_error: 2.3192e-04 - val_loss: 3.1920e-04 - val_mean_squared_error: 3.1920e-04\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 2.3248e-04 - mean_squared_error: 2.3248e-04 - val_loss: 3.1808e-04 - val_mean_squared_error: 3.1808e-04\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 2.3454e-04 - mean_squared_error: 2.3454e-04 - val_loss: 3.1893e-04 - val_mean_squared_error: 3.1893e-04\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 2.3954e-04 - mean_squared_error: 2.3954e-04 - val_loss: 3.2341e-04 - val_mean_squared_error: 3.2341e-04\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 2.5000e-04 - mean_squared_error: 2.5000e-04 - val_loss: 3.3438e-04 - val_mean_squared_error: 3.3438e-04\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 2.7009e-04 - mean_squared_error: 2.7009e-04 - val_loss: 3.5663e-04 - val_mean_squared_error: 3.5663e-04\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 3.0595e-04 - mean_squared_error: 3.0595e-04 - val_loss: 3.9742e-04 - val_mean_squared_error: 3.9742e-04\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 3.6446e-04 - mean_squared_error: 3.6446e-04 - val_loss: 4.6388e-04 - val_mean_squared_error: 4.6388e-04\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.4581e-04 - mean_squared_error: 4.4581e-04 - val_loss: 5.4863e-04 - val_mean_squared_error: 5.4863e-04\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 5.2531e-04 - mean_squared_error: 5.2531e-04 - val_loss: 5.9937e-04 - val_mean_squared_error: 5.9937e-04\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 5.4342e-04 - mean_squared_error: 5.4342e-04 - val_loss: 5.3956e-04 - val_mean_squared_error: 5.3956e-04\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.6155e-04 - mean_squared_error: 4.6155e-04 - val_loss: 4.0685e-04 - val_mean_squared_error: 4.0685e-04\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.3895e-04 - mean_squared_error: 3.3895e-04 - val_loss: 3.2785e-04 - val_mean_squared_error: 3.2785e-04\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.6445e-04 - mean_squared_error: 2.6445e-04 - val_loss: 3.1702e-04 - val_mean_squared_error: 3.1702e-04\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4456e-04 - mean_squared_error: 2.4456e-04 - val_loss: 3.2263e-04 - val_mean_squared_error: 3.2263e-04\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4326e-04 - mean_squared_error: 2.4326e-04 - val_loss: 3.2345e-04 - val_mean_squared_error: 3.2345e-04\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4217e-04 - mean_squared_error: 2.4217e-04 - val_loss: 3.2020e-04 - val_mean_squared_error: 3.2020e-04\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3920e-04 - mean_squared_error: 2.3920e-04 - val_loss: 3.1639e-04 - val_mean_squared_error: 3.1639e-04\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.3606e-04 - mean_squared_error: 2.3606e-04 - val_loss: 3.1358e-04 - val_mean_squared_error: 3.1358e-04\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3396e-04 - mean_squared_error: 2.3396e-04 - val_loss: 3.1189e-04 - val_mean_squared_error: 3.1189e-04\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.3313e-04 - mean_squared_error: 2.3313e-04 - val_loss: 3.1114e-04 - val_mean_squared_error: 3.1114e-04\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3335e-04 - mean_squared_error: 2.3335e-04 - val_loss: 3.1119e-04 - val_mean_squared_error: 3.1119e-04\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.3430e-04 - mean_squared_error: 2.3430e-04 - val_loss: 3.1198e-04 - val_mean_squared_error: 3.1198e-04\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3582e-04 - mean_squared_error: 2.3582e-04 - val_loss: 3.1344e-04 - val_mean_squared_error: 3.1344e-04\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3796e-04 - mean_squared_error: 2.3796e-04 - val_loss: 3.1562e-04 - val_mean_squared_error: 3.1562e-04\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4096e-04 - mean_squared_error: 2.4096e-04 - val_loss: 3.1871e-04 - val_mean_squared_error: 3.1871e-04\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.4529e-04 - mean_squared_error: 2.4529e-04 - val_loss: 3.2321e-04 - val_mean_squared_error: 3.2321e-04\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.5162e-04 - mean_squared_error: 2.5162e-04 - val_loss: 3.2983e-04 - val_mean_squared_error: 3.2983e-04\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.6090e-04 - mean_squared_error: 2.6090e-04 - val_loss: 3.3953e-04 - val_mean_squared_error: 3.3953e-04\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.7430e-04 - mean_squared_error: 2.7430e-04 - val_loss: 3.5348e-04 - val_mean_squared_error: 3.5348e-04\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9319e-04 - mean_squared_error: 2.9319e-04 - val_loss: 3.7291e-04 - val_mean_squared_error: 3.7291e-04\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.1870e-04 - mean_squared_error: 3.1870e-04 - val_loss: 3.9866e-04 - val_mean_squared_error: 3.9866e-04\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.5081e-04 - mean_squared_error: 3.5081e-04 - val_loss: 4.2978e-04 - val_mean_squared_error: 4.2978e-04\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 3.8647e-04 - mean_squared_error: 3.8647e-04 - val_loss: 4.6125e-04 - val_mean_squared_error: 4.6125e-04\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 4.1769e-04 - mean_squared_error: 4.1769e-04 - val_loss: 4.8225e-04 - val_mean_squared_error: 4.8225e-04\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.3208e-04 - mean_squared_error: 4.3208e-04 - val_loss: 4.7978e-04 - val_mean_squared_error: 4.7978e-04\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.1960e-04 - mean_squared_error: 4.1960e-04 - val_loss: 4.4979e-04 - val_mean_squared_error: 4.4979e-04\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 69ms/step - loss: 3.8228e-04 - mean_squared_error: 3.8228e-04 - val_loss: 4.0450e-04 - val_mean_squared_error: 4.0450e-04\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.3511e-04 - mean_squared_error: 3.3511e-04 - val_loss: 3.6290e-04 - val_mean_squared_error: 3.6290e-04\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9448e-04 - mean_squared_error: 2.9448e-04 - val_loss: 3.3539e-04 - val_mean_squared_error: 3.3539e-04\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 2.6762e-04 - mean_squared_error: 2.6762e-04 - val_loss: 3.2115e-04 - val_mean_squared_error: 3.2115e-04\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.5296e-04 - mean_squared_error: 2.5296e-04 - val_loss: 3.1496e-04 - val_mean_squared_error: 3.1496e-04\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.4593e-04 - mean_squared_error: 2.4593e-04 - val_loss: 3.1252e-04 - val_mean_squared_error: 3.1252e-04\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4285e-04 - mean_squared_error: 2.4285e-04 - val_loss: 3.1156e-04 - val_mean_squared_error: 3.1156e-04\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4166e-04 - mean_squared_error: 2.4166e-04 - val_loss: 3.1123e-04 - val_mean_squared_error: 3.1123e-04\n",
      "evaluate lstm with consideration of configs\n",
      "115/115 [==============================] - 0s 632us/step\n",
      "mse:  0.000311232009984\n"
     ]
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(20,20), split=150, batch_size=20, epochs=100, mode='nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=20, split=150, batch_size=20, mode='nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 0.00141, mse validation 0.00111\n"
     ]
    }
   ],
   "source": [
    "m.pred_finalpoints(model, [configs,lcs], steps=20, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm without consideration of configs\n",
      "88/88 [==============================] - 0s 482us/step\n",
      "mse:  0.00275807289555\n"
     ]
    }
   ],
   "source": [
    "mse = m.eval_lstm(model, lcs, Y, steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 7s 742ms/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.0959 - val_mean_squared_error: 0.0959\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 2s 171ms/step - loss: 0.2231 - mean_squared_error: 0.2231 - val_loss: 0.1004 - val_mean_squared_error: 0.1004\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.1406 - mean_squared_error: 0.1406 - val_loss: 0.1044 - val_mean_squared_error: 0.1044\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.2207 - mean_squared_error: 0.2207 - val_loss: 0.0289 - val_mean_squared_error: 0.0289\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 10/50\n",
      "1/9 [==>...........................] - ETA: 1s - loss: 0.0033 - mean_squared_error: 0.0033"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3026717f4502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lcs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"configs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m177\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m177\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL Theory/project/models.py\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, X, Y, steps, split, batch_size, epochs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_train_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL Theory/project/models.py\u001b[0m in \u001b[0;36m_train_lstm\u001b[0;34m(model, X, Y, steps, idx, batch_size, epochs, callbacks)\u001b[0m\n\u001b[1;32m    172\u001b[0m                                \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                                \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                                verbose = 1)\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = m.multi_lstm(lr=0.002)\n",
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,5), split=177, batch_size=20, epochs=50, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 304ms/step - loss: 0.1160 - mean_squared_error: 0.1160 - val_loss: 0.0853 - val_mean_squared_error: 0.0853\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.0558 - mean_squared_error: 0.0558 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 179ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0137 - val_mean_squared_error: 0.0137\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 250ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 0.0145 - mean_squared_error: 0.0145 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0146 - val_mean_squared_error: 0.0146\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 250ms/step - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 228ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 3s 278ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 2s 229ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 2s 168ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 2s 169ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 2s 168ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 2s 240ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0072 - val_mean_squared_error: 0.0072\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "9/9 [==============================] - 2s 231ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 3s 313ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 2s 230ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 2s 253ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 3s 300ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 2s 251ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 2s 265ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 2s 249ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 2s 248ms/step - loss: 9.7024e-04 - mean_squared_error: 9.7024e-04 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 2s 253ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 8.8618e-04 - mean_squared_error: 8.8618e-04 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 9.8973e-04 - mean_squared_error: 9.8973e-04 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 8.1752e-04 - mean_squared_error: 8.1752e-04 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 808us/step\n",
      "mse:  0.00515400678639\n"
     ]
    }
   ],
   "source": [
    "model = m.multi_lstm(lr=0.002)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# now using random length for timesteps considered steps = (0,x)\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,5), split=177, batch_size=20, epochs=100, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm with consideration of configs\n",
      "65/65 [==============================] - 0s 2ms/step\n",
      "mse:  0.000678403697048\n"
     ]
    }
   ],
   "source": [
    "mse = m.eval_lstm(model, [configs,lcs], Y, steps=20, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    evaluating models with cross validation (ridge, XGB, mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'alpha': 1.0}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.02977 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.03703 -0.02671 -0.02556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/jochen/Desktop/DL Theory/project/models.py:528: RuntimeWarning: Mean of empty slice.\n",
      "  fit_params=fit_params)\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "cfg={'alpha':1.0}\n",
    "results = m.eval_cv('ridge', configs, Y, cfg=cfg, splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'maxdepth': 10, 'subsample': 0.7946631901813815, 'cols_bt': 0.9376450587145334, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'n_estimators': 1000}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.00691 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.00898 -0.00403 -0.00771]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.9s finished\n",
      "/home/jochen/Desktop/DL Theory/project/models.py:540: RuntimeWarning: Mean of empty slice.\n",
      "  return results_train.mean(axis=0), results_val.mean(axis=0)\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "#cfg = {'maxdepth': 4, 'lr': 0.07120217610550672, 'gamma': 0.03393596760993278, 'cols_bt': 0.823494199726015, 'n_estimators': 107, 'subsample': 0.7288741544938715}\n",
    "results = m.eval_cv('xgb', configs, Y, cfg=cfg, splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'k_exp': 0.005043479631870928, 'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.04257, storing weights.\n",
      "\n",
      "Epoch 00002: loss improved from 0.04257 to 0.02757, storing weights.\n",
      "\n",
      "Epoch 00003: loss improved from 0.02757 to 0.02698, storing weights.\n",
      "\n",
      "Epoch 00004: loss improved from 0.02698 to 0.02657, storing weights.\n",
      "\n",
      "Epoch 00005: loss improved from 0.02657 to 0.02621, storing weights.\n",
      "\n",
      "Epoch 00006: loss improved from 0.02621 to 0.02462, storing weights.\n",
      "\n",
      "Epoch 00007: loss improved from 0.02462 to 0.02343, storing weights.\n",
      "\n",
      "Epoch 00008: loss improved from 0.02343 to 0.02246, storing weights.\n",
      "\n",
      "Epoch 00009: loss improved from 0.02246 to 0.02149, storing weights.\n",
      "\n",
      "Epoch 00010: loss improved from 0.02149 to 0.02035, storing weights.\n",
      "\n",
      "Epoch 00011: loss improved from 0.02035 to 0.01856, storing weights.\n",
      "\n",
      "Epoch 00012: loss improved from 0.01856 to 0.01792, storing weights.\n",
      "\n",
      "Epoch 00013: loss improved from 0.01792 to 0.01686, storing weights.\n",
      "\n",
      "Epoch 00014: loss improved from 0.01686 to 0.01578, storing weights.\n",
      "\n",
      "Epoch 00015: loss improved from 0.01578 to 0.01477, storing weights.\n",
      "\n",
      "Epoch 00016: loss is 0.01572, did not improve\n",
      "\n",
      "Epoch 00017: loss improved from 0.01477 to 0.01293, storing weights.\n",
      "\n",
      "Epoch 00018: loss improved from 0.01293 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00019: loss is 0.01237, did not improve\n",
      "\n",
      "Epoch 00020: loss improved from 0.01231 to 0.01094, storing weights.\n",
      "\n",
      "Epoch 00021: loss improved from 0.01094 to 0.00996, storing weights.\n",
      "\n",
      "Epoch 00022: loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00023: loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00024: loss improved from 0.00996 to 0.00875, storing weights.\n",
      "\n",
      "Epoch 00025: loss is 0.00935, did not improve\n",
      "\n",
      "Epoch 00026: loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00027: loss improved from 0.00875 to 0.00816, storing weights.\n",
      "\n",
      "Epoch 00028: loss improved from 0.00816 to 0.00794, storing weights.\n",
      "\n",
      "Epoch 00029: loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00030: loss improved from 0.00794 to 0.00757, storing weights.\n",
      "\n",
      "Epoch 00031: loss improved from 0.00757 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00032: loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00033: loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00034: loss improved from 0.00733 to 0.00729, storing weights.\n",
      "\n",
      "Epoch 00035: loss improved from 0.00729 to 0.00725, storing weights.\n",
      "\n",
      "Epoch 00036: loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00037: loss improved from 0.00725 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00038: loss improved from 0.00664 to 0.00635, storing weights.\n",
      "\n",
      "Epoch 00039: loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00040: loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00041: loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00042: loss improved from 0.00635 to 0.00611, storing weights.\n",
      "\n",
      "Epoch 00043: loss improved from 0.00611 to 0.00596, storing weights.\n",
      "\n",
      "Epoch 00044: loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00045: loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00046: loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00047: loss improved from 0.00596 to 0.00572, storing weights.\n",
      "\n",
      "Epoch 00048: loss improved from 0.00572 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00049: loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00050: loss improved from 0.00539 to 0.00522, storing weights.\n",
      "\n",
      "Epoch 00051: loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00052: loss improved from 0.00522 to 0.00514, storing weights.\n",
      "\n",
      "Epoch 00053: loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00054: loss improved from 0.00514 to 0.00513, storing weights.\n",
      "\n",
      "Epoch 00055: loss improved from 0.00513 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00056: loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00057: loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00058: loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00059: loss improved from 0.00488 to 0.00461, storing weights.\n",
      "\n",
      "Epoch 00060: loss is 0.00476, did not improve\n",
      "\n",
      "Epoch 00061: loss is 0.00480, did not improve\n",
      "\n",
      "Epoch 00062: loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00063: loss improved from 0.00461 to 0.00426, storing weights.\n",
      "\n",
      "Epoch 00064: loss is 0.00435, did not improve\n",
      "\n",
      "Epoch 00065: loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00066: loss is 0.00442, did not improve\n",
      "\n",
      "Epoch 00067: loss is 0.00452, did not improve\n",
      "\n",
      "Epoch 00068: loss improved from 0.00426 to 0.00408, storing weights.\n",
      "\n",
      "Epoch 00069: loss is 0.00458, did not improve\n",
      "\n",
      "Epoch 00070: loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00071: loss is 0.00424, did not improve\n",
      "\n",
      "Epoch 00072: loss improved from 0.00408 to 0.00402, storing weights.\n",
      "\n",
      "Epoch 00073: loss improved from 0.00402 to 0.00381, storing weights.\n",
      "\n",
      "Epoch 00074: loss is 0.00393, did not improve\n",
      "\n",
      "Epoch 00075: loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00076: loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00077: loss is 0.00385, did not improve\n",
      "\n",
      "Epoch 00078: loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00079: loss is 0.00387, did not improve\n",
      "\n",
      "Epoch 00080: loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00081: loss is 0.00434, did not improve\n",
      "\n",
      "Epoch 00082: loss improved from 0.00381 to 0.00377, storing weights.\n",
      "\n",
      "Epoch 00083: loss improved from 0.00377 to 0.00353, storing weights.\n",
      "\n",
      "Epoch 00084: loss is 0.00385, did not improve\n",
      "\n",
      "Epoch 00085: loss is 0.00367, did not improve\n",
      "\n",
      "Epoch 00086: loss is 0.00366, did not improve\n",
      "\n",
      "Epoch 00087: loss is 0.00406, did not improve\n",
      "\n",
      "Epoch 00088: loss improved from 0.00353 to 0.00348, storing weights.\n",
      "\n",
      "Epoch 00089: loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00090: loss improved from 0.00348 to 0.00345, storing weights.\n",
      "\n",
      "Epoch 00091: loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00092: loss improved from 0.00345 to 0.00341, storing weights.\n",
      "\n",
      "Epoch 00093: loss is 0.00364, did not improve\n",
      "\n",
      "Epoch 00094: loss improved from 0.00341 to 0.00324, storing weights.\n",
      "\n",
      "Epoch 00095: loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00096: loss is 0.00358, did not improve\n",
      "\n",
      "Epoch 00097: loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00098: loss improved from 0.00324 to 0.00317, storing weights.\n",
      "\n",
      "Epoch 00099: loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00100: loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00101: loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00102: loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00103: loss improved from 0.00317 to 0.00304, storing weights.\n",
      "\n",
      "Epoch 00104: loss is 0.00314, did not improve\n",
      "\n",
      "Epoch 00105: loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00106: loss is 0.00335, did not improve\n",
      "\n",
      "Epoch 00107: loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00108: loss is 0.00333, did not improve\n",
      "\n",
      "Epoch 00109: loss is 0.00352, did not improve\n",
      "\n",
      "Epoch 00110: loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00111: loss is 0.00320, did not improve\n",
      "\n",
      "Epoch 00112: loss improved from 0.00304 to 0.00302, storing weights.\n",
      "\n",
      "Epoch 00113: loss improved from 0.00302 to 0.00298, storing weights.\n",
      "\n",
      "Epoch 00114: loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00115: loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00116: loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00117: loss is 0.00300, did not improve\n",
      "\n",
      "Epoch 00118: loss improved from 0.00298 to 0.00269, storing weights.\n",
      "\n",
      "Epoch 00119: loss is 0.00373, did not improve\n",
      "\n",
      "Epoch 00120: loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00121: loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00122: loss is 0.00300, did not improve\n",
      "\n",
      "Epoch 00123: loss improved from 0.00269 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00124: loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00125: loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00126: loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00127: loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00128: loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00129: loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00130: loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00131: loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00132: loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00133: loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00134: loss improved from 0.00251 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00135: loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00136: loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00137: loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00138: loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00139: loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00140: loss improved from 0.00234 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00141: loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00142: loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00143: loss improved from 0.00227 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00144: loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00145: loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00146: loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00147: loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00148: loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00149: loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00150: loss improved from 0.00224 to 0.00203, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00151: loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00152: loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00153: loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00154: loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00155: loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00156: loss improved from 0.00203 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00157: loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00158: loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00159: loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00160: loss improved from 0.00200 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00161: loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00162: loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00163: loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00164: loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00165: loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00166: loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00167: loss improved from 0.00175 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00168: loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00169: loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00170: loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00171: loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00172: loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00173: loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00174: loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00175: loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00176: loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00177: loss improved from 0.00172 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00178: loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00179: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00180: loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00181: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00182: loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00183: loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00184: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00185: loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00186: loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00187: loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00188: loss improved from 0.00163 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00189: loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00190: loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00191: loss improved from 0.00161 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00192: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00193: loss improved from 0.00145 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00194: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00195: loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00196: loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00197: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00198: loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00199: loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00200: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00201: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00202: loss improved from 0.00141 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00203: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00204: loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00205: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00206: loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00207: loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00208: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00209: loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00210: loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00211: loss improved from 0.00134 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00212: loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00213: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00214: loss improved from 0.00127 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00215: loss improved from 0.00125 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00216: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00217: loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00218: loss improved from 0.00122 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00219: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00220: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00221: loss improved from 0.00117 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00222: loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00223: loss improved from 0.00111 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00224: loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00225: loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00226: loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00227: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00228: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00229: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00230: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00231: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00232: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00233: loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00234: loss improved from 0.00102 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00235: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00236: loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00237: loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00238: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00239: loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00240: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00241: loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00242: loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00243: loss improved from 0.00100 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00244: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00245: loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00246: loss improved from 0.00096 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00247: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00248: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00249: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00250: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00251: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00252: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00253: loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00254: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00255: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00256: loss improved from 0.00091 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00257: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00258: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00259: loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00260: loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00261: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00262: loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00263: loss improved from 0.00085 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00264: loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00265: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00266: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00267: loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00268: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00269: loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00270: loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00271: loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00272: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00273: loss improved from 0.00076 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00274: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00275: loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00276: loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00277: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00278: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00279: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00280: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00281: loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00282: loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00283: loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00284: loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00285: loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00286: loss improved from 0.00067 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00287: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00288: loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00289: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00290: loss improved from 0.00064 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00291: loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00292: loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00293: loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00294: loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00295: loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00296: loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00297: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00298: loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00299: loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00300: loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00301: loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00302: loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00303: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00304: loss improved from 0.00054 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00305: loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00306: loss improved from 0.00051 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00307: loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00308: loss is 0.00051, did not improve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00309: loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00310: loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00311: loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00312: loss improved from 0.00048 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00313: loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00314: loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00315: loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00316: loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00317: loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00318: loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00319: loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00320: loss improved from 0.00045 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00321: loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00322: loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00323: loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00324: loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00325: loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00326: loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00327: loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00328: loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00329: loss improved from 0.00042 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00330: loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00331: loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00332: loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00333: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00334: loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00335: loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00336: loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00337: loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00338: loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00339: loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00340: loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00341: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00342: loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00343: loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00344: loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00345: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00346: loss improved from 0.00039 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00347: loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00348: loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00349: loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00350: loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00351: loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00352: loss improved from 0.00037 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00353: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00354: loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00355: loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00356: loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00357: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00358: loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00359: loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00360: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00361: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00362: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00363: loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00364: loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00365: loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00366: loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00367: loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00368: loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00369: loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00370: loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00371: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00372: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00373: loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00374: loss improved from 0.00031 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00375: loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00376: loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00377: loss improved from 0.00029 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00378: loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00379: loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00380: loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00381: loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00382: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00383: loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00384: loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00385: loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00386: loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00387: loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00388: loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00389: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00390: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00391: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00392: loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00393: loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00394: loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00395: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00396: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00397: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00398: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00399: loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00400: loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00401: loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00402: loss improved from 0.00025 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00403: loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00404: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00405: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00406: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00407: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00408: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00409: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00410: loss improved from 0.00024 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00411: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00412: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00413: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00414: loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00415: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00416: loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00417: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00418: loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00419: loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00420: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00421: loss improved from 0.00023 to 0.00022, storing weights.\n",
      "\n",
      "Epoch 00422: loss improved from 0.00022 to 0.00021, storing weights.\n",
      "\n",
      "Epoch 00423: loss is 0.00022, did not improve\n",
      "\n",
      "Epoch 00424: loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00425: loss improved from 0.00021 to 0.00020, storing weights.\n",
      "\n",
      "Epoch 00426: loss improved from 0.00020 to 0.00019, storing weights.\n",
      "\n",
      "Epoch 00427: loss is 0.00022, did not improve\n",
      "\n",
      "Epoch 00428: loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00429: loss is 0.00021, did not improve\n",
      "\n",
      "Epoch 00430: loss is 0.00022, did not improve\n",
      "\n",
      "Epoch 00431: loss is 0.00021, did not improve\n",
      "\n",
      "Epoch 00432: loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00433: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00434: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00435: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00436: loss improved from 0.00019 to 0.00019, storing weights.\n",
      "\n",
      "Epoch 00437: loss is 0.00021, did not improve\n",
      "\n",
      "Epoch 00438: loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00439: loss is 0.00022, did not improve\n",
      "\n",
      "Epoch 00440: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00441: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00442: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00443: loss is 0.00020, did not improve\n",
      "\n",
      "Epoch 00444: loss improved from 0.00019 to 0.00019, storing weights.\n",
      "\n",
      "Epoch 00445: loss improved from 0.00019 to 0.00019, storing weights.\n",
      "\n",
      "Epoch 00446: loss is 0.00021, did not improve\n",
      "\n",
      "Epoch 00447: loss is 0.00022, did not improve\n",
      "\n",
      "Epoch 00448: loss improved from 0.00019 to 0.00018, storing weights.\n",
      "\n",
      "Epoch 00449: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00450: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00451: loss improved from 0.00018 to 0.00017, storing weights.\n",
      "\n",
      "Epoch 00452: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00453: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00454: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00455: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00456: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00457: loss improved from 0.00017 to 0.00017, storing weights.\n",
      "\n",
      "Epoch 00458: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00459: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00460: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00461: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00462: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00463: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00464: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00465: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00466: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00467: loss is 0.00019, did not improve\n",
      "\n",
      "Epoch 00468: loss improved from 0.00017 to 0.00017, storing weights.\n",
      "\n",
      "Epoch 00469: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00470: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00471: loss is 0.00017, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00472: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00473: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00474: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00475: loss improved from 0.00017 to 0.00016, storing weights.\n",
      "\n",
      "Epoch 00476: loss improved from 0.00016 to 0.00015, storing weights.\n",
      "\n",
      "Epoch 00477: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00478: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00479: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00480: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00481: loss improved from 0.00015 to 0.00015, storing weights.\n",
      "\n",
      "Epoch 00482: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00483: loss is 0.00017, did not improve\n",
      "\n",
      "Epoch 00484: loss is 0.00018, did not improve\n",
      "\n",
      "Epoch 00485: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00486: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00487: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00488: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00489: loss improved from 0.00015 to 0.00014, storing weights.\n",
      "\n",
      "Epoch 00490: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00491: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00492: loss is 0.00014, did not improve\n",
      "\n",
      "Epoch 00493: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00494: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00495: loss is 0.00015, did not improve\n",
      "\n",
      "Epoch 00496: loss is 0.00016, did not improve\n",
      "\n",
      "Epoch 00497: loss is 0.00015, did not improve\n",
      "Epoch 00497: early stopping\n",
      "Using epoch 00489 with loss: 0.00014\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.05820, storing weights.\n",
      "\n",
      "Epoch 00002: loss improved from 0.05820 to 0.03780, storing weights.\n",
      "\n",
      "Epoch 00003: loss improved from 0.03780 to 0.03653, storing weights.\n",
      "\n",
      "Epoch 00004: loss improved from 0.03653 to 0.03581, storing weights.\n",
      "\n",
      "Epoch 00005: loss improved from 0.03581 to 0.03572, storing weights.\n",
      "\n",
      "Epoch 00006: loss improved from 0.03572 to 0.03481, storing weights.\n",
      "\n",
      "Epoch 00007: loss improved from 0.03481 to 0.03383, storing weights.\n",
      "\n",
      "Epoch 00008: loss improved from 0.03383 to 0.03140, storing weights.\n",
      "\n",
      "Epoch 00009: loss improved from 0.03140 to 0.03013, storing weights.\n",
      "\n",
      "Epoch 00010: loss improved from 0.03013 to 0.02892, storing weights.\n",
      "\n",
      "Epoch 00011: loss improved from 0.02892 to 0.02576, storing weights.\n",
      "\n",
      "Epoch 00012: loss improved from 0.02576 to 0.02291, storing weights.\n",
      "\n",
      "Epoch 00013: loss improved from 0.02291 to 0.02148, storing weights.\n",
      "\n",
      "Epoch 00014: loss improved from 0.02148 to 0.01862, storing weights.\n",
      "\n",
      "Epoch 00015: loss improved from 0.01862 to 0.01734, storing weights.\n",
      "\n",
      "Epoch 00016: loss improved from 0.01734 to 0.01539, storing weights.\n",
      "\n",
      "Epoch 00017: loss improved from 0.01539 to 0.01445, storing weights.\n",
      "\n",
      "Epoch 00018: loss improved from 0.01445 to 0.01300, storing weights.\n",
      "\n",
      "Epoch 00019: loss improved from 0.01300 to 0.01216, storing weights.\n",
      "\n",
      "Epoch 00020: loss improved from 0.01216 to 0.01150, storing weights.\n",
      "\n",
      "Epoch 00021: loss improved from 0.01150 to 0.01075, storing weights.\n",
      "\n",
      "Epoch 00022: loss improved from 0.01075 to 0.01056, storing weights.\n",
      "\n",
      "Epoch 00023: loss improved from 0.01056 to 0.00987, storing weights.\n",
      "\n",
      "Epoch 00024: loss improved from 0.00987 to 0.00934, storing weights.\n",
      "\n",
      "Epoch 00025: loss improved from 0.00934 to 0.00867, storing weights.\n",
      "\n",
      "Epoch 00026: loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00027: loss improved from 0.00867 to 0.00859, storing weights.\n",
      "\n",
      "Epoch 00028: loss improved from 0.00859 to 0.00772, storing weights.\n",
      "\n",
      "Epoch 00029: loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00030: loss improved from 0.00772 to 0.00763, storing weights.\n",
      "\n",
      "Epoch 00031: loss improved from 0.00763 to 0.00740, storing weights.\n",
      "\n",
      "Epoch 00032: loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00033: loss improved from 0.00740 to 0.00723, storing weights.\n",
      "\n",
      "Epoch 00034: loss improved from 0.00723 to 0.00698, storing weights.\n",
      "\n",
      "Epoch 00035: loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00036: loss improved from 0.00698 to 0.00654, storing weights.\n",
      "\n",
      "Epoch 00037: loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00038: loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00039: loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00040: loss improved from 0.00654 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00041: loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00042: loss improved from 0.00606 to 0.00573, storing weights.\n",
      "\n",
      "Epoch 00043: loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00044: loss improved from 0.00573 to 0.00559, storing weights.\n",
      "\n",
      "Epoch 00045: loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00046: loss improved from 0.00559 to 0.00527, storing weights.\n",
      "\n",
      "Epoch 00047: loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00048: loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00049: loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00050: loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00051: loss improved from 0.00527 to 0.00523, storing weights.\n",
      "\n",
      "Epoch 00052: loss improved from 0.00523 to 0.00496, storing weights.\n",
      "\n",
      "Epoch 00053: loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00054: loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00055: loss improved from 0.00496 to 0.00474, storing weights.\n",
      "\n",
      "Epoch 00056: loss is 0.00497, did not improve\n",
      "\n",
      "Epoch 00057: loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00058: loss improved from 0.00474 to 0.00465, storing weights.\n",
      "\n",
      "Epoch 00059: loss is 0.00475, did not improve\n",
      "\n",
      "Epoch 00060: loss is 0.00472, did not improve\n",
      "\n",
      "Epoch 00061: loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00062: loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00063: loss improved from 0.00465 to 0.00436, storing weights.\n",
      "\n",
      "Epoch 00064: loss is 0.00457, did not improve\n",
      "\n",
      "Epoch 00065: loss is 0.00470, did not improve\n",
      "\n",
      "Epoch 00066: loss is 0.00443, did not improve\n",
      "\n",
      "Epoch 00067: loss is 0.00465, did not improve\n",
      "\n",
      "Epoch 00068: loss improved from 0.00436 to 0.00411, storing weights.\n",
      "\n",
      "Epoch 00069: loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00070: loss is 0.00443, did not improve\n",
      "\n",
      "Epoch 00071: loss improved from 0.00411 to 0.00385, storing weights.\n",
      "\n",
      "Epoch 00072: loss is 0.00427, did not improve\n",
      "\n",
      "Epoch 00073: loss improved from 0.00385 to 0.00383, storing weights.\n",
      "\n",
      "Epoch 00074: loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00075: loss is 0.00393, did not improve\n",
      "\n",
      "Epoch 00076: loss improved from 0.00383 to 0.00361, storing weights.\n",
      "\n",
      "Epoch 00077: loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00078: loss is 0.00375, did not improve\n",
      "\n",
      "Epoch 00079: loss is 0.00370, did not improve\n",
      "\n",
      "Epoch 00080: loss improved from 0.00361 to 0.00354, storing weights.\n",
      "\n",
      "Epoch 00081: loss is 0.00395, did not improve\n",
      "\n",
      "Epoch 00082: loss improved from 0.00354 to 0.00346, storing weights.\n",
      "\n",
      "Epoch 00083: loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00084: loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00085: loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00086: loss is 0.00376, did not improve\n",
      "\n",
      "Epoch 00087: loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00088: loss is 0.00346, did not improve\n",
      "\n",
      "Epoch 00089: loss is 0.00367, did not improve\n",
      "\n",
      "Epoch 00090: loss improved from 0.00346 to 0.00319, storing weights.\n",
      "\n",
      "Epoch 00091: loss is 0.00382, did not improve\n",
      "\n",
      "Epoch 00092: loss is 0.00352, did not improve\n",
      "\n",
      "Epoch 00093: loss improved from 0.00319 to 0.00319, storing weights.\n",
      "\n",
      "Epoch 00094: loss is 0.00327, did not improve\n",
      "\n",
      "Epoch 00095: loss is 0.00332, did not improve\n",
      "\n",
      "Epoch 00096: loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00097: loss is 0.00327, did not improve\n",
      "\n",
      "Epoch 00098: loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00099: loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00100: loss improved from 0.00319 to 0.00307, storing weights.\n",
      "\n",
      "Epoch 00101: loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00102: loss is 0.00308, did not improve\n",
      "\n",
      "Epoch 00103: loss improved from 0.00307 to 0.00295, storing weights.\n",
      "\n",
      "Epoch 00104: loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00105: loss improved from 0.00295 to 0.00279, storing weights.\n",
      "\n",
      "Epoch 00106: loss is 0.00288, did not improve\n",
      "\n",
      "Epoch 00107: loss improved from 0.00279 to 0.00271, storing weights.\n",
      "\n",
      "Epoch 00108: loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00109: loss is 0.00295, did not improve\n",
      "\n",
      "Epoch 00110: loss is 0.00314, did not improve\n",
      "\n",
      "Epoch 00111: loss is 0.00277, did not improve\n",
      "\n",
      "Epoch 00112: loss is 0.00279, did not improve\n",
      "\n",
      "Epoch 00113: loss is 0.00282, did not improve\n",
      "\n",
      "Epoch 00114: loss improved from 0.00271 to 0.00254, storing weights.\n",
      "\n",
      "Epoch 00115: loss is 0.00322, did not improve\n",
      "\n",
      "Epoch 00116: loss improved from 0.00254 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00117: loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00118: loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00119: loss is 0.00276, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00121: loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00122: loss improved from 0.00244 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00123: loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00124: loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00125: loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00126: loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00127: loss is 0.00283, did not improve\n",
      "\n",
      "Epoch 00128: loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00129: loss improved from 0.00235 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00130: loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00131: loss is 0.00263, did not improve\n",
      "\n",
      "Epoch 00132: loss improved from 0.00230 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00133: loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00134: loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00135: loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00136: loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00137: loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00138: loss improved from 0.00229 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00139: loss improved from 0.00225 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00140: loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00141: loss is 0.00293, did not improve\n",
      "\n",
      "Epoch 00142: loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00143: loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00144: loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00145: loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00146: loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00147: loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00148: loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00149: loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00150: loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00151: loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00152: loss improved from 0.00200 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00153: loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00154: loss improved from 0.00187 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00155: loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00156: loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00157: loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00158: loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00159: loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00160: loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00161: loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00162: loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00163: loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00164: loss improved from 0.00182 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00165: loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00166: loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00167: loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00168: loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00169: loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00170: loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00171: loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00172: loss improved from 0.00179 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00173: loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00174: loss improved from 0.00172 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00175: loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00176: loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00177: loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00178: loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00179: loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00180: loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00181: loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00182: loss improved from 0.00165 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00183: loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00184: loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00185: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00186: loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00187: loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00188: loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00189: loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00190: loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00191: loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00192: loss improved from 0.00157 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00193: loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00194: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00195: loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00196: loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00197: loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00198: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00199: loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00200: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00201: loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00202: loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00203: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00204: loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00205: loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00206: loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00207: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00208: loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00209: loss improved from 0.00147 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00210: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00211: loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00212: loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00213: loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00214: loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00215: loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00216: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00217: loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00218: loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00219: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00220: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00221: loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00222: loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00223: loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00224: loss improved from 0.00144 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00225: loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00226: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00227: loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00228: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00229: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00230: loss improved from 0.00139 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00231: loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00232: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00233: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00234: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00235: loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00236: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00237: loss improved from 0.00132 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00238: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00239: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00240: loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00241: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00242: loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00243: loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00244: loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00245: loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00246: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00247: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00248: loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00249: loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00250: loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00251: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00252: loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00253: loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00254: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00255: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00256: loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00257: loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00258: loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00259: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00260: loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00261: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00262: loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00263: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00264: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00265: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00266: loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00267: loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00268: loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00269: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00270: loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00271: loss improved from 0.00129 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00272: loss improved from 0.00125 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00273: loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00274: loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00275: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00276: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00277: loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00278: loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00279: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00280: loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00281: loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00282: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00283: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00284: loss improved from 0.00119 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00285: loss is 0.00119, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00286: loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00287: loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00288: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00289: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00290: loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00291: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00292: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00293: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00294: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00295: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00296: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00297: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00298: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00299: loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00300: loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00301: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00302: loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00303: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00304: loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00305: loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00306: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00307: loss improved from 0.00115 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00308: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00309: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00310: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00311: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00312: loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00313: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00314: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00315: loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00316: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00317: loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00318: loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00319: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00320: loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00321: loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00322: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00323: loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00324: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00325: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00326: loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00327: loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00328: loss improved from 0.00111 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00329: loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00330: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00331: loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00332: loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00333: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00334: loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00335: loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00336: loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00337: loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00338: loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00339: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00340: loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00341: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00342: loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00343: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00344: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00345: loss improved from 0.00108 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00346: loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00347: loss improved from 0.00108 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00348: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00349: loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00350: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00351: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00352: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00353: loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00354: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00355: loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00356: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00357: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00358: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00359: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00360: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00361: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00362: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00363: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00364: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00365: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00366: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00367: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00368: loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00369: loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00370: loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00371: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00372: loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00373: loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00374: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00375: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00376: loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00377: loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00378: loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00379: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00380: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00381: loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00382: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00383: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00384: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00385: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00386: loss improved from 0.00102 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00387: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00388: loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00389: loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00390: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00391: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00392: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00393: loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00394: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00395: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00396: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00397: loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00398: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00399: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00400: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00401: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00402: loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00403: loss is 0.00107, did not improve\n",
      "Epoch 00403: early stopping\n",
      "Using epoch 00386 with loss: 0.00099\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.05961, storing weights.\n",
      "\n",
      "Epoch 00002: loss improved from 0.05961 to 0.03734, storing weights.\n",
      "\n",
      "Epoch 00003: loss improved from 0.03734 to 0.03524, storing weights.\n",
      "\n",
      "Epoch 00004: loss improved from 0.03524 to 0.03297, storing weights.\n",
      "\n",
      "Epoch 00005: loss improved from 0.03297 to 0.03224, storing weights.\n",
      "\n",
      "Epoch 00006: loss improved from 0.03224 to 0.02881, storing weights.\n",
      "\n",
      "Epoch 00007: loss improved from 0.02881 to 0.02600, storing weights.\n",
      "\n",
      "Epoch 00008: loss improved from 0.02600 to 0.02305, storing weights.\n",
      "\n",
      "Epoch 00009: loss improved from 0.02305 to 0.02126, storing weights.\n",
      "\n",
      "Epoch 00010: loss improved from 0.02126 to 0.01910, storing weights.\n",
      "\n",
      "Epoch 00011: loss improved from 0.01910 to 0.01752, storing weights.\n",
      "\n",
      "Epoch 00012: loss improved from 0.01752 to 0.01602, storing weights.\n",
      "\n",
      "Epoch 00013: loss improved from 0.01602 to 0.01516, storing weights.\n",
      "\n",
      "Epoch 00014: loss improved from 0.01516 to 0.01407, storing weights.\n",
      "\n",
      "Epoch 00015: loss improved from 0.01407 to 0.01264, storing weights.\n",
      "\n",
      "Epoch 00016: loss improved from 0.01264 to 0.01177, storing weights.\n",
      "\n",
      "Epoch 00017: loss improved from 0.01177 to 0.01134, storing weights.\n",
      "\n",
      "Epoch 00018: loss improved from 0.01134 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00019: loss improved from 0.00993 to 0.00987, storing weights.\n",
      "\n",
      "Epoch 00020: loss improved from 0.00987 to 0.00931, storing weights.\n",
      "\n",
      "Epoch 00021: loss improved from 0.00931 to 0.00842, storing weights.\n",
      "\n",
      "Epoch 00022: loss improved from 0.00842 to 0.00829, storing weights.\n",
      "\n",
      "Epoch 00023: loss improved from 0.00829 to 0.00764, storing weights.\n",
      "\n",
      "Epoch 00024: loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00025: loss improved from 0.00764 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00026: loss improved from 0.00733 to 0.00730, storing weights.\n",
      "\n",
      "Epoch 00027: loss improved from 0.00730 to 0.00701, storing weights.\n",
      "\n",
      "Epoch 00028: loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00029: loss improved from 0.00701 to 0.00685, storing weights.\n",
      "\n",
      "Epoch 00030: loss improved from 0.00685 to 0.00638, storing weights.\n",
      "\n",
      "Epoch 00031: loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00032: loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00033: loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00034: loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00035: loss improved from 0.00638 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00036: loss improved from 0.00599 to 0.00540, storing weights.\n",
      "\n",
      "Epoch 00037: loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00038: loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00039: loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00040: loss is 0.00605, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00042: loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00043: loss improved from 0.00540 to 0.00499, storing weights.\n",
      "\n",
      "Epoch 00044: loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00045: loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00046: loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00047: loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00048: loss improved from 0.00499 to 0.00462, storing weights.\n",
      "\n",
      "Epoch 00049: loss improved from 0.00462 to 0.00456, storing weights.\n",
      "\n",
      "Epoch 00050: loss is 0.00475, did not improve\n",
      "\n",
      "Epoch 00051: loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00052: loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00053: loss improved from 0.00456 to 0.00438, storing weights.\n",
      "\n",
      "Epoch 00054: loss improved from 0.00438 to 0.00429, storing weights.\n",
      "\n",
      "Epoch 00055: loss is 0.00452, did not improve\n",
      "\n",
      "Epoch 00056: loss improved from 0.00429 to 0.00429, storing weights.\n",
      "\n",
      "Epoch 00057: loss improved from 0.00429 to 0.00404, storing weights.\n",
      "\n",
      "Epoch 00058: loss is 0.00429, did not improve\n",
      "\n",
      "Epoch 00059: loss is 0.00416, did not improve\n",
      "\n",
      "Epoch 00060: loss is 0.00444, did not improve\n",
      "\n",
      "Epoch 00061: loss is 0.00427, did not improve\n",
      "\n",
      "Epoch 00062: loss improved from 0.00404 to 0.00396, storing weights.\n",
      "\n",
      "Epoch 00063: loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00064: loss is 0.00411, did not improve\n",
      "\n",
      "Epoch 00065: loss is 0.00401, did not improve\n",
      "\n",
      "Epoch 00066: loss is 0.00397, did not improve\n",
      "\n",
      "Epoch 00067: loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00068: loss improved from 0.00396 to 0.00390, storing weights.\n",
      "\n",
      "Epoch 00069: loss is 0.00411, did not improve\n",
      "\n",
      "Epoch 00070: loss is 0.00408, did not improve\n",
      "\n",
      "Epoch 00071: loss improved from 0.00390 to 0.00376, storing weights.\n",
      "\n",
      "Epoch 00072: loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00073: loss improved from 0.00376 to 0.00374, storing weights.\n",
      "\n",
      "Epoch 00074: loss is 0.00388, did not improve\n",
      "\n",
      "Epoch 00075: loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00076: loss improved from 0.00374 to 0.00369, storing weights.\n",
      "\n",
      "Epoch 00077: loss improved from 0.00369 to 0.00357, storing weights.\n",
      "\n",
      "Epoch 00078: loss improved from 0.00357 to 0.00338, storing weights.\n",
      "\n",
      "Epoch 00079: loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00080: loss improved from 0.00338 to 0.00336, storing weights.\n",
      "\n",
      "Epoch 00081: loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00082: loss improved from 0.00336 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00083: loss is 0.00392, did not improve\n",
      "\n",
      "Epoch 00084: loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00085: loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00086: loss is 0.00332, did not improve\n",
      "\n",
      "Epoch 00087: loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00088: loss improved from 0.00323 to 0.00319, storing weights.\n",
      "\n",
      "Epoch 00089: loss is 0.00358, did not improve\n",
      "\n",
      "Epoch 00090: loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00091: loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00092: loss is 0.00332, did not improve\n",
      "\n",
      "Epoch 00093: loss is 0.00326, did not improve\n",
      "\n",
      "Epoch 00094: loss improved from 0.00319 to 0.00294, storing weights.\n",
      "\n",
      "Epoch 00095: loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00096: loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00097: loss improved from 0.00294 to 0.00277, storing weights.\n",
      "\n",
      "Epoch 00098: loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00099: loss is 0.00305, did not improve\n",
      "\n",
      "Epoch 00100: loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00101: loss improved from 0.00277 to 0.00271, storing weights.\n",
      "\n",
      "Epoch 00102: loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00103: loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00104: loss is 0.00303, did not improve\n",
      "\n",
      "Epoch 00105: loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00106: loss improved from 0.00271 to 0.00268, storing weights.\n",
      "\n",
      "Epoch 00107: loss is 0.00308, did not improve\n",
      "\n",
      "Epoch 00108: loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00109: loss improved from 0.00268 to 0.00265, storing weights.\n",
      "\n",
      "Epoch 00110: loss improved from 0.00265 to 0.00262, storing weights.\n",
      "\n",
      "Epoch 00111: loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00112: loss improved from 0.00262 to 0.00243, storing weights.\n",
      "\n",
      "Epoch 00113: loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00114: loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00115: loss is 0.00409, did not improve\n",
      "\n",
      "Epoch 00116: loss is 0.00279, did not improve\n",
      "\n",
      "Epoch 00117: loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00118: loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00119: loss improved from 0.00243 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00120: loss is 0.00263, did not improve\n",
      "\n",
      "Epoch 00121: loss improved from 0.00233 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00122: loss improved from 0.00228 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00123: loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00124: loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00125: loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00126: loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00127: loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00128: loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00129: loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00130: loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00131: loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00132: loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00133: loss improved from 0.00225 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00134: loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00135: loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00136: loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00137: loss improved from 0.00221 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00138: loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00139: loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00140: loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00141: loss improved from 0.00211 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00142: loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00143: loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00144: loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00145: loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00146: loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00147: loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00148: loss improved from 0.00198 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00149: loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00150: loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00151: loss improved from 0.00194 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00152: loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00153: loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00154: loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00155: loss improved from 0.00190 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00156: loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00157: loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00158: loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00159: loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00160: loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00161: loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00162: loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00163: loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00164: loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00165: loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00166: loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00167: loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00168: loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00169: loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00170: loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00171: loss improved from 0.00183 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00172: loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00173: loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00174: loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00175: loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00176: loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00177: loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00178: loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00179: loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00180: loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00181: loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00182: loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00183: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00184: loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00185: loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00186: loss improved from 0.00156 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00187: loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00188: loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00189: loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00190: loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00191: loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00192: loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00193: loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00194: loss improved from 0.00154 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00195: loss improved from 0.00150 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00196: loss is 0.00177, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00197: loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00198: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00199: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00200: loss improved from 0.00143 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00201: loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00202: loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00203: loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00204: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00205: loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00206: loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00207: loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00208: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00209: loss improved from 0.00138 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00210: loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00211: loss improved from 0.00136 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00212: loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00213: loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00214: loss improved from 0.00128 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00215: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00216: loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00217: loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00218: loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00219: loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00220: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00221: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00222: loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00223: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00224: loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00225: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00226: loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00227: loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00228: loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00229: loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00230: loss improved from 0.00125 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00231: loss improved from 0.00125 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00232: loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00233: loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00234: loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00235: loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00236: loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00237: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00238: loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00239: loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00240: loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00241: loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00242: loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00243: loss improved from 0.00123 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00244: loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00245: loss improved from 0.00119 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00246: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00247: loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00248: loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00249: loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00250: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00251: loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00252: loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00253: loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00254: loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00255: loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00256: loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00257: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00258: loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00259: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00260: loss improved from 0.00112 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00261: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00262: loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00263: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00264: loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00265: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00266: loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00267: loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00268: loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00269: loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00270: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00271: loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00272: loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00273: loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00274: loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00275: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00276: loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00277: loss improved from 0.00104 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00278: loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00279: loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00280: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00281: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00282: loss improved from 0.00098 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00283: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00284: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00285: loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00286: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00287: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00288: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00289: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00290: loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00291: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00292: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00293: loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00294: loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00295: loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00296: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00297: loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00298: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00299: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00300: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00301: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00302: loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00303: loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00304: loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00305: loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00306: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00307: loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00308: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00309: loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00310: loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00311: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00312: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00313: loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00314: loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00315: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00316: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00317: loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00318: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00319: loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00320: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00321: loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00322: loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00323: loss improved from 0.00092 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00324: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00325: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00326: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00327: loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00328: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00329: loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00330: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00331: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00332: loss improved from 0.00088 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00333: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00334: loss improved from 0.00086 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00335: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00336: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00337: loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00338: loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00339: loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00340: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00341: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00342: loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00343: loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00344: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00345: loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00346: loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00347: loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00348: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00349: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00350: loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00351: loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00352: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00353: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00354: loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00355: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00356: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00357: loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00358: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00359: loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00360: loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00361: loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00362: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00363: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00364: loss is 0.00081, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00365: loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00366: loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00367: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00368: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00369: loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00370: loss improved from 0.00078 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00371: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00372: loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00373: loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00374: loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00375: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00376: loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00377: loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00378: loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00379: loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00380: loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00381: loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00382: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00383: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00384: loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00385: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00386: loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00387: loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00388: loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00389: loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00390: loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00391: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00392: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00393: loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00394: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00395: loss improved from 0.00072 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00396: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00397: loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00398: loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00399: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00400: loss improved from 0.00067 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00401: loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00402: loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00403: loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00404: loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00405: loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00406: loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00407: loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00408: loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00409: loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00410: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00411: loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00412: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00413: loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00414: loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00415: loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00416: loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00417: loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00418: loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00419: loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00420: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00421: loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00422: loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00423: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00424: loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00425: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00426: loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00427: loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00428: loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00429: loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00430: loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00431: loss improved from 0.00064 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00432: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00433: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00434: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00435: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00436: loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00437: loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00438: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00439: loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00440: loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00441: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00442: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00443: loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00444: loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00445: loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00446: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00447: loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00448: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00449: loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00450: loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00451: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00452: loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00453: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00454: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00455: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00456: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00457: loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00458: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00459: loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00460: loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00461: loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00462: loss improved from 0.00060 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00463: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00464: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00465: loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00466: loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00467: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00468: loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00469: loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00470: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00471: loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00472: loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00473: loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00474: loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00475: loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00476: loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00477: loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00478: loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00479: loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00480: loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00481: loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00482: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00483: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00484: loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00485: loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00486: loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00487: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00488: loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00489: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00490: loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00491: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00492: loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00493: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00494: loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00495: loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00496: loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00497: loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00498: loss improved from 0.00055 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00499: loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00500: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00501: loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00502: loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00503: loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00504: loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00505: loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00506: loss is 0.00069, did not improve\n",
      "Epoch 00506: early stopping\n",
      "Using epoch 00498 with loss: 0.00053\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0064 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.00688 -0.00623 -0.00609]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min finished\n",
      "/home/jochen/Desktop/DL Theory/project/models.py:540: RuntimeWarning: Mean of empty slice.\n",
      "  return results_train.mean(axis=0), results_val.mean(axis=0)\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# evaluate via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928} \n",
    "results = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, \n",
    "                    dropout=False, lr_exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 500 epochs, train on 5 steps, validate on 5 steps\n",
      "config {'batch_size': 20}\n",
      "evaluating with early stopping\n",
      "build lstm with input_dim: 1\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "Epoch 1/500\n",
      "9/9 [==============================] - 1s 156ms/step - loss: 0.0599 - mean_squared_error: 0.0599 - val_loss: 0.0329 - val_mean_squared_error: 0.0329\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0141 - val_mean_squared_error: 0.0141\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0089 - val_mean_squared_error: 0.0089\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0116 - val_mean_squared_error: 0.0116\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0130 - val_mean_squared_error: 0.0130\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 54/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 107/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 155/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 159/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 211/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 213/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 214/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 215/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 216/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 217/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 218/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 219/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 220/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 221/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 222/500\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 223/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 224/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 225/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 226/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 227/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 228/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 229/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 230/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 231/500\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 232/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 233/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 234/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 235/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 236/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 237/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 238/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 239/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 240/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 241/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 242/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 243/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 244/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 245/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 246/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 247/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 248/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 249/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 250/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 251/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 252/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 253/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 254/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 255/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 256/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 257/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 258/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 259/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 260/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 261/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 262/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 263/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 264/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 265/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 266/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 267/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 268/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 269/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 270/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 271/500\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 272/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 273/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 274/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 275/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 276/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 277/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 278/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 279/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 280/500\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 281/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 282/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 283/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 284/500\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 285/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 286/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 287/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 288/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 289/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 290/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 291/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 292/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 293/500\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 294/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 295/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 296/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 297/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 298/500\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 299/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 300/500\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 301/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 302/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 303/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 304/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 305/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 306/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 307/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 308/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 309/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 310/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 311/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 312/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 313/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 314/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 315/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 316/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 317/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 318/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 319/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 320/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 321/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 322/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 323/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 324/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 325/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 326/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 327/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 328/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 329/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 330/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 331/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 332/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 333/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 334/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 335/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 336/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 337/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 338/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 339/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 340/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 341/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 342/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 343/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 344/500\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 345/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 346/500\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 347/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 348/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 349/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 350/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 351/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 352/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 353/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 355/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 356/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 357/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 358/500\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 359/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 360/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 361/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 362/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 363/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 364/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 365/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 366/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 367/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 368/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 369/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 370/500\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 371/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 372/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 373/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 374/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 375/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 376/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 377/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 378/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 379/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 380/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 381/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 382/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 383/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 384/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 385/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 386/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 387/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 388/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 389/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 390/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 391/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 392/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 393/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 394/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 395/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 396/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 397/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 398/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 399/500\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 400/500\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 401/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 402/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 403/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 404/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 405/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 406/500\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 407/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 408/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 409/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 410/500\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 411/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 412/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 413/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 414/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 415/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 416/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 417/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 00417: early stopping\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "Epoch 1/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0644 - mean_squared_error: 0.0644 - val_loss: 0.0333 - val_mean_squared_error: 0.0333\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0145 - val_mean_squared_error: 0.0145\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 54/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 61/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 107/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 112/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 155/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 159/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 203/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 211/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 213/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 214/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 215/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 216/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 217/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 218/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 219/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 220/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 221/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 222/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 223/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 224/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 225/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 226/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 227/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 228/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 229/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 230/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 231/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 232/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 233/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 234/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 235/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 236/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 237/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 238/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 239/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 241/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 242/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 243/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 244/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 245/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 246/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 247/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 248/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 249/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 250/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 251/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 252/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 253/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 254/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 255/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 256/500\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 257/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 258/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 259/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 260/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 261/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 262/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 263/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 264/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 265/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 266/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 267/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 268/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 269/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 270/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 271/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 272/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 273/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 274/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 275/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 276/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 277/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 278/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 279/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 280/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 281/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 282/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 283/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 284/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 285/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 286/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 287/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 288/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 289/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 290/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 291/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 292/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 293/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 294/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 295/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 296/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 297/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 298/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 299/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 300/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 301/500\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 302/500\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 303/500\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 304/500\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 305/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 306/500\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 307/500\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 308/500\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 309/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 310/500\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 311/500\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 312/500\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 313/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 314/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 315/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 316/500\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 317/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 318/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 319/500\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 320/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 321/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 322/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 323/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 324/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 325/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 326/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 327/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 328/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 329/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 330/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 331/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 332/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 333/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 334/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 335/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 336/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 337/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 338/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 339/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 340/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 342/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 343/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 344/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 345/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 346/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 347/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 348/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 349/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 350/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 351/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 352/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 353/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 354/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 355/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 356/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 357/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 358/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 359/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 360/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 361/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 362/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 363/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 364/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 365/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 366/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 367/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 368/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 369/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 370/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 371/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 372/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 373/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 374/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 375/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 376/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 377/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 378/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 379/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 380/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 381/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 382/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 383/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 384/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 385/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 386/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 387/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 388/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 389/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 390/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 391/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 392/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 393/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 394/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 395/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 396/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 397/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 398/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 399/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 400/500\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 401/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 402/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 403/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 404/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 405/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 406/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 407/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 408/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 409/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 410/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 411/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 412/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 413/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 414/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 415/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 416/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 417/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 418/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 419/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 420/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 421/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 422/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 423/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 424/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 425/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 426/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 427/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 428/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 429/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 430/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 431/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 432/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 433/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 434/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 435/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 436/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 437/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 438/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 439/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 440/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 441/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 443/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 444/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 445/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 446/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 447/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 448/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 449/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 450/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 451/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 452/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 453/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 454/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 455/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 456/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 457/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 458/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 459/500\n",
      "9/9 [==============================] - 0s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 460/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 461/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 462/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 463/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 464/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 465/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 466/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 467/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 468/500\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 469/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 470/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 471/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 472/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 473/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 474/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 475/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 476/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 477/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 478/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 479/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 480/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 481/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 482/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 00482: early stopping\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "Epoch 1/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 2/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 3/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 4/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 5/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 6/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 7/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 8/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 9/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 10/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 11/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 12/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 13/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 14/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 15/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 16/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 17/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 18/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 19/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 20/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 21/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 22/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 23/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 24/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 25/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 26/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 27/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 28/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 29/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 30/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 31/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 32/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 33/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 34/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 35/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 36/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 37/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 38/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 39/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 40/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 41/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 42/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 43/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 44/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 45/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 46/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 47/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 48/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 49/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 50/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 51/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 52/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 53/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 54/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 55/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 56/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 57/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 58/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 59/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 60/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 62/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 63/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 64/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 65/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 66/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 67/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 68/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 69/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 70/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 71/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 72/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 73/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 74/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 75/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 76/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 77/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 78/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 79/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 80/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 81/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 82/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 83/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 84/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 85/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 86/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 87/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 88/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 89/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 90/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 91/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 92/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 93/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 94/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 95/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 96/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 97/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 98/500\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 99/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 100/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 101/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 102/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 103/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 104/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 105/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 106/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 107/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 108/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 109/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 110/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 111/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 113/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 114/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 115/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 116/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 117/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 118/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 119/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 120/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 121/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 122/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 123/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 124/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 125/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 126/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 127/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 128/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 129/500\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 130/500\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 131/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 132/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 133/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 134/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 135/500\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 136/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 137/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 138/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 139/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 140/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 141/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 142/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 143/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 144/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 145/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 146/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 147/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 148/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 149/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 150/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 151/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 152/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 153/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 154/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 155/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 156/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 157/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 158/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 159/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 160/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 161/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 162/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 163/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 164/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 165/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 166/500\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 167/500\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 168/500\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 169/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 170/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 171/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 172/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 173/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 174/500\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 175/500\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 176/500\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 177/500\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 178/500\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 179/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 180/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 181/500\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 182/500\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 183/500\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 184/500\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 185/500\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 186/500\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 187/500\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 188/500\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 189/500\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 190/500\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 191/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 192/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 193/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 194/500\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 195/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 196/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 197/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 198/500\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 199/500\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 200/500\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 201/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 202/500\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 203/500\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 204/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 205/500\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 206/500\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 207/500\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 208/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 209/500\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 210/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 211/500\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 212/500\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/500\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 214/500\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 215/500\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 00215: early stopping\n",
      "MSE lstm: mean *** 0.00178 *** std: 0.0006\n",
      "Result of all Folds: [ 0.0016  0.0012  0.0026]\n"
     ]
    }
   ],
   "source": [
    "cfg = {'batch_size': 20}\n",
    "results = m.eval_cv('lstm', lcs, Y, steps=(5,5), cfg=cfg, epochs=500, splits=3, earlystop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15833, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15833 to 0.04731, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04731 to 0.03801, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03801 to 0.02834, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02834 to 0.01149, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01149 to 0.00788, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00788 to 0.00367, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00367 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00310, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00257 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00195 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00300, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00140 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00132 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00113 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00101 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00090 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00085 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00079 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00076 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00073 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00067 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00060 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00053 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00048 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00046 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00039, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00131: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "Epoch 00140: early stopping\n",
      "Using epoch 00103 with val_loss: 0.00039\n",
      "validate on 5 steps, mse on train / validation data: 1.39873 / 2.94549\n",
      "validate on 10 steps, mse on train / validation data: 1.29646 / 2.50751\n",
      "validate on 20 steps, mse on train / validation data: 0.91139 / 1.62842\n",
      "validate on 30 steps, mse on train / validation data: 0.42367 / 0.79404\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36051, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36051 to 0.71615, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71615 to 0.06664, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06664 to 0.05364, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05364 to 0.03628, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03628 to 0.03322, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03322 to 0.03315, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03315 to 0.02767, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.03008, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.03070, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02767 to 0.02729, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02845, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.02813, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.02743, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.02762, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02729 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02692 to 0.02647, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02647 to 0.02543, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02543 to 0.02297, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02297 to 0.01643, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01643 to 0.00435, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00435 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00200 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00160 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00293, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00084 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00064 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00052 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00050 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00046, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00047, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00082 with val_loss: 0.00040\n",
      "validate on 5 steps, mse on train / validation data: 0.33923 / 0.30867\n",
      "validate on 10 steps, mse on train / validation data: 0.29828 / 0.27770\n",
      "validate on 20 steps, mse on train / validation data: 0.24790 / 0.25507\n",
      "validate on 30 steps, mse on train / validation data: 0.15224 / 0.16362\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43205, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43205 to 0.08444, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08444 to 0.03833, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03833 to 0.01168, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02661, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01168 to 0.00909, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00909 to 0.00400, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00400 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00141 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00110 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00089 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00083 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00066, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00110: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00067, did not improve\n",
      "Epoch 00138: early stopping\n",
      "Using epoch 00097 with val_loss: 0.00066\n",
      "validate on 5 steps, mse on train / validation data: 0.05671 / 0.05211\n",
      "validate on 10 steps, mse on train / validation data: 0.05669 / 0.05225\n",
      "validate on 20 steps, mse on train / validation data: 0.08992 / 0.08317\n",
      "validate on 30 steps, mse on train / validation data: 0.07963 / 0.07053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 1.10209  0.94582  0.65555  0.34273] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.94549  2.50751  1.62842  0.79404]\n",
      " [ 0.30867  0.2777   0.25507  0.16362]\n",
      " [ 0.05211  0.05225  0.08317  0.07053]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.59822  0.55048  0.4164   0.21851] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.39873  1.29646  0.91139  0.42367]\n",
      " [ 0.33923  0.29828  0.2479   0.15224]\n",
      " [ 0.05671  0.05669  0.08992  0.07963]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28097, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28097 to 0.04951, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04951 to 0.03098, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03098 to 0.01735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01735 to 0.01043, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01043 to 0.00562, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00562 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00195 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00070 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00051 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00044 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00048, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00057, did not improve\n",
      "Epoch 00159: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00039\n",
      "validate on 5 steps, mse on train / validation data: 0.09498 / 0.14268\n",
      "validate on 10 steps, mse on train / validation data: 0.01112 / 0.00763\n",
      "validate on 20 steps, mse on train / validation data: 0.00481 / 0.00426\n",
      "validate on 30 steps, mse on train / validation data: 0.00179 / 0.00161\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53724, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 6.09075, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 1.76684, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53724 to 0.43818, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43818 to 0.16106, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16106 to 0.10837, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10837 to 0.07243, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07243 to 0.03493, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03493 to 0.03073, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03073 to 0.02940, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02940 to 0.02885, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02885 to 0.02763, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02763 to 0.02669, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.02681, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02669 to 0.02585, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02585 to 0.02527, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02527 to 0.02456, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02456 to 0.02411, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02411 to 0.02343, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02343 to 0.02284, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02284 to 0.02214, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02214 to 0.02141, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02141 to 0.02062, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02062 to 0.01983, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01983 to 0.01897, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01897 to 0.01805, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01805 to 0.01706, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01706 to 0.01600, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01600 to 0.01489, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01489 to 0.01372, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01372 to 0.01251, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01251 to 0.01126, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01126 to 0.01000, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01000 to 0.00873, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00873 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00747 to 0.00627, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00627 to 0.00515, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00515 to 0.00415, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00415 to 0.00331, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00331 to 0.00263, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00263 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00212 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00175 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00150 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00134 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00123 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00116 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00110 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00102 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00098 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00094 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00091 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00089 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00086 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00083 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00081 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00079 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00076 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00074 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00068 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00060 to 0.00058, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00057 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00053 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00058, did not improve\n",
      "Epoch 00157: early stopping\n",
      "Using epoch 00102 with val_loss: 0.00033\n",
      "validate on 5 steps, mse on train / validation data: 0.15210 / 0.15761\n",
      "validate on 10 steps, mse on train / validation data: 0.24021 / 0.22045\n",
      "validate on 20 steps, mse on train / validation data: 0.18364 / 0.17788\n",
      "validate on 30 steps, mse on train / validation data: 0.09263 / 0.09459\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08628, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08628 to 0.05569, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05569 to 0.04042, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04042 to 0.01699, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01699 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01185 to 0.00413, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00413 to 0.00336, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00336 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00162 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00114 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00102 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00100 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00063 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00031 to 0.00031, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00037, did not improve\n",
      "Epoch 00222: early stopping\n",
      "Using epoch 00168 with val_loss: 0.00026\n",
      "validate on 5 steps, mse on train / validation data: 0.08700 / 0.06098\n",
      "validate on 10 steps, mse on train / validation data: 0.08540 / 0.05817\n",
      "validate on 20 steps, mse on train / validation data: 0.12541 / 0.08756\n",
      "validate on 30 steps, mse on train / validation data: 0.10266 / 0.07260\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.12042  0.09541  0.0899   0.05627] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.14268  0.00763  0.00426  0.00161]\n",
      " [ 0.15761  0.22045  0.17788  0.09459]\n",
      " [ 0.06098  0.05817  0.08756  0.0726 ]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.11136  0.11224  0.10462  0.06569] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.09498  0.01112  0.00481  0.00179]\n",
      " [ 0.1521   0.24021  0.18364  0.09263]\n",
      " [ 0.087    0.0854   0.12541  0.10266]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06147, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06147 to 0.02912, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04456, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02912 to 0.00938, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02167, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00938 to 0.00417, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00417 to 0.00318, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00318 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00202 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00183 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00085 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00071 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00068 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00070, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00065 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00057 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00048 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00042 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00039 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00056, did not improve\n",
      "Epoch 00148: early stopping\n",
      "Using epoch 00099 with val_loss: 0.00037\n",
      "validate on 5 steps, mse on train / validation data: 0.00966 / 0.00962\n",
      "validate on 10 steps, mse on train / validation data: 0.00565 / 0.00540\n",
      "validate on 20 steps, mse on train / validation data: 0.00491 / 0.00422\n",
      "validate on 30 steps, mse on train / validation data: 0.00185 / 0.00175\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.38183, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 3.87232, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.38183 to 3.33221, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.33221 to 0.81293, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81293 to 0.47807, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.47807 to 0.22761, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22761 to 0.09574, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09574 to 0.05372, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05372 to 0.03384, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03384 to 0.03228, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03228 to 0.03124, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03124 to 0.02922, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02922 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.02795, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.02846, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02845, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.02814, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.02813, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02816, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02820, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02820, did not improve\n",
      "Epoch 00088: early stopping\n",
      "Using epoch 00013 with val_loss: 0.02754\n",
      "validate on 5 steps, mse on train / validation data: 0.02964 / 0.02665\n",
      "validate on 10 steps, mse on train / validation data: 0.02964 / 0.02665\n",
      "validate on 20 steps, mse on train / validation data: 0.02965 / 0.02666\n",
      "validate on 30 steps, mse on train / validation data: 0.02965 / 0.02666\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03272, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03272 to 0.03242, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03242 to 0.02405, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02405 to 0.01395, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01395 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00753 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00655 to 0.00349, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00349 to 0.00271, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00271 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00151 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00126 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00083 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00077 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00055 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00046 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00033, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00033, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00076 with val_loss: 0.00032\n",
      "validate on 5 steps, mse on train / validation data: 0.00925 / 0.00581\n",
      "validate on 10 steps, mse on train / validation data: 0.00231 / 0.00226\n",
      "validate on 20 steps, mse on train / validation data: 0.00168 / 0.00145\n",
      "validate on 30 steps, mse on train / validation data: 0.00082 / 0.00098\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01403  0.01144  0.01078  0.0098 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00962  0.0054   0.00422  0.00175]\n",
      " [ 0.02665  0.02665  0.02666  0.02666]\n",
      " [ 0.00581  0.00226  0.00145  0.00098]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01618  0.01253  0.01208  0.01077] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00966  0.00565  0.00491  0.00185]\n",
      " [ 0.02964  0.02964  0.02965  0.02965]\n",
      " [ 0.00925  0.00231  0.00168  0.00082]]\n",
      "results training data\n",
      " [[ 0.59822095  0.55047764  0.41640321  0.21851303]\n",
      " [ 0.11135717  0.11224402  0.10462055  0.0656942 ]\n",
      " [ 0.01618353  0.01253378  0.01207872  0.01077083]]\n",
      "results validation data \n",
      " [[ 1.10209298  0.94582258  0.65555483  0.3427303 ]\n",
      " [ 0.12042354  0.09541441  0.08990091  0.05626914]\n",
      " [ 0.01402978  0.01143511  0.01077778  0.00979835]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.2\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='nextstep')\n",
    "print(\"results training data\\n\", res_train)\n",
    "print(\"results validation data \\n\", res_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08584, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08584 to 0.07439, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07439 to 0.02369, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02369 to 0.01386, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01386 to 0.01289, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01289 to 0.00480, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00480 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00335 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00121 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00099 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00063 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00046 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00059, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00036, did not improve\n",
      "Epoch 00160: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.01471 / 0.02526\n",
      "validate on 10 steps, mse on train / validation data: 0.01031 / 0.01362\n",
      "validate on 20 steps, mse on train / validation data: 0.00436 / 0.00365\n",
      "validate on 30 steps, mse on train / validation data: 0.00166 / 0.00143\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60643, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60643 to 0.04722, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04722 to 0.04653, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.06111, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04653 to 0.03205, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03300, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03205 to 0.02693, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02693 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01068 to 0.00309, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00309 to 0.00285, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00285 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00163 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00095 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00060 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00055 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00041 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00035 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00026 to 0.00020, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00081 with val_loss: 0.00020\n",
      "validate on 5 steps, mse on train / validation data: 0.16159 / 0.08641\n",
      "validate on 10 steps, mse on train / validation data: 0.12393 / 0.07440\n",
      "validate on 20 steps, mse on train / validation data: 0.03269 / 0.02646\n",
      "validate on 30 steps, mse on train / validation data: 0.00476 / 0.00441\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11001, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11001 to 0.08327, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08327 to 0.03608, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03608 to 0.01065, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01065 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00554 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00247 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00202 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00197 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00120 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00058 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00043 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00031 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "Epoch 00119: early stopping\n",
      "Using epoch 00055 with val_loss: 0.00025\n",
      "validate on 5 steps, mse on train / validation data: 0.02207 / 0.03434\n",
      "validate on 10 steps, mse on train / validation data: 0.01448 / 0.02045\n",
      "validate on 20 steps, mse on train / validation data: 0.00771 / 0.00859\n",
      "validate on 30 steps, mse on train / validation data: 0.00305 / 0.00316\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04867  0.03615  0.0129   0.003  ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02526  0.01362  0.00365  0.00143]\n",
      " [ 0.08641  0.0744   0.02646  0.00441]\n",
      " [ 0.03434  0.02045  0.00859  0.00316]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.06613  0.04957  0.01492  0.00315] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01471  0.01031  0.00436  0.00166]\n",
      " [ 0.16159  0.12393  0.03269  0.00476]\n",
      " [ 0.02207  0.01448  0.00771  0.00305]]\n",
      "results training data\n",
      " [ 0.06612556  0.04957401  0.01491995  0.00315308]\n",
      "results validation data \n",
      " [ 0.04866848  0.03615473  0.01290235  0.00300086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train with random lenghts\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                     mode='nextstep')\n",
    "print(\"results training data\\n\", res_train)\n",
    "print(\"results validation data \\n\", res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11554, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11554 to 0.04302, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04302 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01769 to 0.01097, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01097 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00882 to 0.00847, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00847 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00606 to 0.00438, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00438 to 0.00315, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.01499, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.01350, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00410, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00315 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00251 to 0.00250, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00250 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00220 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00181 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00160 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00137 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00362, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00150, did not improve\n",
      "Epoch 00123: early stopping\n",
      "Using epoch 00049 with val_loss: 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 459us/step\n",
      "mse:  0.00131853502845\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 870us/step\n",
      "mse:  0.00133469417448\n",
      "validate on 5 steps, mse on train / validation data: 0.00133 / 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.001162388689\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00171085036444\n",
      "validate on 10 steps, mse on train / validation data: 0.00171 / 0.00116\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00321647753217\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00388603562235\n",
      "validate on 20 steps, mse on train / validation data: 0.00389 / 0.00322\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00459700725512\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.00526945944875\n",
      "validate on 30 steps, mse on train / validation data: 0.00527 / 0.00460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34632, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34632 to 0.06494, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06494 to 0.05340, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05340 to 0.02735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02735 to 0.01483, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.02008, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01483 to 0.01388, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01388 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01224 to 0.00833, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00833 to 0.00687, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00687 to 0.00294, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00460, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00359, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00294 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00487, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00441, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00466, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00401, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00226 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00220 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00216 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00214 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00211 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00211 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00209 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00206 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00201 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00197 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00193 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00176 to 0.00175, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00168 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00158 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00155 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00152 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00150 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00148 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00144 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00142 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00131 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00126, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00122 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00120 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00119 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.00118 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00116, did not improve\n",
      "Epoch 00328: early stopping\n",
      "Using epoch 00323 with val_loss: 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00115162648779\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 903us/step\n",
      "mse:  0.0012707624862\n",
      "validate on 5 steps, mse on train / validation data: 0.00127 / 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00158448308833\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0011413448456\n",
      "validate on 10 steps, mse on train / validation data: 0.00114 / 0.00158\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0039176084207\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.00348516953004\n",
      "validate on 20 steps, mse on train / validation data: 0.00349 / 0.00392\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0050399990075\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.00497433785639\n",
      "validate on 30 steps, mse on train / validation data: 0.00497 / 0.00504\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.99042, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.99042 to 1.47941, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 2.96802, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47941 to 0.63913, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63913 to 0.51863, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51863 to 0.08843, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08843 to 0.05703, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05703 to 0.04830, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04830 to 0.03768, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03768 to 0.03411, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03411 to 0.02496, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02528, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02496 to 0.02449, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02449 to 0.02441, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02461, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02456, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02441 to 0.02439, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02445, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02451, did not improve\n",
      "Epoch 00093: early stopping\n",
      "Using epoch 00018 with val_loss: 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 765us/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 776us/step\n",
      "mse:  0.0299257414868\n",
      "validate on 5 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 10 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 20 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 30 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00895  0.00904  0.01051  0.01134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00132  0.00116  0.00322  0.0046 ]\n",
      " [ 0.00115  0.00158  0.00392  0.00504]\n",
      " [ 0.02439  0.02439  0.02439  0.02439]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01084  0.01093  0.01243  0.01339] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00133  0.00171  0.00389  0.00527]\n",
      " [ 0.00127  0.00114  0.00349  0.00497]\n",
      " [ 0.02993  0.02993  0.02993  0.02993]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12057, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12057 to 0.06355, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06355 to 0.03759, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03759 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00790 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00766 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00323 to 0.00304, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00304 to 0.00301, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00301 to 0.00289, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00289 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00259 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00227 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00199 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00178 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00152 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00122 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00120 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00104 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00149, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00089 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00084, did not improve\n",
      "Epoch 00120: early stopping\n",
      "Using epoch 00088 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 481us/step\n",
      "mse:  0.0608066631418\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 855us/step\n",
      "mse:  0.0477681301365\n",
      "validate on 5 steps, mse on train / validation data: 0.04777 / 0.06081\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000824270867403\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000763803920894\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00128009282\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00143760890404\n",
      "validate on 20 steps, mse on train / validation data: 0.00144 / 0.00128\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00213678000401\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.0023780453167\n",
      "validate on 30 steps, mse on train / validation data: 0.00238 / 0.00214\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26239, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26239 to 0.07691, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.12018, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07691 to 0.05873, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05873 to 0.02621, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03345, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02621 to 0.02494, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02494 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01605 to 0.01107, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01107 to 0.00395, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00395 to 0.00253, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00253 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00169 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00079 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00064 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00065, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00068, did not improve\n",
      "Epoch 00106: early stopping\n",
      "Using epoch 00083 with val_loss: 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 590us/step\n",
      "mse:  0.0690140077336\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 744us/step\n",
      "mse:  0.0738325941319\n",
      "validate on 5 steps, mse on train / validation data: 0.07383 / 0.06901\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000604276981903\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000690422686459\n",
      "validate on 10 steps, mse on train / validation data: 0.00069 / 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.00140974406068\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0012300674403\n",
      "validate on 20 steps, mse on train / validation data: 0.00123 / 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00224876565732\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00229339740215\n",
      "validate on 30 steps, mse on train / validation data: 0.00229 / 0.00225\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12354, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12354 to 0.04591, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04597, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04591 to 0.02748, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02748 to 0.01963, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01963 to 0.01252, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01252 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00803 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00658 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00560 to 0.00375, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00375 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00194 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00174 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00172 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00153 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00150 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00140 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00110 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00108 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00104 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00095 to 0.00094, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "Epoch 00104: early stopping\n",
      "Using epoch 00104 with val_loss: 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 526us/step\n",
      "mse:  0.789464980364\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 710us/step\n",
      "mse:  0.750682481601\n",
      "validate on 5 steps, mse on train / validation data: 0.75068 / 0.78946\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000914607464272\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000559727040101\n",
      "validate on 10 steps, mse on train / validation data: 0.00056 / 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.980713968927\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.812348923104\n",
      "validate on 20 steps, mse on train / validation data: 0.81235 / 0.98071\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.750668579882\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "mse:  0.836846585664\n",
      "validate on 30 steps, mse on train / validation data: 0.83685 / 0.75067\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.30643  0.00078  0.3278   0.25168] ***\n",
      "Results validation data of all Folds: \n",
      "[[  6.08100000e-02   8.20000000e-04   1.28000000e-03   2.14000000e-03]\n",
      " [  6.90100000e-02   6.00000000e-04   1.41000000e-03   2.25000000e-03]\n",
      " [  7.89460000e-01   9.10000000e-04   9.80710000e-01   7.50670000e-01]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.29076  0.00067  0.27167  0.28051] ***\n",
      "Results training data of all Folds: \n",
      "[[  4.77700000e-02   7.60000000e-04   1.44000000e-03   2.38000000e-03]\n",
      " [  7.38300000e-02   6.90000000e-04   1.23000000e-03   2.29000000e-03]\n",
      " [  7.50680000e-01   5.60000000e-04   8.12350000e-01   8.36850000e-01]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14798, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14798 to 0.04405, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04405 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00993 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00817 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00592 to 0.00324, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00324 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00244 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00093 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00059 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00055 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00050 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "Epoch 00116: early stopping\n",
      "Using epoch 00041 with val_loss: 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 641us/step\n",
      "mse:  0.0258297447869\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 860us/step\n",
      "mse:  0.0263428334147\n",
      "validate on 5 steps, mse on train / validation data: 0.02634 / 0.02583\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.0107730397683\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00910780364013\n",
      "validate on 10 steps, mse on train / validation data: 0.00911 / 0.01077\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000469433154199\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000500161555961\n",
      "validate on 20 steps, mse on train / validation data: 0.00050 / 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.000675560233544\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.000807838148665\n",
      "validate on 30 steps, mse on train / validation data: 0.00081 / 0.00068\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17151 to 1.22318, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22318 to 0.66304, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.96452, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66304 to 0.11085, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11085 to 0.07196, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07196 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03687 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03687 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00837 to 0.00530, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00530 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00227 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00175 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00161 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00113 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00107 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00103 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00100 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00070 to 0.00069, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00060 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00051 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00051, did not improve\n",
      "Epoch 00223: early stopping\n",
      "Using epoch 00156 with val_loss: 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 665us/step\n",
      "mse:  0.0250337426974\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 799us/step\n",
      "mse:  0.0298722941786\n",
      "validate on 5 steps, mse on train / validation data: 0.02987 / 0.02503\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  4.91890081492\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  5.00050854009\n",
      "validate on 10 steps, mse on train / validation data: 5.00051 / 4.91890\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000351062770113\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000462018283239\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  3.78144966472\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  3.87956652129\n",
      "validate on 30 steps, mse on train / validation data: 3.87957 / 3.78145\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11390, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11390 to 0.05036, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05036 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01418, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01418 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00294, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00257 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00224 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00218 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00158 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00129 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00113 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00096 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00081 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.00073, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00212 with val_loss: 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 614us/step\n",
      "mse:  0.00897776255045\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 853us/step\n",
      "mse:  0.0123261121959\n",
      "validate on 5 steps, mse on train / validation data: 0.01233 / 0.00898\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00932959319008\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0085558105197\n",
      "validate on 10 steps, mse on train / validation data: 0.00856 / 0.00933\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000731091864344\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000461046307382\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000526615978742\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.000265021034481\n",
      "validate on 30 steps, mse on train / validation data: 0.00027 / 0.00053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [  1.99500000e-02   1.64633000e+00   5.20000000e-04   1.26088000e+00] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.58300000e-02   1.07700000e-02   4.70000000e-04   6.80000000e-04]\n",
      " [  2.50300000e-02   4.91890000e+00   3.50000000e-04   3.78145000e+00]\n",
      " [  8.98000000e-03   9.33000000e-03   7.30000000e-04   5.30000000e-04]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [  2.28500000e-02   1.67272000e+00   4.70000000e-04   1.29355000e+00] ***\n",
      "Results training data of all Folds: \n",
      "[[  2.63400000e-02   9.11000000e-03   5.00000000e-04   8.10000000e-04]\n",
      " [  2.98700000e-02   5.00051000e+00   4.60000000e-04   3.87957000e+00]\n",
      " [  1.23300000e-02   8.56000000e-03   4.60000000e-04   2.70000000e-04]]\n",
      "[[  1.08437327e-02   1.09259789e-02   1.24323155e-02   1.33898463e-02]\n",
      " [  2.90761069e-01   6.71317882e-04   2.71672200e-01   2.80506009e-01]\n",
      " [  2.28470799e-02   1.67272405e+00   4.74408716e-04   1.29354646e+00]]\n",
      "[[  8.95270427e-03   9.04494103e-03   1.05073458e-02   1.13416525e-02]\n",
      " [  3.06428550e-01   7.81051771e-04   3.27801269e-01   2.51684709e-01]\n",
      " [  1.99470833e-02   1.64633448e+00   5.17195930e-04   1.26088395e+00]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.3 base line training with fixed lenghts (on final epoch)\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='finalstep')\n",
    "print(\"results training data\\n\", res_train)\n",
    "print(\"results validation data \\n\", res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04549, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.05167, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04549 to 0.01942, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.02287, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01942 to 0.01751, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01751 to 0.01720, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01720 to 0.00615, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00615 to 0.00555, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00555 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00488 to 0.00413, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00413 to 0.00326, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00326 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00456, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00436, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00444, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00388, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00257 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00385, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00239 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00418, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00157 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00249, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00387, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00424, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00147 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00130 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00104 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00305, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00373, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.00263, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00394, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00082 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00070 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00346, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00238, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00063 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00374, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00057 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00090, did not improve\n",
      "Epoch 00248: early stopping\n",
      "Using epoch 00192 with val_loss: 0.00053\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 381us/step\n",
      "mse:  0.00152149519836\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 807us/step\n",
      "mse:  0.00137692030099\n",
      "validate on 5 steps, mse on train / validation data: 0.00138 / 0.00152\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000526376222613\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000751574570049\n",
      "validate on 10 steps, mse on train / validation data: 0.00075 / 0.00053\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000635943812412\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000685406969834\n",
      "validate on 20 steps, mse on train / validation data: 0.00069 / 0.00064\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00172576026238\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.00197781999437\n",
      "validate on 30 steps, mse on train / validation data: 0.00198 / 0.00173\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28546, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 12.84484, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 2.77282, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 2.25465, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 0.57671, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 1.12980, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.35501, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.28546 to 0.12109, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12109 to 0.03442, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.05426, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03442 to 0.02533, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02533 to 0.02353, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.02599, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02353 to 0.02270, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02270 to 0.02023, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02023 to 0.01940, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01940 to 0.01835, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01835 to 0.01375, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01375 to 0.00735, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00735 to 0.00401, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00403, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00401 to 0.00325, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00325 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00391, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00403, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00247 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00174 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00431, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00123 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00277, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00384, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00322, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00086 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00407, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00343, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00120, did not improve\n",
      "Epoch 00145: early stopping\n",
      "Using epoch 00070 with val_loss: 0.00068\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 667us/step\n",
      "mse:  0.00348511340351\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 869us/step\n",
      "mse:  0.00441753392839\n",
      "validate on 5 steps, mse on train / validation data: 0.00442 / 0.00349\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000761275335787\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.00105889618961\n",
      "validate on 10 steps, mse on train / validation data: 0.00106 / 0.00076\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000584286765926\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000738992010629\n",
      "validate on 20 steps, mse on train / validation data: 0.00074 / 0.00058\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000752295505001\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.0010405342926\n",
      "validate on 30 steps, mse on train / validation data: 0.00104 / 0.00075\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08260, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08260 to 0.03279, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03279 to 0.01579, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01579 to 0.00783, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00783 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00634 to 0.00532, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00532 to 0.00347, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 0.00347 to 0.00268, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00367, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00427, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00343, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00333, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00268 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00404, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00435, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00449, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00451, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00436, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00443, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00379, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00432, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00470, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00218 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00335, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00382, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00299, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00344, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00169 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00379, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00326, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00125 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00295, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00378, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00402, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00209, did not improve\n",
      "Epoch 00166: early stopping\n",
      "Using epoch 00118 with val_loss: 0.00122\n",
      "evaluate lstm with consideration of configs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 915us/step\n",
      "mse:  0.00519909668417\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 875us/step\n",
      "mse:  0.00466729941289\n",
      "validate on 5 steps, mse on train / validation data: 0.00467 / 0.00520\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00113231386852\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000758179737384\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00113\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000796099263508\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000717087945767\n",
      "validate on 20 steps, mse on train / validation data: 0.00072 / 0.00080\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00134987773014\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00113224787874\n",
      "validate on 30 steps, mse on train / validation data: 0.00113 / 0.00135\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0034   0.00081  0.00067  0.00128] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00152  0.00053  0.00064  0.00173]\n",
      " [ 0.00349  0.00076  0.00058  0.00075]\n",
      " [ 0.0052   0.00113  0.0008   0.00135]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00349  0.00086  0.00071  0.00138] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00138  0.00075  0.00069  0.00198]\n",
      " [ 0.00442  0.00106  0.00074  0.00104]\n",
      " [ 0.00467  0.00076  0.00072  0.00113]]\n",
      "[ 0.00348725  0.00085622  0.00071383  0.00138353] [ 0.0034019   0.00080666  0.00067211  0.00127598]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train train using final points with random lenghts\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, mode='finalstep')\n",
    "print(\"results training data\\n\", res_train)\n",
    "print(\"results validation data \\n\", res_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    running hyperparameter optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139794893321984\n",
      "call cross_val_score 1 epochs, with config {'lr': 0.004714876890308628, 'n_estimators': 268, 'gamma': 0.26459522737088415, 'cols_bt': 0.7205813491511858, 'subsample': 0.7500236279889205, 'maxdepth': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.01951 *** std: 0.0033\n",
      "Result of all Folds: [-0.024  -0.0165 -0.018 ]\n",
      "call cross_val_score 1 epochs, with config {'lr': 0.16217257533644988, 'n_estimators': 217, 'gamma': 0.17353041959112092, 'cols_bt': 0.5018073452510072, 'subsample': 0.04435576765316558, 'maxdepth': 3}\n",
      "MSE xgb: mean *** 0.01878 *** std: 0.0048\n",
      "Result of all Folds: [-0.0255 -0.0157 -0.0151]\n",
      "call cross_val_score 1 epochs, with config {'lr': 0.006511979192566196, 'n_estimators': 53, 'gamma': 0.5897426049071096, 'cols_bt': 0.8042770696688266, 'subsample': 0.6385951106142573, 'maxdepth': 6}\n",
      "MSE xgb: mean *** 0.04247 *** std: 0.0027\n",
      "Result of all Folds: [-0.0463 -0.0408 -0.0403]\n",
      "call cross_val_score 1 epochs, with config {'lr': 0.0037685582632389246, 'n_estimators': 166, 'gamma': 0.24151871555037485, 'cols_bt': 0.2175434363842382, 'subsample': 0.42201710086334426, 'maxdepth': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.03919 *** std: 0.0041\n",
      "Result of all Folds: [-0.045  -0.0366 -0.036 ]\n",
      "call cross_val_score 2 epochs, with config {'lr': 0.18041444469503112, 'n_estimators': 142, 'gamma': 0.05177245989145496, 'cols_bt': 0.6386477142624469, 'subsample': 0.42853431580055124, 'maxdepth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.01129 *** std: 0.0002\n",
      "Result of all Folds: [-0.0113 -0.0111 -0.0115]\n",
      "call cross_val_score 2 epochs, with config {'lr': 0.004714876890308628, 'n_estimators': 268, 'gamma': 0.26459522737088415, 'cols_bt': 0.7205813491511858, 'subsample': 0.7500236279889205, 'maxdepth': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.01951 *** std: 0.0033\n",
      "Result of all Folds: [-0.024  -0.0165 -0.018 ]\n",
      "call cross_val_score 2 epochs, with config {'lr': 0.16217257533644988, 'n_estimators': 217, 'gamma': 0.17353041959112092, 'cols_bt': 0.5018073452510072, 'subsample': 0.04435576765316558, 'maxdepth': 3}\n",
      "MSE xgb: mean *** 0.01878 *** std: 0.0048\n",
      "Result of all Folds: [-0.0255 -0.0157 -0.0151]\n",
      "call cross_val_score 2 epochs, with config {'lr': 0.0020517177177551056, 'n_estimators': 63, 'gamma': 0.547330352964734, 'cols_bt': 0.19357107178162672, 'subsample': 0.5991884186547828, 'maxdepth': 10}\n",
      "MSE xgb: mean *** 0.05713 *** std: 0.0032\n",
      "Result of all Folds: [-0.0615 -0.0556 -0.0543]\n",
      "call cross_val_score 4 epochs, with config {'lr': 0.16217257533644988, 'n_estimators': 217, 'gamma': 0.17353041959112092, 'cols_bt': 0.5018073452510072, 'subsample': 0.04435576765316558, 'maxdepth': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.01878 *** std: 0.0048\n",
      "Result of all Folds: [-0.0255 -0.0157 -0.0151]\n",
      "call cross_val_score 4 epochs, with config {'lr': 0.18041444469503112, 'n_estimators': 142, 'gamma': 0.05177245989145496, 'cols_bt': 0.6386477142624469, 'subsample': 0.42853431580055124, 'maxdepth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE xgb: mean *** 0.01129 *** std: 0.0002\n",
      "Result of all Folds: [-0.0113 -0.0111 -0.0115]\n",
      "return best_cfg:  {'lr': 0.18041444469503112, 'n_estimators': 142, 'gamma': 0.05177245989145496, 'cols_bt': 0.6386477142624469, 'subsample': 0.42853431580055124, 'maxdepth': 8}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='xgb', min_budget = 1, max_budget=4, run_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139794893321984\n",
      "evaluating with early stopping\n",
      "call cross_val_score 1000 epochs, with config {'batch_size': 42, 'lr': 0.016053361155360754}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.00788 *** std: 0.0009\n",
      "Result of all Folds: [-0.0091 -0.0071 -0.0075]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 1000 epochs, with config {'batch_size': 40, 'lr': 0.0012751322028299946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   14.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.03327 *** std: 0.0098\n",
      "Result of all Folds: [-0.0469 -0.0285 -0.0244]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 1000 epochs, with config {'batch_size': 17, 'lr': 0.0024363705871363653}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.01800 *** std: 0.0048\n",
      "Result of all Folds: [-0.0247 -0.0158 -0.0136]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 1000 epochs, with config {'batch_size': 44, 'lr': 0.00323203530624004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.02694 *** std: 0.0091\n",
      "Result of all Folds: [-0.0398 -0.0201 -0.021 ]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 2000 epochs, with config {'batch_size': 27, 'lr': 0.0594683318735445}\n",
      "Epoch 01459: early stopping\n",
      "Epoch 01185: early stopping\n",
      "Epoch 01131: early stopping\n",
      "MSE mlp: mean *** 0.00579 *** std: 0.0008\n",
      "Result of all Folds: [-0.0069 -0.0052 -0.0053]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 2000 epochs, with config {'batch_size': 42, 'lr': 0.016053361155360754}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   26.8s finished\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   27.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.00655 *** std: 0.0006\n",
      "Result of all Folds: [-0.0074 -0.0059 -0.0064]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 2000 epochs, with config {'batch_size': 17, 'lr': 0.0024363705871363653}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE mlp: mean *** 0.01052 *** std: 0.0028\n",
      "Result of all Folds: [-0.0145 -0.0082 -0.0088]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 2000 epochs, with config {'batch_size': 23, 'lr': 0.00011833479264203744}\n",
      "Epoch 01692: early stopping\n",
      "Epoch 01690: early stopping\n",
      "Epoch 01760: early stopping\n",
      "MSE mlp: mean *** 0.03513 *** std: 0.0099\n",
      "Result of all Folds: [-0.049  -0.0303 -0.0262]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 4000 epochs, with config {'batch_size': 42, 'lr': 0.016053361155360754}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   38.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02270: early stopping\n",
      "Epoch 02702: early stopping\n",
      "Epoch 02280: early stopping\n",
      "MSE mlp: mean *** 0.00676 *** std: 0.0006\n",
      "Result of all Folds: [-0.0076 -0.0065 -0.0062]\n",
      "evaluating with early stopping\n",
      "call cross_val_score 4000 epochs, with config {'batch_size': 27, 'lr': 0.0594683318735445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   40.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01461: early stopping\n",
      "Epoch 01012: early stopping\n",
      "Epoch 01015: early stopping\n",
      "MSE mlp: mean *** 0.00588 *** std: 0.0005\n",
      "Result of all Folds: [-0.0066 -0.0057 -0.0053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return best_cfg:  {'batch_size': 27, 'lr': 0.0594683318735445}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 1000, max_budget=4000, \n",
    "                       run_name='', earlystop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139888802830080\n",
      "evaluating with early stopping\n",
      "call cross_val_score 15 epochs, with config {'batch_size': 3}\n",
      "build lstm with input_dim: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "evaluating with early stopping\n",
      "call cross_val_score 15 epochs, with config {'batch_size': 1}\n",
      "build lstm with input_dim: 1\n",
      "Epoch 1/15\n",
      " 81/176 [============>.................] - ETA: 9s - loss: 0.0190 - mean_squared_error: 0.0190"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fb481f222799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m best_cfg = hp.optimize(lcs, Y, model_type='lstm', \n\u001b[0;32m----> 2\u001b[0;31m                        min_budget = 10, max_budget=30, run_name='', earlystop=True)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DL Theory/project/hyperband.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(X, Y, objective, config_space_getter, model_type, min_budget, max_budget, job_queue_sizes, lr_exp_decay, earlystop, dropout, L1L2, base_path, run_name)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;31m# runs one iteration if at least one worker is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mn_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_budget\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmin_budget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_n_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# shutdown the worker and the dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/hpbandster-0.1.0-py3.5.egg/hpbandster/HB_master.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, n_iterations, iteration_class, min_n_workers)\u001b[0m\n\u001b[1;32m    166\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbudgets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                         \u001b[0;31m# find a new run to start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/hpbandster-0.1.0-py3.5.egg/hpbandster/HB_master.py\u001b[0m in \u001b[0;36mactive_iterations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mactive_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;34m\"\"\" function that returns a list of all iterations that are not marked as finished \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/hpbandster-0.1.0-py3.5.egg/hpbandster/HB_master.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mactive_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;34m\"\"\" function that returns a list of all iterations that are not marked as finished \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 12s 67ms/step - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 2/15\n",
      "158/176 [=========================>....] - ETA: 0s - loss: 0.0089 - mean_squared_error: 0.0089"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(lcs, Y, model_type='lstm', \n",
    "                       min_budget = 10, max_budget=30, run_name='', earlystop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140309731387136\n",
      "cross validate 2 epochs, with config {'batch_size': 7}\n",
      "evaluating with early stopping\n",
      "Train on 176 samples, validate on 89 samples\n",
      "Epoch 1/2\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
      "Epoch 2/2\n",
      "176/176 [==============================] - 1s 7ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/2\n",
      "177/177 [==============================] - 1s 7ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0190 - val_mean_squared_error: 0.0190\n",
      "Epoch 2/2\n",
      "177/177 [==============================] - 1s 7ms/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/2\n",
      "177/177 [==============================] - 1s 8ms/step - loss: 0.0141 - mean_squared_error: 0.0141 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 2/2\n",
      "177/177 [==============================] - 1s 7ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "MSE multi_lstm: mean *** 0.03109 *** std: 0.0190\n",
      "Result of all Folds: [ 0.0159  0.0195  0.0579]\n",
      "cross validate 2 epochs, with config {'batch_size': 6}\n",
      "evaluating with early stopping\n",
      "Train on 176 samples, validate on 89 samples\n",
      "Epoch 1/2\n",
      "176/176 [==============================] - 4s 23ms/step - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "Epoch 2/2\n",
      "176/176 [==============================] - 3s 16ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/2\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n",
      "Epoch 2/2\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/2\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0220 - val_mean_squared_error: 0.0220\n",
      "Epoch 2/2\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
      "MSE multi_lstm: mean *** 0.01689 *** std: 0.0051\n",
      "Result of all Folds: [ 0.0221  0.01    0.0186]\n",
      "cross validate 4 epochs, with config {'batch_size': 7}\n",
      "evaluating with early stopping\n",
      "Train on 176 samples, validate on 89 samples\n",
      "Epoch 1/4\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0188 - val_mean_squared_error: 0.0188\n",
      "Epoch 2/4\n",
      "176/176 [==============================] - 3s 14ms/step - loss: 0.0131 - mean_squared_error: 0.0131 - val_loss: 0.0316 - val_mean_squared_error: 0.0316\n",
      "Epoch 3/4\n",
      "176/176 [==============================] - 3s 14ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0221 - val_mean_squared_error: 0.0221\n",
      "Epoch 4/4\n",
      "176/176 [==============================] - 2s 14ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/4\n",
      "177/177 [==============================] - 3s 14ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 2/4\n",
      "177/177 [==============================] - 2s 14ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0315 - val_mean_squared_error: 0.0315\n",
      "Epoch 3/4\n",
      "177/177 [==============================] - 3s 15ms/step - loss: 0.0141 - mean_squared_error: 0.0141 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 4/4\n",
      "177/177 [==============================] - 3s 15ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/4\n",
      "177/177 [==============================] - 2s 14ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0154 - val_mean_squared_error: 0.0154\n",
      "Epoch 2/4\n",
      "177/177 [==============================] - 3s 14ms/step - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 3/4\n",
      "177/177 [==============================] - 3s 14ms/step - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 4/4\n",
      "177/177 [==============================] - 2s 14ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "MSE multi_lstm: mean *** 0.00961 *** std: 0.0034\n",
      "Result of all Folds: [ 0.014   0.0058  0.009 ]\n",
      "return best_cfg:  {'batch_size': 7}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize([configs,lcs], Y, model_type='multi_lstm', \n",
    "                       min_budget = 2, max_budget=4, run_name='', earlystop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "build lstm with input_dim: 6\n",
      "Train on 200 samples, validate on 65 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0743 - mean_squared_error: 0.0743 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "65/65 [==============================] - 0s 843us/step\n",
      "mse:  0.00873059442697\n"
     ]
    }
   ],
   "source": [
    "# experiment with concatenating the config to each data point of learning curve\n",
    "timesteps = 5\n",
    "configs,lcs,Y = t.load_lstm_data_concat_cfg(timesteps=timesteps)\n",
    "model_type = 'lstm'\n",
    "model = m.lstm(lcs[0][0].shape[0])\n",
    "m.train_lstm(model, lcs, Y, split=200, batch_size=20, epochs=5)\n",
    "mse = m.eval_lstm(model, lcs, Y, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49393387f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_subset=20\n",
    "t_idx = np.arange(1, n_epochs+1)\n",
    "\n",
    "[plt.plot(t_idx, lc) for lc in learning_curves[:n_subset]]\n",
    "plt.title(\"Subset of learning curves\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over the final error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "[ 0.15153689  0.1515819   0.15282866  0.15434343  0.15650082  0.15954709\n",
      "  0.15959183  0.16055092  0.16080048  0.16156045  0.16244856  0.16430366\n",
      "  0.16470223  0.16500051  0.16504065  0.16676737  0.16724419  0.16746314\n",
      "  0.16788982  0.16836734  0.16912972  0.1696      0.17007007  0.17097436\n",
      "  0.17197581  0.17218749  0.17309483  0.17469136  0.17477876  0.17589213\n",
      "  0.1771176   0.17717566  0.18051375  0.18429064  0.18561671  0.18614719\n",
      "  0.18654372  0.1888912   0.18908462  0.18939709  0.19002079  0.19011057\n",
      "  0.19062407  0.19251503  0.19269777  0.19512195  0.19799197  0.20191532\n",
      "  0.2025641   0.20337261  0.20503018  0.20661408  0.2067056   0.20721804\n",
      "  0.20923359  0.20958447  0.21048843  0.21072796  0.21152115  0.21208654\n",
      "  0.21271271  0.21322007  0.21383838  0.21459538  0.2149739   0.21512623\n",
      "  0.21656115  0.2168614   0.2173462   0.21915322  0.22029654  0.22293893\n",
      "  0.22635376  0.22674772  0.22709083  0.23027569  0.23098019  0.23246412\n",
      "  0.23264447  0.23268473  0.23302277  0.23335352  0.23336735  0.23343434\n",
      "  0.23412699  0.23500927  0.23647416  0.23732251  0.2375452   0.23777065\n",
      "  0.23783784  0.24062501  0.24231975  0.24259704  0.24362348  0.24435318\n",
      "  0.24571427  0.24644309  0.2464986   0.24904099  0.24914332  0.25050813\n",
      "  0.25081633  0.25112107  0.25115116  0.25141243  0.25155154  0.25231388\n",
      "  0.25285347  0.25309199  0.25416666  0.2567131   0.25937046  0.25951522\n",
      "  0.26032892  0.26065163  0.26079499  0.26187094  0.2624217   0.26360544\n",
      "  0.26433381  0.26460584  0.26635748  0.26708823  0.26754297  0.26783325\n",
      "  0.26910763  0.26916869  0.26990796  0.26999184  0.27050505  0.27067669\n",
      "  0.27130417  0.27173478  0.27414801  0.27594143  0.27628738  0.27631045\n",
      "  0.27649817  0.27902911  0.27943868  0.27975207  0.28036345  0.28071056\n",
      "  0.2819192   0.28218219  0.28220612  0.28399035  0.28631791  0.2866592\n",
      "  0.28738856  0.28765182  0.28810564  0.28815629  0.28922669  0.29289517\n",
      "  0.29538055  0.29550102  0.29559559  0.2967033   0.297439    0.29830016\n",
      "  0.30056406  0.30062023  0.30236794  0.3029029   0.30427367  0.30525252\n",
      "  0.30558355  0.30674017  0.30742156  0.30936489  0.31012913  0.31084805\n",
      "  0.31117731  0.31134724  0.3121154   0.31726662  0.31868912  0.31876791\n",
      "  0.31962783  0.32151515  0.32217489  0.32231571  0.32381829  0.32454175\n",
      "  0.32527919  0.32738879  0.32871207  0.32940584  0.33004115  0.33357348\n",
      "  0.33494949  0.34071533  0.34141941  0.34167684  0.34295484  0.34467005\n",
      "  0.34560162  0.35090726  0.35261458  0.35325203  0.35509426  0.35739455\n",
      "  0.35766497  0.36118952  0.36355311  0.36651265  0.36814726  0.37251357\n",
      "  0.37533485  0.37769964  0.37815976  0.37994988  0.38020202  0.38165264\n",
      "  0.39293826  0.39895751  0.39897157  0.40262136  0.40277778  0.40509657\n",
      "  0.41080247  0.41186544  0.41676955  0.42396314  0.43043884  0.43374359\n",
      "  0.43489583  0.4712326   0.47561098  0.47720855  0.48276563  0.48861117\n",
      "  0.49195474  0.50221061  0.51912239  0.52155658  0.53157578  0.53413534\n",
      "  0.53465447  0.56480344  0.58298298  0.63064267  0.6986456   0.89738956\n",
      "  0.89742798  0.8975      0.89770334  0.89776127  0.89795918  0.89835934\n",
      "  0.89857724  0.89873029  0.89878788  0.89882447  0.89885726  0.89964912\n",
      "  0.89978547  0.900398    0.90061919  0.90229302  0.90316206  0.90459863\n",
      "  0.90499599]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49357bc908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493579d5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted = np.sort(learning_curves[:, -1])   # sorted list of final val error\n",
    "print(len(sorted))\n",
    "h = plt.hist(sorted, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(len(sorted))/float(len(sorted))   # from 0 to 1 in 265 even steps\n",
    "plt.plot(sorted, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over all error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4935a3ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493581f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_values = np.sort(learning_curves.flatten())\n",
    "\n",
    "h = plt.hist(all_values, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(all_values.shape[0])/all_values.shape[0]\n",
    "plt.plot(all_values, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
