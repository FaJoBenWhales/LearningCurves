{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Learning Curves of Convolutional Neural Network on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tools as t\n",
    "import models as m\n",
    "import hyperband as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n"
     ]
    }
   ],
   "source": [
    "configs,lcs,Y = t.load_data(scale_configs = True)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Testing models (mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/10\n",
      "177/177 [==============================] - 2s 14ms/step - loss: 0.0554 - val_loss: 0.0263\n",
      "Epoch 2/10\n",
      "177/177 [==============================] - 0s 445us/step - loss: 0.0363 - val_loss: 0.0250\n",
      "Epoch 3/10\n",
      "177/177 [==============================] - 0s 446us/step - loss: 0.0353 - val_loss: 0.0234\n",
      "Epoch 4/10\n",
      "177/177 [==============================] - 0s 534us/step - loss: 0.0330 - val_loss: 0.0229\n",
      "Epoch 5/10\n",
      "177/177 [==============================] - 0s 443us/step - loss: 0.0305 - val_loss: 0.0252\n",
      "Epoch 6/10\n",
      "177/177 [==============================] - 0s 596us/step - loss: 0.0287 - val_loss: 0.0227\n",
      "Epoch 7/10\n",
      "177/177 [==============================] - 0s 431us/step - loss: 0.0260 - val_loss: 0.0192\n",
      "Epoch 8/10\n",
      "177/177 [==============================] - 0s 483us/step - loss: 0.0247 - val_loss: 0.0183\n",
      "Epoch 9/10\n",
      "177/177 [==============================] - 0s 683us/step - loss: 0.0224 - val_loss: 0.0185\n",
      "Epoch 10/10\n",
      "177/177 [==============================] - 0s 450us/step - loss: 0.0219 - val_loss: 0.0176\n",
      "mse train: 0.02026, mse validation 0.01757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020264899801071712, 0.017568055259537099)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20}\n",
    "model = m.mlp(cfg)\n",
    "m.train_mlp(model, configs, Y, cfg, split=177, epochs=10)\n",
    "m.eval_mlp(model, configs, Y, split=177, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 10 epochs, eval during training with 10 epochs\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 2s 283ms/step - loss: 0.3323 - mean_squared_error: 0.3323 - val_loss: 0.1488 - val_mean_squared_error: 0.1488\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.1609 - mean_squared_error: 0.1609 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 1s 113ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 1s 110ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "mse train: 0.00087, mse validation 0.00116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00087328647631683377, 0.001162902298025613)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(10,10), split=150, \n",
    "             batch_size=20, epochs=30, mode='finalstep', verbose=1)\n",
    "m.eval_lstm_direct(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, eval during training with 10 epochs\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 4s 451ms/step - loss: 0.2131 - mean_squared_error: 0.2131 - val_loss: 0.0756 - val_mean_squared_error: 0.0756\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.0630 - mean_squared_error: 0.0630 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 2s 241ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 2s 305ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 2s 216ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 2s 255ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "mse train: 0.00228, mse validation 0.00233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0022836772330743558, 0.0023286114485302134)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same with training on random lenghts\n",
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,10), split=150, batch_size=20, \n",
    "             epochs=30, mode='finalstep', verbose=1)\n",
    "m.eval_lstm_direct(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 527ms/step - loss: 0.2842 - mean_squared_error: 0.2842 - val_loss: 0.1273 - val_mean_squared_error: 0.1273\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.0610 - mean_squared_error: 0.0610 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 8.6942e-04 - val_mean_squared_error: 8.6942e-04\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 8.6067e-04 - val_mean_squared_error: 8.6067e-04\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 8.0088e-04 - mean_squared_error: 8.0088e-04 - val_loss: 5.3051e-04 - val_mean_squared_error: 5.3051e-04\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 6.5383e-04 - mean_squared_error: 6.5383e-04 - val_loss: 4.8940e-04 - val_mean_squared_error: 4.8940e-04\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 5.9794e-04 - mean_squared_error: 5.9794e-04 - val_loss: 4.1794e-04 - val_mean_squared_error: 4.1794e-04\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 4.8789e-04 - mean_squared_error: 4.8789e-04 - val_loss: 4.4278e-04 - val_mean_squared_error: 4.4278e-04\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 4.7472e-04 - mean_squared_error: 4.7472e-04 - val_loss: 4.8829e-04 - val_mean_squared_error: 4.8829e-04\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 4.7378e-04 - mean_squared_error: 4.7378e-04 - val_loss: 4.6291e-04 - val_mean_squared_error: 4.6291e-04\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 4.5659e-04 - mean_squared_error: 4.5659e-04 - val_loss: 4.7734e-04 - val_mean_squared_error: 4.7734e-04\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 4.7181e-04 - mean_squared_error: 4.7181e-04 - val_loss: 5.0012e-04 - val_mean_squared_error: 5.0012e-04\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 4.7167e-04 - mean_squared_error: 4.7167e-04 - val_loss: 5.0414e-04 - val_mean_squared_error: 5.0414e-04\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 4.9762e-04 - mean_squared_error: 4.9762e-04 - val_loss: 5.1562e-04 - val_mean_squared_error: 5.1562e-04\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 5.0700e-04 - mean_squared_error: 5.0700e-04 - val_loss: 5.6090e-04 - val_mean_squared_error: 5.6090e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 6.1550e-04 - mean_squared_error: 6.1550e-04 - val_loss: 5.7167e-04 - val_mean_squared_error: 5.7167e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 7.3531e-04 - mean_squared_error: 7.3531e-04 - val_loss: 8.5170e-04 - val_mean_squared_error: 8.5170e-04\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 8.7663e-04 - val_mean_squared_error: 8.7663e-04\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 7.7629e-04 - val_mean_squared_error: 7.7629e-04\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 3.5146e-04 - val_mean_squared_error: 3.5146e-04\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 5.8703e-04 - mean_squared_error: 5.8703e-04 - val_loss: 5.1905e-04 - val_mean_squared_error: 5.1905e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 117ms/step - loss: 6.5990e-04 - mean_squared_error: 6.5990e-04 - val_loss: 6.0620e-04 - val_mean_squared_error: 6.0620e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 6.2369e-04 - mean_squared_error: 6.2369e-04 - val_loss: 5.1306e-04 - val_mean_squared_error: 5.1306e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 5.4402e-04 - mean_squared_error: 5.4402e-04 - val_loss: 5.0181e-04 - val_mean_squared_error: 5.0181e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 5.4179e-04 - mean_squared_error: 5.4179e-04 - val_loss: 5.0539e-04 - val_mean_squared_error: 5.0539e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 5.5562e-04 - mean_squared_error: 5.5562e-04 - val_loss: 5.0386e-04 - val_mean_squared_error: 5.0386e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 5.4436e-04 - mean_squared_error: 5.4436e-04 - val_loss: 5.1037e-04 - val_mean_squared_error: 5.1037e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 5.5181e-04 - mean_squared_error: 5.5181e-04 - val_loss: 5.2507e-04 - val_mean_squared_error: 5.2507e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 5.6531e-04 - mean_squared_error: 5.6531e-04 - val_loss: 5.2816e-04 - val_mean_squared_error: 5.2816e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 5.7088e-04 - mean_squared_error: 5.7088e-04 - val_loss: 5.3062e-04 - val_mean_squared_error: 5.3062e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 5.8097e-04 - mean_squared_error: 5.8097e-04 - val_loss: 5.2733e-04 - val_mean_squared_error: 5.2733e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 5.8217e-04 - mean_squared_error: 5.8217e-04 - val_loss: 5.3640e-04 - val_mean_squared_error: 5.3640e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 6.1269e-04 - mean_squared_error: 6.1269e-04 - val_loss: 5.2189e-04 - val_mean_squared_error: 5.2189e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 6.1096e-04 - mean_squared_error: 6.1096e-04 - val_loss: 5.7000e-04 - val_mean_squared_error: 5.7000e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 7.2944e-04 - mean_squared_error: 7.2944e-04 - val_loss: 5.4656e-04 - val_mean_squared_error: 5.4656e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 8.0794e-04 - mean_squared_error: 8.0794e-04 - val_loss: 7.6281e-04 - val_mean_squared_error: 7.6281e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 8.4114e-04 - val_mean_squared_error: 8.4114e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 9.5205e-04 - val_mean_squared_error: 9.5205e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 7.7647e-04 - val_mean_squared_error: 7.7647e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 8.4677e-04 - mean_squared_error: 8.4677e-04 - val_loss: 7.4963e-04 - val_mean_squared_error: 7.4963e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 7.4931e-04 - mean_squared_error: 7.4931e-04 - val_loss: 5.9438e-04 - val_mean_squared_error: 5.9438e-04\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 6.7311e-04 - mean_squared_error: 6.7311e-04 - val_loss: 4.7425e-04 - val_mean_squared_error: 4.7425e-04\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 112ms/step - loss: 6.5365e-04 - mean_squared_error: 6.5365e-04 - val_loss: 4.5641e-04 - val_mean_squared_error: 4.5641e-04\n",
      "mse train: 0.06521, mse validation 0.02669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.065214293491303491, 0.026686737793762665)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(10,10), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=10, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 20 epochs, eval during training with 20 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 6s 616ms/step - loss: 0.3187 - mean_squared_error: 0.3187 - val_loss: 0.1667 - val_mean_squared_error: 0.1667\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 0.0876 - mean_squared_error: 0.0876 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0228 - val_mean_squared_error: 0.0228\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 157ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 171ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 153ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 8.5395e-04 - val_mean_squared_error: 8.5395e-04\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 7.7500e-04 - mean_squared_error: 7.7500e-04 - val_loss: 4.8302e-04 - val_mean_squared_error: 4.8302e-04\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 6.5339e-04 - mean_squared_error: 6.5339e-04 - val_loss: 7.3556e-04 - val_mean_squared_error: 7.3556e-04\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 4.7104e-04 - mean_squared_error: 4.7104e-04 - val_loss: 5.0899e-04 - val_mean_squared_error: 5.0899e-04\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 4.3912e-04 - mean_squared_error: 4.3912e-04 - val_loss: 5.1421e-04 - val_mean_squared_error: 5.1421e-04\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 4.4952e-04 - mean_squared_error: 4.4952e-04 - val_loss: 4.1170e-04 - val_mean_squared_error: 4.1170e-04\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 4.4389e-04 - mean_squared_error: 4.4389e-04 - val_loss: 3.6920e-04 - val_mean_squared_error: 3.6920e-04\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 160ms/step - loss: 3.9739e-04 - mean_squared_error: 3.9739e-04 - val_loss: 3.9454e-04 - val_mean_squared_error: 3.9454e-04\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 3.6438e-04 - mean_squared_error: 3.6438e-04 - val_loss: 4.1874e-04 - val_mean_squared_error: 4.1874e-04\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 118ms/step - loss: 3.7776e-04 - mean_squared_error: 3.7776e-04 - val_loss: 4.4277e-04 - val_mean_squared_error: 4.4277e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 3.9011e-04 - mean_squared_error: 3.9011e-04 - val_loss: 4.5991e-04 - val_mean_squared_error: 4.5991e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 3.9392e-04 - mean_squared_error: 3.9392e-04 - val_loss: 4.8633e-04 - val_mean_squared_error: 4.8633e-04\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 4.0337e-04 - mean_squared_error: 4.0337e-04 - val_loss: 5.2011e-04 - val_mean_squared_error: 5.2011e-04\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 118ms/step - loss: 4.1083e-04 - mean_squared_error: 4.1083e-04 - val_loss: 5.4903e-04 - val_mean_squared_error: 5.4903e-04\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 4.1645e-04 - mean_squared_error: 4.1645e-04 - val_loss: 5.8611e-04 - val_mean_squared_error: 5.8611e-04\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 4.2205e-04 - mean_squared_error: 4.2205e-04 - val_loss: 6.2044e-04 - val_mean_squared_error: 6.2044e-04\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 4.2445e-04 - mean_squared_error: 4.2445e-04 - val_loss: 6.4510e-04 - val_mean_squared_error: 6.4510e-04\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 4.2634e-04 - mean_squared_error: 4.2634e-04 - val_loss: 6.5912e-04 - val_mean_squared_error: 6.5912e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 4.2768e-04 - mean_squared_error: 4.2768e-04 - val_loss: 6.5945e-04 - val_mean_squared_error: 6.5945e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 4.2838e-04 - mean_squared_error: 4.2838e-04 - val_loss: 6.4277e-04 - val_mean_squared_error: 6.4277e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 4.3042e-04 - mean_squared_error: 4.3042e-04 - val_loss: 6.1855e-04 - val_mean_squared_error: 6.1855e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 4.3192e-04 - mean_squared_error: 4.3192e-04 - val_loss: 5.8745e-04 - val_mean_squared_error: 5.8745e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 4.3334e-04 - mean_squared_error: 4.3334e-04 - val_loss: 5.6487e-04 - val_mean_squared_error: 5.6487e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 4.3174e-04 - mean_squared_error: 4.3174e-04 - val_loss: 5.4573e-04 - val_mean_squared_error: 5.4573e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 4.2783e-04 - mean_squared_error: 4.2783e-04 - val_loss: 5.4693e-04 - val_mean_squared_error: 5.4693e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 4.2134e-04 - mean_squared_error: 4.2134e-04 - val_loss: 5.4281e-04 - val_mean_squared_error: 5.4281e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 4.1351e-04 - mean_squared_error: 4.1351e-04 - val_loss: 5.6364e-04 - val_mean_squared_error: 5.6364e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 2s 169ms/step - loss: 4.0610e-04 - mean_squared_error: 4.0610e-04 - val_loss: 5.4848e-04 - val_mean_squared_error: 5.4848e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 3.9829e-04 - mean_squared_error: 3.9829e-04 - val_loss: 5.8068e-04 - val_mean_squared_error: 5.8068e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 3.9303e-04 - mean_squared_error: 3.9303e-04 - val_loss: 5.2665e-04 - val_mean_squared_error: 5.2665e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 3.8678e-04 - mean_squared_error: 3.8678e-04 - val_loss: 5.9208e-04 - val_mean_squared_error: 5.9208e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 3.8725e-04 - mean_squared_error: 3.8725e-04 - val_loss: 4.8470e-04 - val_mean_squared_error: 4.8470e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 3.8458e-04 - mean_squared_error: 3.8458e-04 - val_loss: 6.2418e-04 - val_mean_squared_error: 6.2418e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 3.9485e-04 - mean_squared_error: 3.9485e-04 - val_loss: 4.4557e-04 - val_mean_squared_error: 4.4557e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 3.9114e-04 - mean_squared_error: 3.9114e-04 - val_loss: 6.6658e-04 - val_mean_squared_error: 6.6658e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 4.0717e-04 - mean_squared_error: 4.0717e-04 - val_loss: 4.1475e-04 - val_mean_squared_error: 4.1475e-04\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.9160e-04 - mean_squared_error: 3.9160e-04 - val_loss: 6.6236e-04 - val_mean_squared_error: 6.6236e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 4.0314e-04 - mean_squared_error: 4.0314e-04 - val_loss: 3.8992e-04 - val_mean_squared_error: 3.8992e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 3.8044e-04 - mean_squared_error: 3.8044e-04 - val_loss: 5.9433e-04 - val_mean_squared_error: 5.9433e-04\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 3.8620e-04 - mean_squared_error: 3.8620e-04 - val_loss: 3.8484e-04 - val_mean_squared_error: 3.8484e-04\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.7075e-04 - mean_squared_error: 3.7075e-04 - val_loss: 5.0869e-04 - val_mean_squared_error: 5.0869e-04\n",
      "mse train: 0.07631, mse validation 0.03996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.076310423364451249, 0.039959337114572585)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(20,20), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=20, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 0.2543 - mean_squared_error: 0.2543 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0601 - mean_squared_error: 0.0601 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 9.4203e-04 - val_mean_squared_error: 9.4203e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 9.7494e-04 - val_mean_squared_error: 9.7494e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 9.9589e-04 - val_mean_squared_error: 9.9589e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 7.8937e-04 - val_mean_squared_error: 7.8937e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 9.7421e-04 - mean_squared_error: 9.7421e-04 - val_loss: 7.6363e-04 - val_mean_squared_error: 7.6363e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 9.1752e-04 - mean_squared_error: 9.1752e-04 - val_loss: 6.7368e-04 - val_mean_squared_error: 6.7368e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 8.6539e-04 - mean_squared_error: 8.6539e-04 - val_loss: 6.6338e-04 - val_mean_squared_error: 6.6338e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 9.6177e-04 - mean_squared_error: 9.6177e-04 - val_loss: 7.0253e-04 - val_mean_squared_error: 7.0253e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 8.1121e-04 - val_mean_squared_error: 8.1121e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 8.2198e-04 - val_mean_squared_error: 8.2198e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.2502e-04 - val_mean_squared_error: 8.2502e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 6.6508e-04 - val_mean_squared_error: 6.6508e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 7.6501e-04 - val_mean_squared_error: 7.6501e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 7.4475e-04 - mean_squared_error: 7.4475e-04 - val_loss: 8.0915e-04 - val_mean_squared_error: 8.0915e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 7.8624e-04 - mean_squared_error: 7.8624e-04 - val_loss: 6.1501e-04 - val_mean_squared_error: 6.1501e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 6.5213e-04 - mean_squared_error: 6.5213e-04 - val_loss: 5.7252e-04 - val_mean_squared_error: 5.7252e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 6.2322e-04 - mean_squared_error: 6.2322e-04 - val_loss: 5.3362e-04 - val_mean_squared_error: 5.3362e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 6.2554e-04 - mean_squared_error: 6.2554e-04 - val_loss: 5.4672e-04 - val_mean_squared_error: 5.4672e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 6.5832e-04 - mean_squared_error: 6.5832e-04 - val_loss: 5.5547e-04 - val_mean_squared_error: 5.5547e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 7.0015e-04 - mean_squared_error: 7.0015e-04 - val_loss: 6.4986e-04 - val_mean_squared_error: 6.4986e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 7.6763e-04 - mean_squared_error: 7.6763e-04 - val_loss: 6.2912e-04 - val_mean_squared_error: 6.2912e-04\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 8.4450e-04 - mean_squared_error: 8.4450e-04 - val_loss: 8.5492e-04 - val_mean_squared_error: 8.5492e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 9.5290e-04 - mean_squared_error: 9.5290e-04 - val_loss: 7.2314e-04 - val_mean_squared_error: 7.2314e-04\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 7.3492e-04 - val_mean_squared_error: 7.3492e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 8.2414e-04 - val_mean_squared_error: 8.2414e-04\n",
      "mse train: 7.22660, mse validation 7.20777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.2265960036872281, 7.2077733345183974)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(5,5), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=5, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, eval during training with 10 epochs\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 0.2961 - mean_squared_error: 0.2961 - val_loss: 0.0789 - val_mean_squared_error: 0.0789\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.0863 - mean_squared_error: 0.0863 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 2s 244ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 2s 283ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "mse train: 0.92659, mse validation 0.73805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.92658545341745213, 0.73805311720421751)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,10), split=150, batch_size=20, \n",
    "             epochs=10, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.277989 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.2915 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.276354 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.292358 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.274371 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.290001 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.274371 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.287972 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.274371 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.287972 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.274371 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.284571 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.274371 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.281017 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.274371 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.272158 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.269471 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.269471 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.269471 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.269471 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.269471 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.269471 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.269471 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.269471 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.269471 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.269471 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.269471 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.269471 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.269471 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.269471 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.51741443  0.4540801   0.42677908  0.39405802  0.37759711  0.36866406\n",
      "  0.35782395  0.34698384  0.33524039  0.32951922  0.32249322  0.32148951\n",
      "  0.31506574  0.31356018  0.30683529  0.31135201  0.30442638  0.30312155\n",
      "  0.29308442  0.29298404  0.29338553  0.29388738  0.2857573   0.28977216\n",
      "  0.28073873  0.2899729   0.28214393  0.27792833  0.27863094  0.27381311\n",
      "  0.27270902  0.27903242  0.2728094   0.27009937  0.27050085  0.26809194\n",
      "  0.26538191  0.26558265  0.27120345  0.26708823]\n",
      "step nr. 10 prediction / true value for lc number 13 0.343269 / 0.322493222025\n",
      "step nr. 11 prediction / true value for lc number 13 0.345114 / 0.321489512185\n",
      "step nr. 12 prediction / true value for lc number 13 0.345114 / 0.315065743747\n",
      "step nr. 13 prediction / true value for lc number 13 0.339905 / 0.313560176043\n",
      "step nr. 14 prediction / true value for lc number 13 0.346333 / 0.306835287883\n",
      "step nr. 15 prediction / true value for lc number 13 0.346333 / 0.311352006447\n",
      "step nr. 16 prediction / true value for lc number 13 0.345114 / 0.304426380146\n",
      "step nr. 17 prediction / true value for lc number 13 0.346333 / 0.303121549848\n",
      "step nr. 18 prediction / true value for lc number 13 0.346333 / 0.293084416125\n",
      "step nr. 19 prediction / true value for lc number 13 0.346333 / 0.292984041167\n",
      "step nr. 20 prediction / true value for lc number 13 0.346333 / 0.293385528488\n",
      "step nr. 21 prediction / true value for lc number 13 0.346333 / 0.2938873812\n",
      "step nr. 22 prediction / true value for lc number 13 0.346333 / 0.285757301766\n",
      "step nr. 23 prediction / true value for lc number 13 0.346333 / 0.289772155108\n",
      "step nr. 24 prediction / true value for lc number 13 0.346333 / 0.280738732697\n",
      "step nr. 25 prediction / true value for lc number 13 0.346333 / 0.289972896929\n",
      "step nr. 26 prediction / true value for lc number 13 0.346333 / 0.282143932802\n",
      "step nr. 27 prediction / true value for lc number 13 0.346333 / 0.277928333959\n",
      "step nr. 28 prediction / true value for lc number 13 0.346333 / 0.278630935116\n",
      "step nr. 29 prediction / true value for lc number 13 0.346333 / 0.273813112282\n",
      "step nr. 30 prediction / true value for lc number 13 0.346333 / 0.272709022334\n",
      "step nr. 31 prediction / true value for lc number 13 0.346333 / 0.279032418757\n",
      "step nr. 32 prediction / true value for lc number 13 0.346333 / 0.272809395084\n",
      "step nr. 33 prediction / true value for lc number 13 0.346333 / 0.270099369096\n",
      "step nr. 34 prediction / true value for lc number 13 0.346333 / 0.270500852738\n",
      "step nr. 35 prediction / true value for lc number 13 0.346333 / 0.26809193985\n",
      "step nr. 36 prediction / true value for lc number 13 0.346333 / 0.26538191239\n",
      "step nr. 37 prediction / true value for lc number 13 0.346333 / 0.265582654211\n",
      "step nr. 38 prediction / true value for lc number 13 0.346333 / 0.271203452422\n",
      "step nr. 39 prediction / true value for lc number 13 0.346333 / 0.26708822633\n",
      "mse train: 0.08315, mse validation 0.05617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.083146945607023348, 0.056170584855359744)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, \n",
    "       'cols_bt': 0.9376450587145334, 'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "model = m.xgb_next(cfg)\n",
    "m.train_xgb_next(model, [configs,lcs], split = 200)\n",
    "m.eval_xgb_stepwise(model, [configs,lcs], Y, 10, split=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    evaluating models with cross validation (ridge, XGB, mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'alpha': 1.0}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.02977] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03703]\n",
      " [ 0.02671]\n",
      " [ 0.02556]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.02747] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02433]\n",
      " [ 0.02864]\n",
      " [ 0.02943]]\n",
      "mse over all validation data 0.0297968665436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf7069b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg={'alpha':1.0}\n",
    "res = m.eval_cv('ridge', configs, Y, cfg=cfg, splits = 3)\n",
    "t.scatter_plot(Y, res['y_pred'], res['mse'], 'ridge regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'n_estimators': 1000, 'gamma': 0.007833441242813044, 'maxdepth': 10, 'cols_bt': 0.9376450587145334}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00728] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00881]\n",
      " [ 0.00549]\n",
      " [ 0.00753]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00059] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00053]\n",
      " [ 0.00061]\n",
      " [ 0.00062]]\n",
      "mse over all validation data 0.00728375623487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf645b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "#cfg = {'maxdepth': 4, 'lr': 0.07120217610550672, 'gamma': 0.03393596760993278, 'cols_bt': 0.823494199726015, 'n_estimators': 107, 'subsample': 0.7288741544938715}\n",
    "res = m.eval_cv('xgb', configs, Y, cfg=cfg, splits = 3)\n",
    "t.scatter_plot(Y, res['y_pred'], res['mse'], 'xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 500 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00601] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00614]\n",
      " [ 0.00569]\n",
      " [ 0.0062 ]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00051] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00019]\n",
      " [ 0.00076]\n",
      " [ 0.00056]]\n",
      "mse over all validation data 0.00601034541179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf585b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "res = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=500, splits = 3, earlystop=False) \n",
    "t.scatter_plot(Y, res['y_pred'], res['mse'], 'mlp 500 epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04596, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04596 to 0.04550, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04579, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04550 to 0.04279, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04279 to 0.04151, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.04443, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04151 to 0.03859, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03859 to 0.03638, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03638 to 0.03513, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03513 to 0.03436, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03436 to 0.02978, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02978 to 0.02816, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02816 to 0.02616, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02616 to 0.02345, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02345 to 0.02335, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02335 to 0.02248, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02248 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01869 to 0.01845, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01851, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01845 to 0.01539, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01539 to 0.01453, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01453 to 0.01330, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01330 to 0.01288, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01337, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01288 to 0.01272, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01272 to 0.01146, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01400, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01146 to 0.01053, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01053 to 0.00997, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00997 to 0.00975, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00975 to 0.00946, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00946 to 0.00920, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00920 to 0.00883, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00883 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00919, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00854 to 0.00848, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00848 to 0.00845, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00845 to 0.00820, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00820 to 0.00804, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00804 to 0.00801, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00801 to 0.00794, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00794 to 0.00788, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00959, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00950, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00788 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00766 to 0.00761, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00761 to 0.00761, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00761 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00801, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00753 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00795, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00747 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00741 to 0.00739, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00756, did not improve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00142: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00739 to 0.00734, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00734 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00728 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00882, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00852, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00842, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00761, did not improve\n",
      "Epoch 00235: early stopping\n",
      "Using epoch 00160 with val_loss: 0.00724\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02952, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02952 to 0.02874, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02874 to 0.02751, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02751 to 0.02682, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02682 to 0.02504, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02504 to 0.02161, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02161 to 0.01973, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01973 to 0.01793, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01793 to 0.01640, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01640 to 0.01603, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.01606, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01603 to 0.01291, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01291 to 0.01266, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01266 to 0.01160, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01160 to 0.01050, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01050 to 0.00992, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00992 to 0.00990, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00990 to 0.00912, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00912 to 0.00868, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00868 to 0.00816, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00816 to 0.00797, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00940, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00797 to 0.00761, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00761 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00747 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00717 to 0.00692, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00692 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00668 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00662 to 0.00653, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00653 to 0.00636, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00636 to 0.00611, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00640, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00611 to 0.00603, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00603 to 0.00593, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00593 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00587 to 0.00580, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00580 to 0.00577, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00981, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00577 to 0.00568, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00568 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00556 to 0.00552, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00552 to 0.00543, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00543 to 0.00540, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00549, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00211: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.00540 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00271: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00539 to 0.00538, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.00538 to 0.00533, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00533 to 0.00531, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00561, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00376: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00583, did not improve\n",
      "Epoch 00423: early stopping\n",
      "Using epoch 00348 with val_loss: 0.00531\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02537, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02537 to 0.02465, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.02704, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02465 to 0.02350, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02350 to 0.02265, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02265 to 0.02066, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02066 to 0.01932, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01932 to 0.01857, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01857 to 0.01747, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01936, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01747 to 0.01606, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01606 to 0.01485, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01485 to 0.01390, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01390 to 0.01268, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01268 to 0.01246, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01246 to 0.01209, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01209 to 0.01086, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01086 to 0.01023, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01023 to 0.00972, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00972 to 0.00903, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00903 to 0.00868, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00868 to 0.00835, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00835 to 0.00826, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00826 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00762 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00882, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00762 to 0.00730, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00730 to 0.00688, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00688 to 0.00686, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00686 to 0.00660, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00660 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00647 to 0.00641, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00641 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00599 to 0.00572, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00572 to 0.00553, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00700, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00100: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00553 to 0.00511, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00511 to 0.00510, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00510 to 0.00503, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00503 to 0.00489, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00489 to 0.00479, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00479 to 0.00475, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00483, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00495, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00485, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00495, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00499, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00509, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00482, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00498, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00494, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00483, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00497, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00523, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00546, did not improve\n",
      "Epoch 00257: early stopping\n",
      "Using epoch 00182 with val_loss: 0.00475\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00577] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00724]\n",
      " [ 0.00531]\n",
      " [ 0.00475]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00176] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00214]\n",
      " [ 0.00113]\n",
      " [ 0.002  ]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True)\n",
    "t.scatter_plot(Y, res['y_pred'], res['mse'], 'mlp earlystop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "create mlp using Dropout\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04859, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04859 to 0.04643, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04711, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04643 to 0.04581, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.04680, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04581 to 0.04379, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04379 to 0.04234, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.04240, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04234 to 0.03941, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03941 to 0.03765, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.03908, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.03817, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03765 to 0.03222, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03222 to 0.03172, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03172 to 0.02978, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02978 to 0.02764, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02764 to 0.02627, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02627 to 0.02562, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02562 to 0.02436, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02436 to 0.02308, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02308 to 0.02295, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02295 to 0.02083, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02083 to 0.01998, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01998 to 0.01988, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01988 to 0.01834, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01834 to 0.01709, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.01759, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01709 to 0.01665, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01665 to 0.01381, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01589, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.01567, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01381 to 0.01297, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01297 to 0.01267, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01267 to 0.01166, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.01166 to 0.01092, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01092 to 0.01011, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.01521, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.01011 to 0.00896, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00896 to 0.00879, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.01020, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01007, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00879 to 0.00856, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.01260, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00856 to 0.00810, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00810 to 0.00809, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00889, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00962, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00977, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.01026, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00964, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00936, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00809 to 0.00783, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00872, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00978, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00783 to 0.00778, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00864, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.01183, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00908, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00813, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00778 to 0.00776, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00776 to 0.00744, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00923, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00842, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00744 to 0.00720, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00923, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00879, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01009, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00720 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00888, did not improve\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00702 to 0.00697, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00697 to 0.00690, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00907, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00907, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00966, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01036, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00872, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00908, did not improve\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00690 to 0.00685, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00964, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00685 to 0.00677, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00970, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00677 to 0.00673, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00950, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00878, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00762, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00912, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01011, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01049, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00977, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00870, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00976, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00896, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00968, did not improve\n",
      "Epoch 00339: early stopping\n",
      "Using epoch 00264 with val_loss: 0.00673\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02977, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02977 to 0.02897, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02897 to 0.02839, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.03257, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02839 to 0.02767, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02767 to 0.02567, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02567 to 0.02418, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02418 to 0.02340, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02340 to 0.02186, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02186 to 0.02073, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02073 to 0.01956, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01956 to 0.01937, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01937 to 0.01770, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01770 to 0.01691, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01691 to 0.01629, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01629 to 0.01578, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01578 to 0.01474, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01474 to 0.01408, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01408 to 0.01325, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01425, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01325 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01231 to 0.01188, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01188 to 0.01136, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01136 to 0.01130, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01561, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01130 to 0.00986, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00986 to 0.00968, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00968 to 0.00925, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00932, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00925 to 0.00862, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00862 to 0.00844, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00844 to 0.00815, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00815 to 0.00796, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00870, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00796 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00758 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00758 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00888, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00758 to 0.00752, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00752 to 0.00735, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00936, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00735 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00699 to 0.00691, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00691 to 0.00688, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00688 to 0.00653, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00714, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00103: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00653 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00633 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00618 to 0.00595, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00595 to 0.00579, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00579 to 0.00567, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00567 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00573, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00263: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.00554 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00271: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.00549 to 0.00541, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00907, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00597, did not improve\n",
      "Epoch 00368: early stopping\n",
      "Using epoch 00293 with val_loss: 0.00541\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02707, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02707 to 0.02571, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.02600, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02571 to 0.02500, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02645, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02500 to 0.02447, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02447 to 0.02209, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02209 to 0.02047, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02047 to 0.02042, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02042 to 0.01909, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01909 to 0.01850, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01850 to 0.01826, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01826 to 0.01723, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.01794, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01723 to 0.01656, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01656 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01556 to 0.01469, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01469 to 0.01406, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01406 to 0.01406, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01406 to 0.01314, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.01315, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01314 to 0.01171, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01171 to 0.01153, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01153 to 0.01046, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01046 to 0.00989, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00989 to 0.00967, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00967 to 0.00922, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00922 to 0.00915, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00915 to 0.00859, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00859 to 0.00856, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00856 to 0.00820, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00820 to 0.00813, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00813 to 0.00781, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00789, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00781 to 0.00778, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00778 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00984, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00747 to 0.00744, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00901, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00744 to 0.00731, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00731 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00872, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00684 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00814, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00679 to 0.00630, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00630 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.01550, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00587 to 0.00553, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00711, did not improve\n",
      "Epoch 00199: early stopping\n",
      "Using epoch 00124 with val_loss: 0.00553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00589] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00673]\n",
      " [ 0.00541]\n",
      " [ 0.00553]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00242] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00186]\n",
      " [ 0.00183]\n",
      " [ 0.00357]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "evaluating with early stopping\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09025, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09025 to 0.08450, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08450 to 0.08431, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08431 to 0.08385, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08385 to 0.08373, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08373 to 0.08253, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08253 to 0.08147, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.08173, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.08276, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08147 to 0.07972, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.08052, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.07993, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.07972 to 0.07894, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07894 to 0.07759, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.07844, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07759 to 0.07678, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07678 to 0.07413, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.07462, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07413 to 0.07285, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07285 to 0.07154, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.07154 to 0.07080, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.07080 to 0.06988, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06988 to 0.06839, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06839 to 0.06747, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06747 to 0.06688, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06688 to 0.06404, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.06404 to 0.06263, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.06263 to 0.06097, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.06097 to 0.05985, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.06038, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05985 to 0.05769, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05769 to 0.05732, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.05732 to 0.05590, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05590 to 0.05421, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05421 to 0.05399, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.05399 to 0.05240, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.05240 to 0.05096, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.05096 to 0.05062, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.05062 to 0.04945, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04945 to 0.04874, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04874 to 0.04835, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04835 to 0.04750, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04750 to 0.04612, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.04685, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.04612 to 0.04595, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04595 to 0.04471, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.04471 to 0.04442, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.04442 to 0.04347, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04347 to 0.04307, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.04307 to 0.04269, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.04269 to 0.04190, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.04190 to 0.04115, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.04125, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.04115 to 0.04106, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.04106 to 0.03973, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.03997, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03973 to 0.03962, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03962 to 0.03869, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03869 to 0.03842, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03842 to 0.03832, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03832 to 0.03817, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.03817 to 0.03708, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.03761, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.03708 to 0.03672, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.03763, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.03672 to 0.03663, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03663 to 0.03589, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03589 to 0.03578, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.03578 to 0.03562, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.03562 to 0.03526, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03526 to 0.03493, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03493 to 0.03471, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.03471 to 0.03419, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.03490, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.03427, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.03419 to 0.03377, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.03451, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.03377 to 0.03332, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss is 0.03350, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.03346, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.03350, did not improve\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.03332 to 0.03220, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.03276, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.03220 to 0.03174, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.03185, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.03174 to 0.03130, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.03142, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.03134, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.03130 to 0.03089, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.03089 to 0.03054, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.03084, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.03072, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.03085, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.03054 to 0.03033, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.03033 to 0.03003, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.03036, did not improve\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.03003 to 0.02971, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02971 to 0.02941, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.02941 to 0.02885, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.02900, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.02885 to 0.02861, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.02865, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02861 to 0.02834, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.02837, did not improve\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.02834 to 0.02797, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.02909, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.02797 to 0.02792, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.02792 to 0.02778, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.02809, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.02778 to 0.02720, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss is 0.02749, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.02738, did not improve\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.02720 to 0.02715, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.02715 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.02692 to 0.02654, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.02680, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02654 to 0.02623, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.02623 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.02632, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02604 to 0.02572, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02572 to 0.02571, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.02581, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02571 to 0.02533, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.02550, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00125: val_loss improved from 0.02533 to 0.02502, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.02508, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02502 to 0.02492, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.02494, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.02500, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.02492 to 0.02484, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.02484 to 0.02481, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02481 to 0.02430, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.02460, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.02430 to 0.02396, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.02401, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.02396 to 0.02368, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.02368 to 0.02358, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.02365, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.02390, did not improve\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02358 to 0.02320, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss is 0.02429, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.02368, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.02344, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.02330, did not improve\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.02320 to 0.02278, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.02278 to 0.02259, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02259 to 0.02250, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.02250 to 0.02244, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.02244 to 0.02231, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.02231 to 0.02219, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.02235, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.02219 to 0.02190, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.02190 to 0.02187, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.02240, did not improve\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.02187 to 0.02179, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.02179 to 0.02176, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.02176 to 0.02150, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss is 0.02167, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.02170, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.02150 to 0.02111, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.02117, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.02111 to 0.02088, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.02088 to 0.02082, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss is 0.02091, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.02082 to 0.02079, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.02079 to 0.02058, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.02061, did not improve\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.02058 to 0.02044, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.02044 to 0.02035, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.02035 to 0.02022, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.02022 to 0.02007, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.02038, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.02007 to 0.01985, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01985 to 0.01977, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss is 0.02001, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01977 to 0.01974, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss is 0.02037, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.01974 to 0.01967, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.01972, did not improve\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.01967 to 0.01952, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.01952 to 0.01950, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01950 to 0.01918, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.01927, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.01991, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01933, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01918 to 0.01902, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.01902 to 0.01877, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss is 0.01903, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01887, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.01877 to 0.01859, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.01890, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.01893, did not improve\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.01859 to 0.01836, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss is 0.01836, did not improve\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.01836 to 0.01824, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.01824 to 0.01818, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01818 to 0.01816, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.01816 to 0.01796, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.01796 to 0.01785, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.01785 to 0.01782, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.01782 to 0.01776, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.01776 to 0.01763, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.01790, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.01767, did not improve\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.01763 to 0.01744, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss is 0.01744, did not improve\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.01744 to 0.01734, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss is 0.01737, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01741, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.01734 to 0.01716, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.01716 to 0.01711, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss is 0.01734, did not improve\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.01711 to 0.01704, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.01704 to 0.01685, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss is 0.01692, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01690, did not improve\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.01685 to 0.01679, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.01679 to 0.01674, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.01751, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.01674 to 0.01664, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss is 0.01696, did not improve\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.01664 to 0.01656, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss is 0.01657, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.01674, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01665, did not improve\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.01656 to 0.01611, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.01614, did not improve\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.01611 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss is 0.01622, did not improve\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.01605 to 0.01595, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.01595 to 0.01594, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss is 0.01597, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01628, did not improve\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.01594 to 0.01582, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss is 0.01611, did not improve\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.01582 to 0.01565, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss is 0.01640, did not improve\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.01565 to 0.01565, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.01565 to 0.01559, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.01559 to 0.01548, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss is 0.01690, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01603, did not improve\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.01548 to 0.01533, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.01533 to 0.01528, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss is 0.01532, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01548, did not improve\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.01528 to 0.01527, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss is 0.01539, did not improve\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.01527 to 0.01502, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss is 0.01511, did not improve\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.01502 to 0.01497, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.01497 to 0.01491, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00253: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.01491 to 0.01483, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.01483 to 0.01472, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.01494, did not improve\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.01472 to 0.01463, storing weights.\n",
      "\n",
      "Epoch 00258: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.01463 to 0.01461, storing weights.\n",
      "\n",
      "Epoch 00260: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01520, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01467, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.01461 to 0.01436, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss is 0.01457, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01450, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.01505, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.01436 to 0.01416, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.01425, did not improve\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.01416 to 0.01407, storing weights.\n",
      "\n",
      "Epoch 00270: val_loss is 0.01410, did not improve\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.01407 to 0.01403, storing weights.\n",
      "\n",
      "Epoch 00272: val_loss is 0.01448, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01419, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.01515, did not improve\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.01403 to 0.01401, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.01401 to 0.01390, storing weights.\n",
      "\n",
      "Epoch 00277: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.01390 to 0.01373, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.01373 to 0.01373, storing weights.\n",
      "\n",
      "Epoch 00280: val_loss is 0.01381, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.01468, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01378, did not improve\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.01373 to 0.01371, storing weights.\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.01371 to 0.01358, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.01358 to 0.01358, storing weights.\n",
      "\n",
      "Epoch 00286: val_loss is 0.01358, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01377, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.01358 to 0.01341, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.01356, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.01341 to 0.01338, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.01344, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01355, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01364, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.01338 to 0.01307, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss is 0.01311, did not improve\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.01307 to 0.01303, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01311, did not improve\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.01303 to 0.01302, storing weights.\n",
      "\n",
      "Epoch 00305: val_loss is 0.01458, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.01302 to 0.01302, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss is 0.01305, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01323, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.01302 to 0.01289, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.01289 to 0.01274, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01297, did not improve\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.01274 to 0.01269, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss is 0.01273, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.01269 to 0.01265, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01334, did not improve\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.01265 to 0.01263, storing weights.\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.01263 to 0.01257, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.01257 to 0.01250, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01265, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01314, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01306, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.01250 to 0.01240, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.01240 to 0.01234, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01236, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01254, did not improve\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.01234 to 0.01233, storing weights.\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.01233 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss is 0.01243, did not improve\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.01224 to 0.01219, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss is 0.01271, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.01219 to 0.01213, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.01213 to 0.01206, storing weights.\n",
      "\n",
      "Epoch 00341: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01224, did not improve\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.01206 to 0.01205, storing weights.\n",
      "\n",
      "Epoch 00344: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.01205 to 0.01200, storing weights.\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.01200 to 0.01195, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.01195 to 0.01192, storing weights.\n",
      "\n",
      "Epoch 00350: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01195, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.01192 to 0.01183, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.01183 to 0.01172, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01183, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01297, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01209, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.01172 to 0.01161, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.01173, did not improve\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.01161 to 0.01159, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.01159 to 0.01148, storing weights.\n",
      "\n",
      "Epoch 00379: val_loss is 0.01242, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01151, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.01148 to 0.01146, storing weights.\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.01146 to 0.01143, storing weights.\n",
      "\n",
      "Epoch 00386: val_loss improved from 0.01143 to 0.01142, storing weights.\n",
      "\n",
      "Epoch 00387: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.01142 to 0.01137, storing weights.\n",
      "\n",
      "Epoch 00393: val_loss improved from 0.01137 to 0.01135, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00394: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00396: val_loss improved from 0.01135 to 0.01120, storing weights.\n",
      "\n",
      "Epoch 00397: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01137, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00407: val_loss improved from 0.01120 to 0.01115, storing weights.\n",
      "\n",
      "Epoch 00408: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01131, did not improve\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.01115 to 0.01103, storing weights.\n",
      "\n",
      "Epoch 00412: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.01103 to 0.01101, storing weights.\n",
      "\n",
      "Epoch 00417: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.01101 to 0.01101, storing weights.\n",
      "\n",
      "Epoch 00422: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.01101 to 0.01098, storing weights.\n",
      "\n",
      "Epoch 00428: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.01098 to 0.01090, storing weights.\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.01090 to 0.01088, storing weights.\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.01088 to 0.01085, storing weights.\n",
      "\n",
      "Epoch 00433: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.01085 to 0.01084, storing weights.\n",
      "\n",
      "Epoch 00435: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.01084 to 0.01083, storing weights.\n",
      "\n",
      "Epoch 00443: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.01083 to 0.01083, storing weights.\n",
      "\n",
      "Epoch 00445: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.01083 to 0.01083, storing weights.\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.01083 to 0.01080, storing weights.\n",
      "\n",
      "Epoch 00450: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.01080 to 0.01079, storing weights.\n",
      "\n",
      "Epoch 00459: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.01079 to 0.01073, storing weights.\n",
      "\n",
      "Epoch 00465: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.01084, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.01073 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00488: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00502: val_loss improved from 0.01068 to 0.01067, storing weights.\n",
      "\n",
      "Epoch 00503: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.01084, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.01067 to 0.01067, storing weights.\n",
      "\n",
      "Epoch 00541: val_loss improved from 0.01067 to 0.01064, storing weights.\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.01064 to 0.01061, storing weights.\n",
      "\n",
      "Epoch 00543: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.01087, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00546: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.01068, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.01068, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00601: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00607: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00608: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.01114, did not improve\n",
      "Epoch 00617: early stopping\n",
      "Using epoch 00542 with val_loss: 0.01061\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06725, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06725 to 0.06722, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.08102, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06722 to 0.06536, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06536 to 0.06498, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06498 to 0.06407, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06407 to 0.06350, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06350 to 0.06261, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.06292, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06261 to 0.06116, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06116 to 0.06034, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06034 to 0.05953, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05953 to 0.05842, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05842 to 0.05741, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05741 to 0.05630, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05630 to 0.05516, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05516 to 0.05415, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05415 to 0.05362, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05362 to 0.05199, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05199 to 0.05096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05096 to 0.05056, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05056 to 0.04991, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.04991 to 0.04909, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04909 to 0.04804, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.04804 to 0.04711, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.05892, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.04711 to 0.04554, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.05247, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.05733, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04554 to 0.04424, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04424 to 0.04384, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04384 to 0.04241, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04241 to 0.04189, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.05071, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04189 to 0.04106, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04106 to 0.04081, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04081 to 0.04036, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04036 to 0.03967, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.03967 to 0.03931, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.05746, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.04141, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.04058, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03931 to 0.03809, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03809 to 0.03754, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03754 to 0.03738, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03738 to 0.03699, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03699 to 0.03662, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03662 to 0.03643, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03643 to 0.03614, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03614 to 0.03558, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.03563, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.03965, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.03715, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03558 to 0.03546, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.03660, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.03546 to 0.03427, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.03427 to 0.03377, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.03398, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03377 to 0.03339, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03339 to 0.03338, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03338 to 0.03337, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.04059, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.04591, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.03337 to 0.03218, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.03389, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.04335, did not improve\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03218 to 0.03189, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03189 to 0.03126, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss is 0.03297, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.03126 to 0.03085, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.03189, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03085 to 0.03052, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss is 0.03094, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.03165, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.03809, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.03052 to 0.02958, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.02958 to 0.02955, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.02955 to 0.02910, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02910 to 0.02891, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02891 to 0.02855, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss is 0.02888, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.03040, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02859, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.02855 to 0.02837, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02837 to 0.02803, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.02803 to 0.02762, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.02766, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02806, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.02762 to 0.02735, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02735 to 0.02721, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.02737, did not improve\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02721 to 0.02680, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.02691, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.02680 to 0.02646, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.02646 to 0.02619, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.02649, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.02842, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.02619 to 0.02564, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.02570, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.04414, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.02658, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.02572, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02564 to 0.02528, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.03854, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.03203, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.02528 to 0.02458, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.02458 to 0.02428, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.02428 to 0.02408, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.02424, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.02420, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02408 to 0.02406, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.04048, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.02436, did not improve\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.02406 to 0.02394, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.02394 to 0.02328, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.02377, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.02489, did not improve\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.02328 to 0.02305, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.02307, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02305 to 0.02266, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.02266 to 0.02259, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02259 to 0.02247, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.02592, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.02247 to 0.02244, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.02277, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.02276, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.02244 to 0.02231, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss is 0.02248, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.02344, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.02795, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02231 to 0.02227, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.02227 to 0.02143, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.02192, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.02162, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.03307, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.02151, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.02143 to 0.02094, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.02252, did not improve\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02094 to 0.02092, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.02092 to 0.02085, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.02085 to 0.02046, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss is 0.02064, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.02046 to 0.02042, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.02042 to 0.02025, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.02025 to 0.02019, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02019 to 0.01990, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.01990 to 0.01969, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.01982, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.02005, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.01969 to 0.01952, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.02894, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.01952 to 0.01941, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.01941 to 0.01912, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.01912 to 0.01882, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.01882 to 0.01877, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.01915, did not improve\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.01877 to 0.01870, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.01870 to 0.01859, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.01859 to 0.01845, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.01947, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01858, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.01853, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.02296, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.01857, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.02125, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.01845 to 0.01791, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.01791 to 0.01785, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.01785 to 0.01779, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.01783, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.01800, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01816, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.01897, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.03894, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.01887, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.01835, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01786, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.02350, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.03325, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.01797, did not improve\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.01779 to 0.01730, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss is 0.01782, did not improve\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01730 to 0.01697, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss is 0.01703, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01733, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01697 to 0.01687, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.01687 to 0.01672, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss is 0.01746, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.02763, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.01708, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01877, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.01672 to 0.01636, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.01652, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.01636 to 0.01619, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.01619 to 0.01603, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss is 0.01615, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01615, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.01623, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.01605, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01607, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01647, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.01625, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.01630, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.01603 to 0.01577, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00205: val_loss improved from 0.01577 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss is 0.01674, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.01584, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.01556 to 0.01517, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.01534, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01676, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01659, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01563, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.01560, did not improve\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.01517 to 0.01509, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.01509 to 0.01464, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss is 0.01483, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01526, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01582, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.01469, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.01464 to 0.01439, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss is 0.01566, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.01466, did not improve\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.01439 to 0.01437, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss is 0.01477, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01492, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.01465, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01467, did not improve\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.01437 to 0.01436, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.01436 to 0.01422, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01516, did not improve\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.01422 to 0.01413, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.01413 to 0.01368, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01384, did not improve\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.01368 to 0.01356, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.04679, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01426, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01410, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.02419, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01388, did not improve\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.01356 to 0.01311, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.01340, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01434, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01374, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.01350, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.01321, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.01388, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.01325, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.01477, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01420, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01314, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01376, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01334, did not improve\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.01311 to 0.01302, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss is 0.02512, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.01470, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01383, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.01351, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.01325, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01333, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.01302 to 0.01296, storing weights.\n",
      "\n",
      "Epoch 00273: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01305, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01374, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.02438, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.01296 to 0.01252, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.01259, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01384, did not improve\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.01252 to 0.01247, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.01247 to 0.01227, storing weights.\n",
      "\n",
      "Epoch 00283: val_loss is 0.01283, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01784, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01514, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01297, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.01239, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.05228, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.01787, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01345, did not improve\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.01227 to 0.01225, storing weights.\n",
      "\n",
      "Epoch 00293: val_loss is 0.01297, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.01225 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00296: val_loss is 0.01363, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01253, did not improve\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.01224 to 0.01214, storing weights.\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.01214 to 0.01213, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.01213 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01469, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.01202 to 0.01189, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01211, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01211, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01340, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.01201, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.01915, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.01189 to 0.01178, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.04621, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.01178 to 0.01178, storing weights.\n",
      "\n",
      "Epoch 00326: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.01178 to 0.01159, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01211, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.03501, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.06627, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01409, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.03787, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.03056, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01229, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01739, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01195, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01209, did not improve\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.01159 to 0.01120, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.01120 to 0.01116, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss is 0.01187, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00354: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.01116 to 0.01103, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.01103 to 0.01090, storing weights.\n",
      "\n",
      "Epoch 00359: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01289, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01276, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.01090 to 0.01086, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.01086 to 0.01086, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.01086 to 0.01067, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01171, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.02276, did not improve\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.01067 to 0.01034, storing weights.\n",
      "\n",
      "Epoch 00390: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.02317, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.01637, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01583, did not improve\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.01034 to 0.01011, storing weights.\n",
      "\n",
      "Epoch 00401: val_loss is 0.01030, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01052, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01039, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01323, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01604, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01531, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.01049, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.02286, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.02029, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01726, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.01050, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01050, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.01035, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.01046, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.01277, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.01022, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.01028, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.01045, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.01182, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.01426, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.01036, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.01049, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.02146, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01377, did not improve\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.01011 to 0.01010, storing weights.\n",
      "\n",
      "Epoch 00463: val_loss is 0.02245, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01022, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.01307, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.01052, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.05852, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.01035, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.01247, did not improve\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.01010 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00484: val_loss improved from 0.00993 to 0.00990, storing weights.\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.00990 to 0.00986, storing weights.\n",
      "\n",
      "Epoch 00486: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.01026, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.01046, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.01024, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00991, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.01009, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00998, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.01014, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.01018, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.01013, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.01058, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00510: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.01004, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.01049, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.01018, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.01029, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.01012, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.01043, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.01004, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.01016, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.01011, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.01011, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.01015, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.01036, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.02149, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.01003, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.01190, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.01031, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.01008, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00994, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.01027, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.01012, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.01006, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.01009, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00999, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.01680, did not improve\n",
      "Epoch 00560: early stopping\n",
      "Using epoch 00485 with val_loss: 0.00986\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06332, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06332 to 0.06280, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.06294, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06280 to 0.06176, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.06295, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06176 to 0.06061, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06061 to 0.06038, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06038 to 0.05938, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05938 to 0.05875, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.06065, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05875 to 0.05715, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.07804, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05715 to 0.05519, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05519 to 0.05444, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05444 to 0.05357, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05357 to 0.05280, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.05509, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05280 to 0.05151, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05151 to 0.05032, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05032 to 0.04966, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04966 to 0.04885, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.04885 to 0.04827, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.04928, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04827 to 0.04705, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.04705 to 0.04637, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.04637 to 0.04577, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.04578, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.04924, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.04577 to 0.04458, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04458 to 0.04380, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04380 to 0.04319, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04319 to 0.04298, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04298 to 0.04151, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04151 to 0.04118, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04118 to 0.04097, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04097 to 0.04060, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04060 to 0.03989, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.03989 to 0.03962, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.03962 to 0.03885, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03885 to 0.03872, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03872 to 0.03836, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.07853, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.05895, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.04204, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03836 to 0.03785, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03785 to 0.03710, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03710 to 0.03662, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03662 to 0.03626, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03626 to 0.03608, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03608 to 0.03601, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03601 to 0.03541, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.03721, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.04043, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03541 to 0.03504, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.03511, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.05323, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.04760, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03504 to 0.03396, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.03500, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03396 to 0.03387, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03387 to 0.03346, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.03346 to 0.03314, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.03347, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.05725, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.03397, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.03318, did not improve\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03314 to 0.03216, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03216 to 0.03201, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.03201 to 0.03180, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.03180 to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03170 to 0.03157, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03157 to 0.03152, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.03152 to 0.03116, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.03116 to 0.03106, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.03203, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.03123, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.03106 to 0.03031, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.03031 to 0.02993, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02993 to 0.02984, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.03046, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.05624, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.03057, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.03192, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.02984 to 0.02942, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.02979, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_loss is 0.03006, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.05379, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02942 to 0.02903, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.02906, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02903 to 0.02810, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02810 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.02866, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02763, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.05178, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.02754 to 0.02720, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.05273, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.02986, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02720 to 0.02656, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.05537, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.03493, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02656 to 0.02525, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.02571, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.02554, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.02626, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.02610, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.04667, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.02551, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.02757, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.02525 to 0.02521, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.02521 to 0.02462, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.02593, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.02498, did not improve\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.02462 to 0.02454, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss is 0.02487, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.02490, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02454 to 0.02440, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.02440 to 0.02376, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.02383, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.02433, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.02456, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.02612, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02376 to 0.02313, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.03047, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.02313 to 0.02270, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.02292, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.02277, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.02316, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.02283, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.02293, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.02289, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02270 to 0.02265, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.02265 to 0.02175, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.02183, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.02196, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.02229, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.02175 to 0.02149, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.02149 to 0.02085, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.02209, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.02143, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.07646, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.02211, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.02113, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.02173, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.04232, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.02085 to 0.02063, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.02063 to 0.02057, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.02097, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.02788, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.02057 to 0.01959, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.02012, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.02009, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.01998, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.01976, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.04870, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.02027, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.02075, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.01959 to 0.01954, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.01977, did not improve\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.01954 to 0.01943, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss is 0.02013, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.01984, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.01943 to 0.01912, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.01980, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.01912 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.01887, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01956, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.01901, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.01963, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.01914, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01914, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.01869 to 0.01862, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01862 to 0.01793, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss is 0.01860, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.01859, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01951, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01824, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.01836, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.04678, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.02228, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.01990, did not improve\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01793 to 0.01749, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss is 0.01802, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01751, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.01780, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.01832, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.01749 to 0.01707, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.01707 to 0.01699, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss is 0.01704, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01703, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.01745, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.01762, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.01699 to 0.01689, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss is 0.01734, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.01689 to 0.01676, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01676 to 0.01672, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss is 0.01724, did not improve\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.01672 to 0.01636, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.01636 to 0.01599, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.01599 to 0.01597, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.01617, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.01672, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.01608, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01652, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.01597 to 0.01569, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.01569 to 0.01567, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.01655, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01873, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01640, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01628, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.01576, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01655, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.04668, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01718, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01616, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01600, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.01652, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.01567 to 0.01522, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss is 0.05118, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.01649, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01592, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.01522 to 0.01502, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss is 0.01648, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.01556, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01530, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01554, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.01502, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.01699, did not improve\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.01502 to 0.01479, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00232: val_loss is 0.01496, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01497, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01537, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.01479 to 0.01410, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.01438, did not improve\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.01410 to 0.01397, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.01468, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.01516, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.02310, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01398, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01523, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01459, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01581, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.01596, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01403, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01481, did not improve\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.01397 to 0.01358, storing weights.\n",
      "\n",
      "Epoch 00251: val_loss is 0.01451, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.01410, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01363, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.01475, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.01358 to 0.01331, storing weights.\n",
      "\n",
      "Epoch 00257: val_loss is 0.01519, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01343, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01344, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01398, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.01391, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.03566, did not improve\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.01331 to 0.01309, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.01309 to 0.01222, storing weights.\n",
      "\n",
      "Epoch 00267: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.01222 to 0.01211, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss is 0.01337, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.04401, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01261, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01367, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01413, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.01277, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.01222, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.01211 to 0.01189, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss is 0.01224, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01249, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01226, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.01332, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01201, did not improve\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.01189 to 0.01161, storing weights.\n",
      "\n",
      "Epoch 00296: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01263, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.01161 to 0.01139, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.03117, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01440, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.01341, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01223, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.01234, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.01139 to 0.01125, storing weights.\n",
      "\n",
      "Epoch 00318: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.04544, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.01371, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01342, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01448, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.03079, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01439, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01324, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01229, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.03846, did not improve\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.01125 to 0.01121, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.01121 to 0.01079, storing weights.\n",
      "\n",
      "Epoch 00336: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.01079 to 0.01076, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.02187, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.01076 to 0.01074, storing weights.\n",
      "\n",
      "Epoch 00356: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.03182, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.08361, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.01074 to 0.01071, storing weights.\n",
      "\n",
      "Epoch 00361: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.01071 to 0.01029, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss is 0.01043, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.01029 to 0.01021, storing weights.\n",
      "\n",
      "Epoch 00368: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.01021 to 0.00994, storing weights.\n",
      "\n",
      "Epoch 00370: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01042, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01291, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.01383, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.03490, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.04227, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01020, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01006, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00388: val_loss improved from 0.00994 to 0.00981, storing weights.\n",
      "\n",
      "Epoch 00389: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01049, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01026, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01011, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.01486, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01027, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01029, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.00981 to 0.00952, storing weights.\n",
      "\n",
      "Epoch 00403: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01008, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00957, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01007, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.03492, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01251, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01236, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.03011, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01004, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00999, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.03011, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00993, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01317, did not improve\n",
      "\n",
      "Epoch 00434: val_loss improved from 0.00952 to 0.00935, storing weights.\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.00935 to 0.00911, storing weights.\n",
      "\n",
      "Epoch 00436: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01024, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00950, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00943, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00970, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01417, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00944, did not improve\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.00911 to 0.00895, storing weights.\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.00895 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00448: val_loss is 0.01151, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.05730, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00954, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00934, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01027, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.03499, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00938, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.03757, did not improve\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.00887 to 0.00852, storing weights.\n",
      "\n",
      "Epoch 00474: val_loss is 0.01003, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00974, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01024, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00957, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00922, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00963, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.05344, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.02565, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.01262, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00943, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.01886, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00907, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00904, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01012, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00994, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00960, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01617, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00938, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00962, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00978, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00957, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01884, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.01045, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.05451, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00957, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00906, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00901, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00876, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01722, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00946, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.01654, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.01034, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.01949, did not improve\n",
      "\n",
      "Epoch 00547: val_loss improved from 0.00852 to 0.00848, storing weights.\n",
      "\n",
      "Epoch 00548: val_loss is 0.02922, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.01375, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00550: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.01022, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00943, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.01526, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.01043, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.01623, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00933, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00947, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00918, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.01205, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.00962, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.06245, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.01312, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00933, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.04519, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00970, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.01722, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00919, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.00892, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.00947, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.00889, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.03890, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00601: val_loss is 0.00936, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.00875, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.01111, did not improve\n",
      "\n",
      "Epoch 00607: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00608: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.01039, did not improve\n",
      "\n",
      "Epoch 00611: val_loss improved from 0.00848 to 0.00843, storing weights.\n",
      "\n",
      "Epoch 00612: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.00970, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.00912, did not improve\n",
      "\n",
      "Epoch 00618: val_loss is 0.00892, did not improve\n",
      "\n",
      "Epoch 00619: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.00970, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.00843 to 0.00829, storing weights.\n",
      "\n",
      "Epoch 00626: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00627: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00628: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00629: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00630: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00631: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00632: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00633: val_loss is 0.04643, did not improve\n",
      "\n",
      "Epoch 00634: val_loss is 0.05218, did not improve\n",
      "\n",
      "Epoch 00635: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00636: val_loss is 0.01841, did not improve\n",
      "\n",
      "Epoch 00637: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00638: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00639: val_loss is 0.01454, did not improve\n",
      "\n",
      "Epoch 00640: val_loss is 0.00842, did not improve\n",
      "\n",
      "Epoch 00641: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00642: val_loss is 0.01216, did not improve\n",
      "\n",
      "Epoch 00643: val_loss improved from 0.00829 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00644: val_loss is 0.00910, did not improve\n",
      "\n",
      "Epoch 00645: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00646: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00647: val_loss is 0.03371, did not improve\n",
      "\n",
      "Epoch 00648: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00649: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00650: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00651: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00652: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00653: val_loss is 0.00904, did not improve\n",
      "\n",
      "Epoch 00654: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00655: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00656: val_loss improved from 0.00817 to 0.00813, storing weights.\n",
      "\n",
      "Epoch 00657: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00658: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00659: val_loss is 0.00907, did not improve\n",
      "\n",
      "Epoch 00660: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00661: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00662: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00663: val_loss is 0.00908, did not improve\n",
      "\n",
      "Epoch 00664: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00665: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00666: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00667: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00668: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00669: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00670: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00671: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00672: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00673: val_loss is 0.06517, did not improve\n",
      "\n",
      "Epoch 00674: val_loss is 0.00983, did not improve\n",
      "\n",
      "Epoch 00675: val_loss is 0.00939, did not improve\n",
      "\n",
      "Epoch 00676: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00677: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00678: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00679: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00680: val_loss is 0.00999, did not improve\n",
      "\n",
      "Epoch 00681: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00682: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00683: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00684: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00685: val_loss is 0.00901, did not improve\n",
      "\n",
      "Epoch 00686: val_loss is 0.00901, did not improve\n",
      "\n",
      "Epoch 00687: val_loss is 0.03782, did not improve\n",
      "\n",
      "Epoch 00688: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00689: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00690: val_loss is 0.04947, did not improve\n",
      "\n",
      "Epoch 00691: val_loss is 0.00947, did not improve\n",
      "\n",
      "Epoch 00692: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00693: val_loss is 0.00882, did not improve\n",
      "\n",
      "Epoch 00694: val_loss is 0.02796, did not improve\n",
      "\n",
      "Epoch 00695: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00696: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00697: val_loss is 0.06679, did not improve\n",
      "\n",
      "Epoch 00698: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00699: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00700: val_loss is 0.06180, did not improve\n",
      "\n",
      "Epoch 00701: val_loss is 0.00918, did not improve\n",
      "\n",
      "Epoch 00702: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00703: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00704: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00705: val_loss improved from 0.00813 to 0.00813, storing weights.\n",
      "\n",
      "Epoch 00706: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00707: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00708: val_loss is 0.01217, did not improve\n",
      "\n",
      "Epoch 00709: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00710: val_loss is 0.00906, did not improve\n",
      "\n",
      "Epoch 00711: val_loss is 0.00933, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00712: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00713: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00714: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00715: val_loss is 0.00965, did not improve\n",
      "\n",
      "Epoch 00716: val_loss is 0.02074, did not improve\n",
      "\n",
      "Epoch 00717: val_loss is 0.04158, did not improve\n",
      "\n",
      "Epoch 00718: val_loss improved from 0.00813 to 0.00811, storing weights.\n",
      "\n",
      "Epoch 00719: val_loss improved from 0.00811 to 0.00768, storing weights.\n",
      "\n",
      "Epoch 00720: val_loss is 0.00875, did not improve\n",
      "\n",
      "Epoch 00721: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00722: val_loss is 0.05880, did not improve\n",
      "\n",
      "Epoch 00723: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00724: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00725: val_loss is 0.02134, did not improve\n",
      "\n",
      "Epoch 00726: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00727: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00728: val_loss is 0.00887, did not improve\n",
      "\n",
      "Epoch 00729: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00730: val_loss is 0.01864, did not improve\n",
      "\n",
      "Epoch 00731: val_loss is 0.04589, did not improve\n",
      "\n",
      "Epoch 00732: val_loss is 0.00904, did not improve\n",
      "\n",
      "Epoch 00733: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00734: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00735: val_loss is 0.00870, did not improve\n",
      "\n",
      "Epoch 00736: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00737: val_loss is 0.00998, did not improve\n",
      "\n",
      "Epoch 00738: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00739: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00740: val_loss is 0.00946, did not improve\n",
      "\n",
      "Epoch 00741: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00742: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00743: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00744: val_loss is 0.00963, did not improve\n",
      "\n",
      "Epoch 00745: val_loss is 0.00878, did not improve\n",
      "\n",
      "Epoch 00746: val_loss is 0.00872, did not improve\n",
      "\n",
      "Epoch 00747: val_loss is 0.00938, did not improve\n",
      "\n",
      "Epoch 00748: val_loss is 0.01604, did not improve\n",
      "\n",
      "Epoch 00749: val_loss is 0.00906, did not improve\n",
      "\n",
      "Epoch 00750: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00751: val_loss is 0.00994, did not improve\n",
      "\n",
      "Epoch 00752: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00753: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00754: val_loss is 0.00888, did not improve\n",
      "\n",
      "Epoch 00755: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00756: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00757: val_loss is 0.00912, did not improve\n",
      "\n",
      "Epoch 00758: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00759: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00760: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00761: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00762: val_loss is 0.00878, did not improve\n",
      "\n",
      "Epoch 00763: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00764: val_loss is 0.04222, did not improve\n",
      "\n",
      "Epoch 00765: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00766: val_loss is 0.04503, did not improve\n",
      "\n",
      "Epoch 00767: val_loss is 0.01300, did not improve\n",
      "\n",
      "Epoch 00768: val_loss is 0.01463, did not improve\n",
      "\n",
      "Epoch 00769: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00770: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00771: val_loss is 0.00978, did not improve\n",
      "\n",
      "Epoch 00772: val_loss is 0.00977, did not improve\n",
      "\n",
      "Epoch 00773: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00774: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00775: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00776: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00777: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00778: val_loss is 0.01004, did not improve\n",
      "\n",
      "Epoch 00779: val_loss is 0.01009, did not improve\n",
      "\n",
      "Epoch 00780: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00781: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00782: val_loss is 0.00981, did not improve\n",
      "\n",
      "Epoch 00783: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00784: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00785: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00786: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00787: val_loss is 0.02254, did not improve\n",
      "\n",
      "Epoch 00788: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00789: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00790: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00791: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00792: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00793: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00794: val_loss is 0.00848, did not improve\n",
      "Epoch 00794: early stopping\n",
      "Using epoch 00719 with val_loss: 0.00768\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00564] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.007  ]\n",
      " [ 0.00579]\n",
      " [ 0.00411]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0027] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00324]\n",
      " [ 0.00242]\n",
      " [ 0.00245]]\n"
     ]
    }
   ],
   "source": [
    "# cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'l1': 0.0005, 'l2': 0.0005} \n",
    "# config found by hyperband on L1L2 case\n",
    "cfg = {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
    "m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, L1L2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
      "evaluating with early stopping\n",
      "evaluating with exponential decay\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04587, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.04762, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04587 to 0.04261, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04261 to 0.04231, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.04421, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04231 to 0.04166, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04166 to 0.03874, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03874 to 0.03482, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03482 to 0.03240, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03240 to 0.03028, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03028 to 0.02890, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02890 to 0.02684, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02684 to 0.02647, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02647 to 0.02316, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02316 to 0.02186, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02186 to 0.02009, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02009 to 0.01967, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01967 to 0.01836, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01836 to 0.01728, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01751, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01728 to 0.01496, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.01571, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01496 to 0.01347, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01436, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01347 to 0.01221, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.01412, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01221 to 0.01218, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01218 to 0.01137, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01137 to 0.01132, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01132 to 0.01116, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01116 to 0.00952, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00952 to 0.00916, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00964, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00916 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00908, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00882 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00932, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00854 to 0.00822, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00972, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00822 to 0.00815, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00815 to 0.00806, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00806 to 0.00783, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00783 to 0.00777, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00777 to 0.00769, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00769 to 0.00769, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00769 to 0.00760, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00760 to 0.00759, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00759 to 0.00743, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00795, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00743 to 0.00739, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00739 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00719 to 0.00718, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00718 to 0.00718, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00718 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00715 to 0.00713, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00713 to 0.00710, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00710 to 0.00703, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00703 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00702 to 0.00691, storing weights.\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00691 to 0.00690, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00690 to 0.00683, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00683 to 0.00671, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00671 to 0.00671, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00671 to 0.00665, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00665 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00664 to 0.00663, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00718, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00663 to 0.00654, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00654 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00651 to 0.00646, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00646 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00642 to 0.00637, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00637 to 0.00635, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00635 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00646, did not improve\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00633 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00633 to 0.00630, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00630 to 0.00622, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00622 to 0.00622, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00622 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00618 to 0.00615, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00615 to 0.00614, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.00614 to 0.00610, storing weights.\n",
      "\n",
      "Epoch 00280: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00632, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00296: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00615, did not improve\n",
      "new lr:  0.048749254134\n",
      "\n",
      "Epoch 00301: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00610 to 0.00610, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00610 to 0.00609, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.00609 to 0.00605, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.00605 to 0.00602, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.00602 to 0.00601, storing weights.\n",
      "\n",
      "Epoch 00395: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00400: val_loss improved from 0.00601 to 0.00601, storing weights.\n",
      "new lr:  0.0294396361375\n",
      "\n",
      "Epoch 00401: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.00601 to 0.00600, storing weights.\n",
      "\n",
      "Epoch 00403: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00413: val_loss improved from 0.00600 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00414: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00416: val_loss improved from 0.00599 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00417: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00422: val_loss improved from 0.00599 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00423: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.00599 to 0.00598, storing weights.\n",
      "\n",
      "Epoch 00425: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00598, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00451: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00454: val_loss improved from 0.00598 to 0.00598, storing weights.\n",
      "\n",
      "Epoch 00455: val_loss improved from 0.00598 to 0.00597, storing weights.\n",
      "\n",
      "Epoch 00456: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00470: val_loss improved from 0.00597 to 0.00595, storing weights.\n",
      "\n",
      "Epoch 00471: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.00595 to 0.00595, storing weights.\n",
      "\n",
      "Epoch 00487: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.00595 to 0.00594, storing weights.\n",
      "new lr:  0.0177785730532\n",
      "\n",
      "Epoch 00501: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00502: val_loss improved from 0.00594 to 0.00594, storing weights.\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.00594 to 0.00594, storing weights.\n",
      "\n",
      "Epoch 00504: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00521: val_loss improved from 0.00594 to 0.00594, storing weights.\n",
      "\n",
      "Epoch 00522: val_loss improved from 0.00594 to 0.00593, storing weights.\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.00593 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00524: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00532: val_loss improved from 0.00592 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00533: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00535: val_loss improved from 0.00592 to 0.00591, storing weights.\n",
      "\n",
      "Epoch 00536: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.00591 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00538: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00539: val_loss improved from 0.00590 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00540: val_loss improved from 0.00590 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00541: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.00590 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00547: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00574: val_loss improved from 0.00590 to 0.00589, storing weights.\n",
      "\n",
      "Epoch 00575: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.00589 to 0.00589, storing weights.\n",
      "\n",
      "Epoch 00589: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.00590, did not improve\n",
      "new lr:  0.0107364662501\n",
      "\n",
      "Epoch 00601: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.00591, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00607: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00608: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00618: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00619: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00625: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00626: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00627: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00628: val_loss improved from 0.00589 to 0.00588, storing weights.\n",
      "\n",
      "Epoch 00629: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00630: val_loss improved from 0.00588 to 0.00588, storing weights.\n",
      "\n",
      "Epoch 00631: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00632: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00633: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00634: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00635: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00636: val_loss improved from 0.00588 to 0.00588, storing weights.\n",
      "\n",
      "Epoch 00637: val_loss improved from 0.00588 to 0.00588, storing weights.\n",
      "\n",
      "Epoch 00638: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00639: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00640: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00641: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00642: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00643: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00644: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00645: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00646: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00647: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00648: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00649: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00650: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00651: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00652: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00653: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00654: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00655: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00656: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00657: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00658: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00659: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00660: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00661: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00662: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00663: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00664: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00665: val_loss improved from 0.00588 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00666: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00667: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00668: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00669: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00670: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00671: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00672: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00673: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00674: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00675: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00676: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00677: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00678: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00679: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00680: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00681: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00682: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00683: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00684: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00685: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00686: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00687: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00688: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00689: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00690: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00691: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00692: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00693: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00694: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00695: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00696: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00697: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00698: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00699: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00700: val_loss is 0.00588, did not improve\n",
      "new lr:  0.00648374350381\n",
      "\n",
      "Epoch 00701: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00702: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00703: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00704: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00705: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00706: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00707: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00708: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00709: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00710: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00711: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00712: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00713: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00714: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00715: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00716: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00717: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00718: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00719: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00720: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00721: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00722: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00723: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00724: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00725: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00726: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00727: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00728: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00729: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00730: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00731: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00732: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00733: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00734: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00735: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00736: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00737: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00738: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00739: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00740: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00741: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00742: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00743: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00744: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00745: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00746: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00747: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00748: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00749: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00750: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00751: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00752: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00753: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00754: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00755: val_loss improved from 0.00587 to 0.00587, storing weights.\n",
      "\n",
      "Epoch 00756: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00757: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00758: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00759: val_loss is 0.00587, did not improve\n",
      "Epoch 00759: early stopping\n",
      "Using epoch 00755 with val_loss: 0.00587\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03070, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03070 to 0.02851, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02851 to 0.02714, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02714 to 0.02602, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02602 to 0.02579, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02579 to 0.02280, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02280 to 0.02091, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02091 to 0.02052, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 0.02052 to 0.01779, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01779 to 0.01686, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01686 to 0.01578, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.01631, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.01594, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.01630, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01578 to 0.01287, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01287 to 0.01240, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01240 to 0.01166, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01166 to 0.01027, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01027 to 0.01014, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01014 to 0.00928, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00928 to 0.00908, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00908 to 0.00873, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00873 to 0.00823, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.01013, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00823 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00762 to 0.00744, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00744 to 0.00743, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00743 to 0.00726, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00726 to 0.00703, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00703 to 0.00683, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00683 to 0.00678, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00678 to 0.00677, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00677 to 0.00663, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00663 to 0.00654, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.01195, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00654 to 0.00638, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00638 to 0.00629, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00629 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00624 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00620 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00618 to 0.00613, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00732, did not improve\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00613 to 0.00605, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00605 to 0.00604, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00604 to 0.00598, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00648, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00160: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00598 to 0.00596, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00596 to 0.00593, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00621, did not improve\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00593 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00590 to 0.00590, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00590 to 0.00586, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00586 to 0.00584, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.00584 to 0.00583, storing weights.\n",
      "\n",
      "Epoch 00287: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00592, did not improve\n",
      "new lr:  0.048749254134\n",
      "\n",
      "Epoch 00301: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00592, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00599, did not improve\n",
      "Epoch 00333: early stopping\n",
      "Using epoch 00286 with val_loss: 0.00583\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02617, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02617 to 0.02581, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02581 to 0.02403, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.02570, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02403 to 0.02173, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02200, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02173 to 0.01926, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01926 to 0.01858, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01858 to 0.01771, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01786, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01771 to 0.01594, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01594 to 0.01493, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01493 to 0.01404, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.01547, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01404 to 0.01325, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01325 to 0.01236, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01236 to 0.01207, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01207 to 0.01158, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01158 to 0.01060, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01068, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01060 to 0.00964, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00990, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00972, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00964 to 0.00912, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00912 to 0.00857, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00878, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00857 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00864, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00854 to 0.00849, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00849 to 0.00838, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00838 to 0.00825, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00825 to 0.00783, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00783 to 0.00769, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00889, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00805, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00769 to 0.00750, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00750 to 0.00723, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00723 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00717 to 0.00710, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00710 to 0.00703, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00703 to 0.00672, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00803, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00672 to 0.00628, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00628 to 0.00611, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00669, did not improve\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00611 to 0.00600, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00642, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00600 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00582 to 0.00578, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00578 to 0.00576, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00576 to 0.00569, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00569 to 0.00559, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00596, did not improve\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00583, did not improve\n",
      "Epoch 00263: early stopping\n",
      "Using epoch 00188 with val_loss: 0.00559\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00576] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00587]\n",
      " [ 0.00583]\n",
      " [ 0.00559]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00156] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00098]\n",
      " [ 0.00163]\n",
      " [ 0.00206]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928} \n",
    "m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, lr_exp_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07359, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07359 to 0.03587, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03587 to 0.03064, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03064 to 0.01689, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01689 to 0.01086, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01086 to 0.00439, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00439 to 0.00387, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00387 to 0.00370, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00370 to 0.00308, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00393, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00308 to 0.00272, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00332, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00282, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00272 to 0.00264, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00264 to 0.00249, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00249 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00235 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00226 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00219 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00214 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00212 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00211 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00206 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00201 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00196 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00191 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00188 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00184 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00181 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00178 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00176 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00173 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00170 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00167 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00164 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00161 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00159 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00156 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00154 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00152 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00150 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00147 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00145 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00143 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00140 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00137 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00131 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00130 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00130 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00130 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00127 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00122 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00118 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00115 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00112 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00105 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00103 to 0.00102, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00123: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00100 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00095 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00092 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00089 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00086 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00084 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00080 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00078 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00076 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00074 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00068 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.00046 to 0.00046, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00297: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00299: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00305: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00309: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00044, did not improve\n",
      "Epoch 00359: early stopping\n",
      "Using epoch 00326 with val_loss: 0.00044\n",
      "mse stepwise internally 0.0469266076473\n",
      "mse stepwise internally 0.0275177496763\n",
      "validate on 5 steps, mse on train / validation data: 0.02752 / 0.04693\n",
      "mse stepwise internally 0.04714533019\n",
      "mse stepwise internally 0.0276782881636\n",
      "validate on 10 steps, mse on train / validation data: 0.02768 / 0.04715\n",
      "mse stepwise internally 0.05511352508\n",
      "mse stepwise internally 0.0323009135367\n",
      "validate on 20 steps, mse on train / validation data: 0.03230 / 0.05511\n",
      "mse stepwise internally 0.0422007361886\n",
      "mse stepwise internally 0.0245136478143\n",
      "validate on 30 steps, mse on train / validation data: 0.02451 / 0.04220\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10700, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10700 to 0.05263, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05263 to 0.02960, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02960 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00640 to 0.00564, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00564 to 0.00336, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss improved from 0.00336 to 0.00240, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00240 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00204 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00169 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00145 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00133 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00126 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00121 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00119 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00116 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00113 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00109 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00105 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00101 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00096 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00091 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00086 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00081 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00076 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00072 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00068 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00053 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00055, did not improve\n",
      "Epoch 00151: early stopping\n",
      "Using epoch 00077 with val_loss: 0.00044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse stepwise internally 0.0339977157827\n",
      "mse stepwise internally 0.0436377948348\n",
      "validate on 5 steps, mse on train / validation data: 0.04364 / 0.03400\n",
      "mse stepwise internally 0.0325395576491\n",
      "mse stepwise internally 0.0418941139403\n",
      "validate on 10 steps, mse on train / validation data: 0.04189 / 0.03254\n",
      "mse stepwise internally 0.0338674056761\n",
      "mse stepwise internally 0.0438319488015\n",
      "validate on 20 steps, mse on train / validation data: 0.04383 / 0.03387\n",
      "mse stepwise internally 0.0464414242323\n",
      "mse stepwise internally 0.059395445632\n",
      "validate on 30 steps, mse on train / validation data: 0.05940 / 0.04644\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07906, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07906 to 0.04933, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04933 to 0.03220, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03220 to 0.01277, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01277 to 0.00580, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00580 to 0.00369, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00369 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00231 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00134 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00128 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00124 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00121 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00117 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00115 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00113 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00111 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00108 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00106 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00104 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00102 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00100 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00098 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00096 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00093 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00091 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00084 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00078 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00052 to 0.00052, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00050, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00185 with val_loss: 0.00049\n",
      "mse stepwise internally 0.424842885813\n",
      "mse stepwise internally 0.0440101990581\n",
      "validate on 5 steps, mse on train / validation data: 0.04401 / 0.42484\n",
      "mse stepwise internally 0.425116523065\n",
      "mse stepwise internally 0.0387560819955\n",
      "validate on 10 steps, mse on train / validation data: 0.03876 / 0.42512\n",
      "mse stepwise internally 0.0226071816626\n",
      "mse stepwise internally 0.026095001501\n",
      "validate on 20 steps, mse on train / validation data: 0.02610 / 0.02261\n",
      "mse stepwise internally 0.00767537811606\n",
      "mse stepwise internally 0.00827062445034\n",
      "validate on 30 steps, mse on train / validation data: 0.00827 / 0.00768\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.16859  0.16827  0.0372   0.03211] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04693  0.04715  0.05511  0.0422 ]\n",
      " [ 0.034    0.03254  0.03387  0.04644]\n",
      " [ 0.42484  0.42512  0.02261  0.00768]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.03839  0.03611  0.03408  0.03073] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02752  0.02768  0.0323   0.02451]\n",
      " [ 0.04364  0.04189  0.04383  0.0594 ]\n",
      " [ 0.04401  0.03876  0.0261   0.00827]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03170 to 0.02267, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02267 to 0.00738, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00738 to 0.00364, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00403, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00364 to 0.00258, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00320, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00258 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00182 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00167 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00151 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00133 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00122 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00117 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00109 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00096 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00088 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00086 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00079 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00070 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00064 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00107, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00059 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00050 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00049, did not improve\n",
      "Epoch 00277: early stopping\n",
      "Using epoch 00247 with val_loss: 0.00047\n",
      "mse stepwise internally 0.379906095379\n",
      "mse stepwise internally 0.318434579375\n",
      "validate on 5 steps, mse on train / validation data: 0.31843 / 0.37991\n",
      "mse stepwise internally 0.0461703408201\n",
      "mse stepwise internally 0.0300786423094\n",
      "validate on 10 steps, mse on train / validation data: 0.03008 / 0.04617\n",
      "mse stepwise internally 0.0262568468982\n",
      "mse stepwise internally 0.0180226784409\n",
      "validate on 20 steps, mse on train / validation data: 0.01802 / 0.02626\n",
      "mse stepwise internally 0.00824592730408\n",
      "mse stepwise internally 0.00626874559753\n",
      "validate on 30 steps, mse on train / validation data: 0.00627 / 0.00825\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07759, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07759 to 0.02841, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02841 to 0.01022, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01022 to 0.00465, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00465 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00406, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00663, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00151 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00129 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00088 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00071 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00037 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00032 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00036, did not improve\n",
      "Epoch 00094: early stopping\n",
      "Using epoch 00019 with val_loss: 0.00029\n",
      "mse stepwise internally 0.0325966905164\n",
      "mse stepwise internally 0.0394062507787\n",
      "validate on 5 steps, mse on train / validation data: 0.03941 / 0.03260\n",
      "mse stepwise internally 0.095712086821\n",
      "mse stepwise internally 0.114883071326\n",
      "validate on 10 steps, mse on train / validation data: 0.11488 / 0.09571\n",
      "mse stepwise internally 0.0312029236239\n",
      "mse stepwise internally 0.0317669568705\n",
      "validate on 20 steps, mse on train / validation data: 0.03177 / 0.03120\n",
      "mse stepwise internally 0.00436366639366\n",
      "mse stepwise internally 0.00499804170101\n",
      "validate on 30 steps, mse on train / validation data: 0.00500 / 0.00436\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08037, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08037 to 0.03530, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03530 to 0.01631, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01631 to 0.00917, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00917 to 0.00269, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00269 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00172 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00160 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00113 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00054 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00036 to 0.00036, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00030, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00180: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00033, did not improve\n",
      "Epoch 00184: early stopping\n",
      "Using epoch 00113 with val_loss: 0.00029\n",
      "mse stepwise internally 0.0314387384037\n",
      "mse stepwise internally 0.0517544761135\n",
      "validate on 5 steps, mse on train / validation data: 0.05175 / 0.03144\n",
      "mse stepwise internally 0.0314865506983\n",
      "mse stepwise internally 0.0518683157799\n",
      "validate on 10 steps, mse on train / validation data: 0.05187 / 0.03149\n",
      "mse stepwise internally 0.0297549918051\n",
      "mse stepwise internally 0.0492975517072\n",
      "validate on 20 steps, mse on train / validation data: 0.04930 / 0.02975\n",
      "mse stepwise internally 0.0149845102706\n",
      "mse stepwise internally 0.0244711617438\n",
      "validate on 30 steps, mse on train / validation data: 0.02447 / 0.01498\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.14798  0.05779  0.02907  0.0092 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.37991  0.04617  0.02626  0.00825]\n",
      " [ 0.0326   0.09571  0.0312   0.00436]\n",
      " [ 0.03144  0.03149  0.02975  0.01498]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.13653  0.06561  0.03303  0.01191] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.31843  0.03008  0.01802  0.00627]\n",
      " [ 0.03941  0.11488  0.03177  0.005  ]\n",
      " [ 0.05175  0.05187  0.0493   0.02447]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03613, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03613 to 0.02653, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02653 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00808 to 0.00506, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00506 to 0.00281, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00281 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00245 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00105 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00093 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00085 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00077 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00059 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00057 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00055 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00053 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00055, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00113: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00063, did not improve\n",
      "Epoch 00260: early stopping\n",
      "Using epoch 00187 with val_loss: 0.00047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse stepwise internally 0.0594358847986\n",
      "mse stepwise internally 0.0559445553609\n",
      "validate on 5 steps, mse on train / validation data: 0.05594 / 0.05944\n",
      "mse stepwise internally 0.0275901818439\n",
      "mse stepwise internally 0.0230632018756\n",
      "validate on 10 steps, mse on train / validation data: 0.02306 / 0.02759\n",
      "mse stepwise internally 0.00643500588673\n",
      "mse stepwise internally 0.0066676469793\n",
      "validate on 20 steps, mse on train / validation data: 0.00667 / 0.00644\n",
      "mse stepwise internally 0.00182183466232\n",
      "mse stepwise internally 0.00205125136484\n",
      "validate on 30 steps, mse on train / validation data: 0.00205 / 0.00182\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08263, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08263 to 0.05208, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05208 to 0.03203, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03203 to 0.01209, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01209 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00719 to 0.00292, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00292 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00127 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00054 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00046 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00038 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00034 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00025 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00024 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00024 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00023 to 0.00023, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00023, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00024, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00027, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00027, did not improve\n",
      "Epoch 00129: early stopping\n",
      "Using epoch 00056 with val_loss: 0.00023\n",
      "mse stepwise internally 0.00769630315048\n",
      "mse stepwise internally 0.00950980023551\n",
      "validate on 5 steps, mse on train / validation data: 0.00951 / 0.00770\n",
      "mse stepwise internally 0.00210668924071\n",
      "mse stepwise internally 0.00272537674322\n",
      "validate on 10 steps, mse on train / validation data: 0.00273 / 0.00211\n",
      "mse stepwise internally 0.0023849642457\n",
      "mse stepwise internally 0.00195853266255\n",
      "validate on 20 steps, mse on train / validation data: 0.00196 / 0.00238\n",
      "mse stepwise internally 0.000818691221897\n",
      "mse stepwise internally 0.000966860888012\n",
      "validate on 30 steps, mse on train / validation data: 0.00097 / 0.00082\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06469, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06469 to 0.03679, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03679 to 0.00814, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00814 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00689 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00083 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00057 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00048 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00045 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00035 to 0.00035, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00035, did not improve\n",
      "Epoch 00192: early stopping\n",
      "Using epoch 00149 with val_loss: 0.00033\n",
      "mse stepwise internally 0.0776240137403\n",
      "mse stepwise internally 0.080989971531\n",
      "validate on 5 steps, mse on train / validation data: 0.08099 / 0.07762\n",
      "mse stepwise internally 0.0217949965997\n",
      "mse stepwise internally 0.0251495907417\n",
      "validate on 10 steps, mse on train / validation data: 0.02515 / 0.02179\n",
      "mse stepwise internally 0.00302881019981\n",
      "mse stepwise internally 0.00293656814442\n",
      "validate on 20 steps, mse on train / validation data: 0.00294 / 0.00303\n",
      "mse stepwise internally 0.000942590104584\n",
      "mse stepwise internally 0.000704046322488\n",
      "validate on 30 steps, mse on train / validation data: 0.00070 / 0.00094\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04825  0.01716  0.00395  0.00119] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.05944  0.02759  0.00644  0.00182]\n",
      " [ 0.0077   0.00211  0.00238  0.00082]\n",
      " [ 0.07762  0.02179  0.00303  0.00094]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04881  0.01698  0.00385  0.00124] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.05594  0.02306  0.00667  0.00205]\n",
      " [ 0.00951  0.00273  0.00196  0.00097]\n",
      " [ 0.08099  0.02515  0.00294  0.0007 ]]\n",
      "results validation data \n",
      " [[ 0.16859  0.16827  0.0372   0.03211]\n",
      " [ 0.14798  0.05779  0.02907  0.0092 ]\n",
      " [ 0.04825  0.01716  0.00395  0.00119]]\n",
      "results training data\n",
      " [[ 0.03839  0.03611  0.03408  0.03073]\n",
      " [ 0.13653  0.06561  0.03303  0.01191]\n",
      " [ 0.04881  0.01698  0.00385  0.00124]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.2\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                    steps=(train_steps,[5,10,20,30]), \n",
    "                    cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "    \n",
    "    res_train[i], res_val[i] = res['trn_means'], res['val_means']\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08584, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08584 to 0.07439, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07439 to 0.02369, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02369 to 0.01386, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01386 to 0.01289, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01289 to 0.00480, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00480 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00335 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00121 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00099 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00063 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00046 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00059, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00036, did not improve\n",
      "Epoch 00160: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.01471 / 0.02526\n",
      "validate on 10 steps, mse on train / validation data: 0.01031 / 0.01362\n",
      "validate on 20 steps, mse on train / validation data: 0.00436 / 0.00365\n",
      "validate on 30 steps, mse on train / validation data: 0.00166 / 0.00143\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60643, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60643 to 0.04722, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04722 to 0.04653, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.06111, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04653 to 0.03205, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03300, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03205 to 0.02693, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02693 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01068 to 0.00309, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00309 to 0.00285, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00285 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00163 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00095 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00060 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00055 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00041 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00035 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00026 to 0.00020, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00081 with val_loss: 0.00020\n",
      "validate on 5 steps, mse on train / validation data: 0.16159 / 0.08641\n",
      "validate on 10 steps, mse on train / validation data: 0.12393 / 0.07440\n",
      "validate on 20 steps, mse on train / validation data: 0.03269 / 0.02646\n",
      "validate on 30 steps, mse on train / validation data: 0.00476 / 0.00441\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11001, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11001 to 0.08327, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08327 to 0.03608, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03608 to 0.01065, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01065 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00554 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00247 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00202 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00197 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00120 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00058 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00043 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00031 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "Epoch 00119: early stopping\n",
      "Using epoch 00055 with val_loss: 0.00025\n",
      "validate on 5 steps, mse on train / validation data: 0.02207 / 0.03434\n",
      "validate on 10 steps, mse on train / validation data: 0.01448 / 0.02045\n",
      "validate on 20 steps, mse on train / validation data: 0.00771 / 0.00859\n",
      "validate on 30 steps, mse on train / validation data: 0.00305 / 0.00316\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04867  0.03615  0.0129   0.003  ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02526  0.01362  0.00365  0.00143]\n",
      " [ 0.08641  0.0744   0.02646  0.00441]\n",
      " [ 0.03434  0.02045  0.00859  0.00316]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.06613  0.04957  0.01492  0.00315] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01471  0.01031  0.00436  0.00166]\n",
      " [ 0.16159  0.12393  0.03269  0.00476]\n",
      " [ 0.02207  0.01448  0.00771  0.00305]]\n",
      "results training data\n",
      " [ 0.06612556  0.04957401  0.01491995  0.00315308]\n",
      "results validation data \n",
      " [ 0.04866848  0.03615473  0.01290235  0.00300086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train with random lenghts\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11554, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11554 to 0.04302, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04302 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01769 to 0.01097, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01097 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00882 to 0.00847, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00847 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00606 to 0.00438, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00438 to 0.00315, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.01499, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.01350, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00410, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00315 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00251 to 0.00250, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00250 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00220 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00181 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00160 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00137 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00362, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00150, did not improve\n",
      "Epoch 00123: early stopping\n",
      "Using epoch 00049 with val_loss: 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 459us/step\n",
      "mse:  0.00131853502845\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 870us/step\n",
      "mse:  0.00133469417448\n",
      "validate on 5 steps, mse on train / validation data: 0.00133 / 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.001162388689\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00171085036444\n",
      "validate on 10 steps, mse on train / validation data: 0.00171 / 0.00116\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00321647753217\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00388603562235\n",
      "validate on 20 steps, mse on train / validation data: 0.00389 / 0.00322\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00459700725512\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.00526945944875\n",
      "validate on 30 steps, mse on train / validation data: 0.00527 / 0.00460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34632, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34632 to 0.06494, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06494 to 0.05340, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05340 to 0.02735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02735 to 0.01483, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.02008, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01483 to 0.01388, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01388 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01224 to 0.00833, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00833 to 0.00687, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00687 to 0.00294, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00460, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00359, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00294 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00487, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00441, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00466, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00401, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00226 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00220 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00216 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00214 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00211 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00211 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00209 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00206 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00201 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00197 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00193 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00176 to 0.00175, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00168 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00158 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00155 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00152 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00150 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00148 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00144 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00142 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00131 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00126, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00122 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00120 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00119 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.00118 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00116, did not improve\n",
      "Epoch 00328: early stopping\n",
      "Using epoch 00323 with val_loss: 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00115162648779\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 903us/step\n",
      "mse:  0.0012707624862\n",
      "validate on 5 steps, mse on train / validation data: 0.00127 / 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00158448308833\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0011413448456\n",
      "validate on 10 steps, mse on train / validation data: 0.00114 / 0.00158\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0039176084207\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.00348516953004\n",
      "validate on 20 steps, mse on train / validation data: 0.00349 / 0.00392\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0050399990075\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.00497433785639\n",
      "validate on 30 steps, mse on train / validation data: 0.00497 / 0.00504\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.99042, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.99042 to 1.47941, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 2.96802, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47941 to 0.63913, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63913 to 0.51863, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51863 to 0.08843, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08843 to 0.05703, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05703 to 0.04830, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04830 to 0.03768, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03768 to 0.03411, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03411 to 0.02496, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02528, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02496 to 0.02449, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02449 to 0.02441, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02461, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02456, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02441 to 0.02439, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02445, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02451, did not improve\n",
      "Epoch 00093: early stopping\n",
      "Using epoch 00018 with val_loss: 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 765us/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 776us/step\n",
      "mse:  0.0299257414868\n",
      "validate on 5 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 10 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 20 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 30 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00895  0.00904  0.01051  0.01134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00132  0.00116  0.00322  0.0046 ]\n",
      " [ 0.00115  0.00158  0.00392  0.00504]\n",
      " [ 0.02439  0.02439  0.02439  0.02439]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01084  0.01093  0.01243  0.01339] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00133  0.00171  0.00389  0.00527]\n",
      " [ 0.00127  0.00114  0.00349  0.00497]\n",
      " [ 0.02993  0.02993  0.02993  0.02993]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12057, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12057 to 0.06355, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06355 to 0.03759, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03759 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00790 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00766 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00323 to 0.00304, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00304 to 0.00301, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00301 to 0.00289, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00289 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00259 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00227 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00199 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00178 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00152 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00122 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00120 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00104 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00149, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00089 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00084, did not improve\n",
      "Epoch 00120: early stopping\n",
      "Using epoch 00088 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 481us/step\n",
      "mse:  0.0608066631418\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 855us/step\n",
      "mse:  0.0477681301365\n",
      "validate on 5 steps, mse on train / validation data: 0.04777 / 0.06081\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000824270867403\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000763803920894\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00128009282\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00143760890404\n",
      "validate on 20 steps, mse on train / validation data: 0.00144 / 0.00128\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00213678000401\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.0023780453167\n",
      "validate on 30 steps, mse on train / validation data: 0.00238 / 0.00214\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26239, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26239 to 0.07691, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.12018, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07691 to 0.05873, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05873 to 0.02621, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03345, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02621 to 0.02494, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02494 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01605 to 0.01107, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01107 to 0.00395, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00395 to 0.00253, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00253 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00169 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00079 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00064 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00065, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00068, did not improve\n",
      "Epoch 00106: early stopping\n",
      "Using epoch 00083 with val_loss: 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 590us/step\n",
      "mse:  0.0690140077336\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 744us/step\n",
      "mse:  0.0738325941319\n",
      "validate on 5 steps, mse on train / validation data: 0.07383 / 0.06901\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000604276981903\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000690422686459\n",
      "validate on 10 steps, mse on train / validation data: 0.00069 / 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.00140974406068\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0012300674403\n",
      "validate on 20 steps, mse on train / validation data: 0.00123 / 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00224876565732\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00229339740215\n",
      "validate on 30 steps, mse on train / validation data: 0.00229 / 0.00225\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12354, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12354 to 0.04591, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04597, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04591 to 0.02748, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02748 to 0.01963, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01963 to 0.01252, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01252 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00803 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00658 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00560 to 0.00375, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00375 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00194 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00174 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00172 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00153 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00150 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00140 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00110 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00108 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00104 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00095 to 0.00094, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "Epoch 00104: early stopping\n",
      "Using epoch 00104 with val_loss: 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 526us/step\n",
      "mse:  0.789464980364\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 710us/step\n",
      "mse:  0.750682481601\n",
      "validate on 5 steps, mse on train / validation data: 0.75068 / 0.78946\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000914607464272\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000559727040101\n",
      "validate on 10 steps, mse on train / validation data: 0.00056 / 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.980713968927\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.812348923104\n",
      "validate on 20 steps, mse on train / validation data: 0.81235 / 0.98071\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.750668579882\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "mse:  0.836846585664\n",
      "validate on 30 steps, mse on train / validation data: 0.83685 / 0.75067\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.30643  0.00078  0.3278   0.25168] ***\n",
      "Results validation data of all Folds: \n",
      "[[  6.08100000e-02   8.20000000e-04   1.28000000e-03   2.14000000e-03]\n",
      " [  6.90100000e-02   6.00000000e-04   1.41000000e-03   2.25000000e-03]\n",
      " [  7.89460000e-01   9.10000000e-04   9.80710000e-01   7.50670000e-01]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.29076  0.00067  0.27167  0.28051] ***\n",
      "Results training data of all Folds: \n",
      "[[  4.77700000e-02   7.60000000e-04   1.44000000e-03   2.38000000e-03]\n",
      " [  7.38300000e-02   6.90000000e-04   1.23000000e-03   2.29000000e-03]\n",
      " [  7.50680000e-01   5.60000000e-04   8.12350000e-01   8.36850000e-01]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14798, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14798 to 0.04405, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04405 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00993 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00817 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00592 to 0.00324, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00324 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00244 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00093 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00059 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00055 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00050 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "Epoch 00116: early stopping\n",
      "Using epoch 00041 with val_loss: 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 641us/step\n",
      "mse:  0.0258297447869\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 860us/step\n",
      "mse:  0.0263428334147\n",
      "validate on 5 steps, mse on train / validation data: 0.02634 / 0.02583\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.0107730397683\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00910780364013\n",
      "validate on 10 steps, mse on train / validation data: 0.00911 / 0.01077\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000469433154199\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000500161555961\n",
      "validate on 20 steps, mse on train / validation data: 0.00050 / 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.000675560233544\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.000807838148665\n",
      "validate on 30 steps, mse on train / validation data: 0.00081 / 0.00068\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17151 to 1.22318, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22318 to 0.66304, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.96452, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66304 to 0.11085, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11085 to 0.07196, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07196 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03687 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03687 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00837 to 0.00530, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00530 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00227 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00175 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00161 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00113 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00107 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00103 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00100 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00070 to 0.00069, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00060 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00051 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00051, did not improve\n",
      "Epoch 00223: early stopping\n",
      "Using epoch 00156 with val_loss: 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 665us/step\n",
      "mse:  0.0250337426974\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 799us/step\n",
      "mse:  0.0298722941786\n",
      "validate on 5 steps, mse on train / validation data: 0.02987 / 0.02503\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  4.91890081492\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  5.00050854009\n",
      "validate on 10 steps, mse on train / validation data: 5.00051 / 4.91890\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000351062770113\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000462018283239\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  3.78144966472\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  3.87956652129\n",
      "validate on 30 steps, mse on train / validation data: 3.87957 / 3.78145\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11390, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11390 to 0.05036, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05036 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01418, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01418 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00294, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00257 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00224 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00218 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00158 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00129 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00113 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00096 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00081 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.00073, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00212 with val_loss: 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 614us/step\n",
      "mse:  0.00897776255045\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 853us/step\n",
      "mse:  0.0123261121959\n",
      "validate on 5 steps, mse on train / validation data: 0.01233 / 0.00898\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00932959319008\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0085558105197\n",
      "validate on 10 steps, mse on train / validation data: 0.00856 / 0.00933\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000731091864344\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000461046307382\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000526615978742\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.000265021034481\n",
      "validate on 30 steps, mse on train / validation data: 0.00027 / 0.00053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [  1.99500000e-02   1.64633000e+00   5.20000000e-04   1.26088000e+00] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.58300000e-02   1.07700000e-02   4.70000000e-04   6.80000000e-04]\n",
      " [  2.50300000e-02   4.91890000e+00   3.50000000e-04   3.78145000e+00]\n",
      " [  8.98000000e-03   9.33000000e-03   7.30000000e-04   5.30000000e-04]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [  2.28500000e-02   1.67272000e+00   4.70000000e-04   1.29355000e+00] ***\n",
      "Results training data of all Folds: \n",
      "[[  2.63400000e-02   9.11000000e-03   5.00000000e-04   8.10000000e-04]\n",
      " [  2.98700000e-02   5.00051000e+00   4.60000000e-04   3.87957000e+00]\n",
      " [  1.23300000e-02   8.56000000e-03   4.60000000e-04   2.70000000e-04]]\n",
      "[[  1.08437327e-02   1.09259789e-02   1.24323155e-02   1.33898463e-02]\n",
      " [  2.90761069e-01   6.71317882e-04   2.71672200e-01   2.80506009e-01]\n",
      " [  2.28470799e-02   1.67272405e+00   4.74408716e-04   1.29354646e+00]]\n",
      "[[  8.95270427e-03   9.04494103e-03   1.05073458e-02   1.13416525e-02]\n",
      " [  3.06428550e-01   7.81051771e-04   3.27801269e-01   2.51684709e-01]\n",
      " [  1.99470833e-02   1.64633448e+00   5.17195930e-04   1.26088395e+00]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.3 base line training with fixed lenghts (on final epoch)\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                     steps=(train_steps,[5,10,20,30]), \n",
    "                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                     mode='finalstep')\n",
    "    res_train[i], res_val[i] = res['trn_means'], res['val_means']\n",
    "    \n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06410, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06410 to 0.03095, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03095 to 0.00941, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00941 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00702 to 0.00428, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00428 to 0.00373, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00373 to 0.00358, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00358 to 0.00316, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00437, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00316 to 0.00297, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00297 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00180 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00426, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00432, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00333, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00167 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00277, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00387, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00408, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00366, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00453, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00162 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00150 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00434, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00360, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00143 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00121 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00101 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00360, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00083 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00075 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00101, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_loss is 0.00392, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00336, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00068 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00062 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00086, did not improve\n",
      "Epoch 00301: early stopping\n",
      "Using epoch 00235 with val_loss: 0.00059\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00130 / 0.00146\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00073 / 0.00066\n",
      "evaluate lstm with consideration of configs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00058 / 0.00057\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00128 / 0.00111\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09386, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09386 to 0.05620, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05620 to 0.01881, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01881 to 0.00845, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00845 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00803 to 0.00357, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00357 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00228 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00137 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00122 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00103 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00087 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00076 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00071 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00067 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00063 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00063, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00050 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00045 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00043 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "Epoch 00253: early stopping\n",
      "Using epoch 00253 with val_loss: 0.00041\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00144 / 0.00070\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00065 / 0.00049\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00053 / 0.00042\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00082 / 0.00061\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18725, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18725 to 0.08344, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08344 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01869 to 0.01259, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01259 to 0.00759, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00759 to 0.00330, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00434, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00375, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00344, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00330 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00219 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00208 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00128 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00170, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss is 0.00295, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00111 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00087 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00124, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00190: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00096, did not improve\n",
      "Epoch 00226: early stopping\n",
      "Using epoch 00151 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00157 / 0.00258\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00053 / 0.00090\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00042 / 0.00059\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00049 / 0.00077\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00158  0.00068  0.00053  0.00083] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00146  0.00066  0.00057  0.00111]\n",
      " [ 0.0007   0.00049  0.00042  0.00061]\n",
      " [ 0.00258  0.0009   0.00059  0.00077]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00144  0.00064  0.00051  0.00086] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0013   0.00073  0.00058  0.00128]\n",
      " [ 0.00144  0.00065  0.00053  0.00082]\n",
      " [ 0.00157  0.00053  0.00042  0.00049]]\n",
      "results validation data \n",
      " [ 0.00158  0.00068  0.00053  0.00083]\n",
      "results training data\n",
      " [ 0.00144  0.00064  0.00051  0.00086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train train using final points with random lenghts\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='finalstep')\n",
    "\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'n_estimators': 1000, 'gamma': 0.007833441242813044, 'maxdepth': 10, 'cols_bt': 0.9376450587145334}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.584156378552\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.578395060919\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.527057613488\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.500720170913\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.494855966833\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.454835391707\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.458230453509\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.465432094203\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.472633742624\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.44094650061\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.398662551686\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.417901235598\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.432510285466\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.37952675422\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.399382723702\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.371913578775\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.374074074957\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.378189303257\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.351543205756\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.330349789725\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.318209884343\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.36121398652\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.320370373902\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.34917695434\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.318004115864\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.313580245883\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.321296292323\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.32448559558\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.34002057049\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.311934159862\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.339094645447\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.330349800763\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.306172834502\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.32294238276\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.302572014155\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.326982 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.318048 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.325919 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308534 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.317099 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.306027 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.311122 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.301471 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.301088 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.296533 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.296533 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.296533 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.296533 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.296533 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.296533 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.296533 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.296533 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.296533 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.296533 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.296533 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.296533 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.296533 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.296533 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.296533 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.296533 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.296533 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.296533 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.296533 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.296533 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.296533 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.296533 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.296533 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.827147 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.792634 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.71391 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.742944 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.870816 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.838877 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.817926 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.842618 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.874557 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.874557 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.874557 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.874557 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.874557 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.874557 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.874557 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.874557 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.874557 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.874557 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.874557 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.874557 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.874557 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.874557 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.874557 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.874557 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.874557 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.874557 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.874557 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.874557 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.874557 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.874557 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.874557 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.874557 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.874557 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.874557 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.874557 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.281761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293365 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.273654 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.287996 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27613 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.285244 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.273654 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.283982 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.273654 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281323 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.273654 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27613 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.273654 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.273654 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.273654 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.273654 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.273654 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.273654 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.273654 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.273654 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.273654 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.273654 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.273654 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.273654 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.273654 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.273654 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.273654 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.273654 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.273654 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.273654 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.542816 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.474905 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.501156 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.474768 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.559699 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.470011 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.501156 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.474768 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.584386 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470011 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.501156 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.474768 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.692753 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470011 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.501156 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.50006 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.771477 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.479863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.501156 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.504955 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.792428 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.479863 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.501156 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.504955 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.792428 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.479863 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.501156 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.504955 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.792428 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.479863 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.240653 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.251472 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.244716 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.239961 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.244298 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.244716 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.244298 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.244298 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.244298 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.244298 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.244298 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.244298 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.244298 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.244298 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.244298 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.244298 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.244298 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.244298 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.244298 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.244298 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.387555 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.383345 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.377112 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.367352 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.374395 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.365674 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.374395 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.35863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.369658 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.354651 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.365404 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.354651 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.354376 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.354651 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.350397 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.350397 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.348069 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.348069 / 0.32294238276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 38 prediction / true value for lc number 13 0.348069 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.348069 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221099 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.221099 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.214998 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.216762 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.216762 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.214998 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.214998 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.214998 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.214998 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.214998 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.316017 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.320824 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.311122 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311122 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.306027 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.306027 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.301471 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.330041154667\n",
      "validate on 30 steps, mse on train / validation data: 0.00049 / 0.00105\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.806425 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.756053 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.741435 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.735141 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.860325 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.814813 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.810539 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.829144 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.878929 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.878929 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.878929 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.878929 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.878929 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.878929 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.878929 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.878929 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.878929 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.878929 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.878929 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.878929 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.878929 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.878929 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.878929 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.878929 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.878929 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.878929 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.878929 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.878929 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.878929 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.878929 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.878929 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.878929 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.878929 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.878929 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.878929 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.33173 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.308723 / 0.275050303766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 7 prediction / true value for lc number 13 0.323158 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308723 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.311356 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.308723 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.307247 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.307247 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.307247 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.307247 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.307247 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.307247 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.307247 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.307247 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.307247 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.307247 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.307247 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.307247 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.307247 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.307247 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.307247 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.307247 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.307247 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.307247 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.307247 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.307247 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.307247 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.307247 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.307247 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.307247 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.518932 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.496154 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.47981 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.470411 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.518932 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.478753 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.478264 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.470411 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.518932 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470411 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.478264 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.470411 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.518932 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470411 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.478264 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.470411 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.518932 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.470411 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.478264 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.470411 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.518932 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.470411 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.478264 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.470411 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.518932 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.470411 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.478264 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.470411 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.518932 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.470411 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.284761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293934 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.292961 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295669 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.291778 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.301144 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.295669 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.301144 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.301144 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.301144 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.301144 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.301144 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.301144 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.301144 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.301144 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.301144 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.301144 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.301144 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.301144 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.301144 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.301144 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.301144 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.301144 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.301144 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.301144 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.301144 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.301144 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.301144 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.301144 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.301144 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.402062 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.379441 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.384904 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.374249 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.384904 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.371541 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.376876 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.366349 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.376876 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.363279 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.366349 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.359094 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.360761 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.359094 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.356575 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.351391 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.356575 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.351391 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.349468 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.344566 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.238221 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.243812 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.231052 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.238221 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.231052 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.231052 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.231052 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.231052 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.231052 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.231052 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.231052 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.231052 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.231052 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.231052 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.231052 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.231052 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.231052 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.231052 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.231052 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.231052 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.31241 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.329096 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.308723 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311356 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.308723 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221494 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.216509 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.212515 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.214149 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.209929 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.209929 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.209929 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.207869 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.207869 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.207869 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00050 / 0.00036\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 5 prediction / true value for lc number 13 0.485691 / 0.421210902518\n",
      "step nr. 6 prediction / true value for lc number 13 0.437519 / 0.407979146417\n",
      "step nr. 7 prediction / true value for lc number 13 0.424689 / 0.396952684712\n",
      "step nr. 8 prediction / true value for lc number 13 0.4422 / 0.393143543139\n",
      "step nr. 9 prediction / true value for lc number 13 0.451538 / 0.383019244234\n",
      "step nr. 10 prediction / true value for lc number 13 0.421922 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.419173 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.43516 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.424689 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.415883 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.415883 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.419563 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.409141 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.40588 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.406599 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.397296 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.397296 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.397296 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.394435 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.394435 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.394435 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.394435 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.389773 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.389773 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.386784 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.376037 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.373892 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.373892 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.373892 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.373892 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.363841 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.361281 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.361281 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.35565 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.35565 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.311133 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.311415 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.308342 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.29057 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.305116 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.305116 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.300425 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.289175 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295496 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.289611 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.28329 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.281266 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 10 prediction / true value for lc number 13 0.394435 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.391446 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.379026 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.384639 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.383023 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.376037 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.373892 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.373892 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.373892 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.373892 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.363841 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.361281 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.361281 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.35565 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.35565 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.280891 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.289175 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.277666 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.297836 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27627 / 0.252716300743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.27627 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.27627 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 20 prediction / true value for lc number 13 0.342239 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.342239 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.342239 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.342239 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.342239 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.228495 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.228228 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.227044 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.227044 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.227044 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.227044 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.227044 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.227044 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.227044 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.227044 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.227044 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.227044 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.227044 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.227044 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.227044 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.227044 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.227044 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.227044 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.227044 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.227044 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 30 prediction / true value for lc number 13 0.324573 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.324573 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.324573 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.324573 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.324573 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.324573 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.324573 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.324573 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.324573 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.324573 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.211502 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.211502 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.211502 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.211502 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.211502 / 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 35 prediction / true value for lc number 13 0.211502 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.211502 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.211502 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.211502 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.211502 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00048 / 0.00101\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04854  0.01058  0.00153  0.00081] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04451  0.0149   0.00263  0.00105]\n",
      " [ 0.06447  0.01171  0.00082  0.00036]\n",
      " [ 0.03663  0.00513  0.00113  0.00101]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04393  0.01114  0.00172  0.00049] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0508   0.01723  0.00177  0.00049]\n",
      " [ 0.04719  0.01064  0.00247  0.0005 ]\n",
      " [ 0.03379  0.00554  0.00093  0.00048]]\n",
      "results validation data \n",
      " [ 0.04854  0.01058  0.00153  0.00081]\n",
      "results training data\n",
      " [ 0.04393  0.01114  0.00172  0.00049]\n"
     ]
    }
   ],
   "source": [
    "# task 3.4 \n",
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "res = m.eval_cv('xgb_next', [configs, lcs], Y, steps=(0,[5,10,20,30]), cfg=cfg)\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    running hyperparameter optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139861376198400\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.00975 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.01163 -0.00698 -0.01064]\n",
      "hyperband obj crossval results 0.00975\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0221 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.03018 -0.01989 -0.01623]\n",
      "hyperband obj crossval results 0.0221\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01826 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02461 -0.01537 -0.0148 ]\n",
      "hyperband obj crossval results 0.01826\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "traj {'losses': [-0.00975, -0.04739, -0.0611], 'budgets': [10.0, 10.0, 10.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2)], 'time_finished': [0.1408522129058838, 0.5603299140930176, 0.603823184967041]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(0, 0, 0): {'config_info': {}, 'config': {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}}, (0, 0, 2): {'config_info': {}, 'config': {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}}, (1, 0, 0): {'config_info': {}, 'config': {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}}, (0, 0, 1): {'config_info': {}, 'config': {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}}, (0, 0, 3): {'config_info': {}, 'config': {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}}, (1, 0, 1): {'config_info': {}, 'config': {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}}}\n",
      "return best config:  {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='xgb', min_budget = 10, max_budget=40, run_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140187071411968\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.008436577394848653, 'batch_size': 19}\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015971832060604426, 'batch_size': 25}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 169us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 138us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 215us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.08455] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.10933]\n",
      " [ 0.07648]\n",
      " [ 0.06783]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.08455\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 108us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02906] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04199]\n",
      " [ 0.02392]\n",
      " [ 0.02129]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02906\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 193us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 191us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 167us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03437] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04792]\n",
      " [ 0.02945]\n",
      " [ 0.02575]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03437\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.009586457136379657, 'batch_size': 27}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 176us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 238us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03834] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.05514]\n",
      " [ 0.03206]\n",
      " [ 0.02784]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03834\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.06945643081756749, 'batch_size': 42}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 153us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 217us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 186us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03485] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04871]\n",
      " [ 0.02974]\n",
      " [ 0.02609]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03485\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 168us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 166us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 193us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03232] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04512]\n",
      " [ 0.02714]\n",
      " [ 0.02468]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03232\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 141us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 299us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03407] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04797]\n",
      " [ 0.02893]\n",
      " [ 0.02531]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03407\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 175us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 463us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 98us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02954] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04222]\n",
      " [ 0.02424]\n",
      " [ 0.02215]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02954\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 164us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 116us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 156us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03475] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04788]\n",
      " [ 0.02994]\n",
      " [ 0.02643]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03475\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 116us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 115us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01829] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02596]\n",
      " [ 0.01484]\n",
      " [ 0.01408]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01829\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 132us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 152us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 175us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03005] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0427 ]\n",
      " [ 0.02536]\n",
      " [ 0.0221 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03005\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 143us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 144us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 179us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03387] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04688]\n",
      " [ 0.0292 ]\n",
      " [ 0.02552]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03387\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013227285301914478, 'batch_size': 23}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 180us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 174us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0697] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.09383]\n",
      " [ 0.06129]\n",
      " [ 0.05398]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0697\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 159us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 123us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00868] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00941]\n",
      " [ 0.00721]\n",
      " [ 0.00941]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00868\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 142us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 150us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03079]\n",
      " [ 0.01677]\n",
      " [ 0.01646]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02134\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 141us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 189us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0307] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0426 ]\n",
      " [ 0.02644]\n",
      " [ 0.02307]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0307\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 111us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00811]\n",
      " [ 0.007  ]\n",
      " [ 0.00624]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013453747549283328, 'batch_size': 54}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 89us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 140us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.09446] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.12028]\n",
      " [ 0.08502]\n",
      " [ 0.07807]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.09446\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 167us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 136us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00976] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01218]\n",
      " [ 0.00842]\n",
      " [ 0.00868]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00976\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 155us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 145us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 132us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02208] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03183]\n",
      " [ 0.01757]\n",
      " [ 0.01684]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02208\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 126us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 180us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 155us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02839] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0411 ]\n",
      " [ 0.02349]\n",
      " [ 0.02059]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02839\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 100us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 124us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 89us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03028] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04305]\n",
      " [ 0.02544]\n",
      " [ 0.02235]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03028\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 161us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 194us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 137us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01432] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02139]\n",
      " [ 0.01098]\n",
      " [ 0.0106 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01432\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015944533862384857, 'batch_size': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 160us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 130us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.05398] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.07588]\n",
      " [ 0.046  ]\n",
      " [ 0.04005]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.05398\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.002410925627224085, 'batch_size': 52}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 278us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 128us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 190us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.04653] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.06616]\n",
      " [ 0.03931]\n",
      " [ 0.03413]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.04653\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 122us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 118us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 181us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00846]\n",
      " [ 0.00592]\n",
      " [ 0.00698]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 329us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 246us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 218us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02235] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03238]\n",
      " [ 0.01768]\n",
      " [ 0.01699]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02235\n",
      "traj {'time_finished': [2.2007694244384766, 3.744567394256592, 21.17214035987854, 31.383424997329712, 38.972952127456665], 'budgets': [12.5, 12.5, 25.0, 50.0, 100.0], 'losses': [0.08455, 0.02906, 0.01829, 0.00868, 0.00712], 'config_ids': [(0, 0, 1), (0, 0, 2), (0, 0, 2), (0, 0, 2), (0, 0, 2)]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(1, 0, 3): {'config': {'lr': 0.0013453747549283328, 'batch_size': 54}, 'config_info': {}}, (0, 0, 7): {'config': {'lr': 0.021337462797969355, 'batch_size': 40}, 'config_info': {}}, (2, 0, 3): {'config': {'lr': 0.002410925627224085, 'batch_size': 52}, 'config_info': {}}, (0, 0, 2): {'config': {'lr': 0.37364476899503524, 'batch_size': 48}, 'config_info': {}}, (1, 0, 0): {'config': {'lr': 0.07769230011012973, 'batch_size': 41}, 'config_info': {}}, (0, 0, 6): {'config': {'lr': 0.07388122579837485, 'batch_size': 33}, 'config_info': {}}, (2, 0, 2): {'config': {'lr': 0.0015944533862384857, 'batch_size': 42}, 'config_info': {}}, (0, 0, 1): {'config': {'lr': 0.0015971832060604426, 'batch_size': 25}, 'config_info': {}}, (1, 0, 1): {'config': {'lr': 0.0013227285301914478, 'batch_size': 23}, 'config_info': {}}, (0, 0, 5): {'config': {'lr': 0.06945643081756749, 'batch_size': 42}, 'config_info': {}}, (0, 0, 0): {'config': {'lr': 0.008436577394848653, 'batch_size': 19}, 'config_info': {}}, (2, 0, 1): {'config': {'lr': 0.07673276670965525, 'batch_size': 62}, 'config_info': {}}, (0, 0, 4): {'config': {'lr': 0.009586457136379657, 'batch_size': 27}, 'config_info': {}}, (1, 0, 2): {'config': {'lr': 0.04580026289754379, 'batch_size': 43}, 'config_info': {}}, (2, 0, 0): {'config': {'lr': 0.164477508973482, 'batch_size': 30}, 'config_info': {}}, (0, 0, 3): {'config': {'lr': 0.016073094701798084, 'batch_size': 19}, 'config_info': {}}}\n",
      "return best config:  {'lr': 0.37364476899503524, 'batch_size': 48}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 10, max_budget=100, \n",
    "                       run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05283, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05283 to 0.04822, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04822 to 0.04465, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.04470, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04465 to 0.04354, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04354 to 0.04342, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.04631, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04342 to 0.04184, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04184 to 0.03921, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.04097, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03921 to 0.03690, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03690 to 0.03614, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03614 to 0.03320, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03320 to 0.03232, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03232 to 0.03137, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03137 to 0.02900, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.03021, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02900 to 0.02758, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02758 to 0.02378, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02378 to 0.02372, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02372 to 0.02174, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02174 to 0.02093, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02093 to 0.01898, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01898 to 0.01830, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.02128, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01830 to 0.01674, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01674 to 0.01596, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01596 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01556 to 0.01350, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01350 to 0.01340, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01340 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01316 to 0.01282, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01282 to 0.01170, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01170 to 0.01109, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.01383, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.01155, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01109 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00966 to 0.00964, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00969, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00964 to 0.00926, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00926 to 0.00921, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00921 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00887 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00944, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00854 to 0.00832, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01047, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00832 to 0.00812, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00812 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00790 to 0.00775, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00775 to 0.00770, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00770 to 0.00767, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00767 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00758 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00753 to 0.00751, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00751 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00745 to 0.00742, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00742 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00940, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00741 to 0.00740, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00740 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00733 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00733 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00732 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01269, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00728 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00727 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00724 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00719 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00715 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00709 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00935, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00709 to 0.00700, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.00700 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00699 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.00699 to 0.00696, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss is 0.00703, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.00696 to 0.00695, storing weights.\n",
      "\n",
      "Epoch 00299: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.00695 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00689 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00689 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00680 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00679 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00679 to 0.00669, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.00669 to 0.00666, storing weights.\n",
      "\n",
      "Epoch 00407: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.00666 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00413: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.00664 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00418: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.00658 to 0.00657, storing weights.\n",
      "\n",
      "Epoch 00424: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.00657 to 0.00652, storing weights.\n",
      "\n",
      "Epoch 00428: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.00652 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00431: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.00647 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00433: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.00645 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00447: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.00642 to 0.00638, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00449: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00638 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00490: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.00634 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00505: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.00624 to 0.00623, storing weights.\n",
      "\n",
      "Epoch 00515: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.00623 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00569: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.00620 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.00618 to 0.00617, storing weights.\n",
      "\n",
      "Epoch 00574: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00713, did not improve\n",
      "Epoch 00589: early stopping\n",
      "Using epoch 00573 with val_loss: 0.00617\n",
      "89/89 [==============================] - 0s 147us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03010, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03055, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03010 to 0.02792, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02792 to 0.02726, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02965, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02726 to 0.02655, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02655 to 0.02478, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02478 to 0.02292, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02292 to 0.02190, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02190 to 0.02047, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02047 to 0.02037, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.02037 to 0.01899, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01899 to 0.01775, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01775 to 0.01697, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01697 to 0.01573, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01636, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01573 to 0.01439, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01468, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01439 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01316 to 0.01254, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01254 to 0.01225, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01225 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01106 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01068 to 0.01052, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01052 to 0.01034, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01034 to 0.00997, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00997 to 0.00945, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00945 to 0.00922, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00922 to 0.00892, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00892 to 0.00824, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00824 to 0.00786, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00786 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00732 to 0.00731, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00731 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00727 to 0.00697, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00697 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00668 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00634 to 0.00626, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00626 to 0.00612, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00612 to 0.00589, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00954, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00589 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00582 to 0.00569, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00569 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00556 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00551 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00549 to 0.00547, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00547 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00539 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00539 to 0.00533, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00538, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00169: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00565, did not improve\n",
      "Epoch 00179: early stopping\n",
      "Using epoch 00164 with val_loss: 0.00533\n",
      "88/88 [==============================] - 0s 133us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02599, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02599 to 0.02511, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.02629, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.02653, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02511 to 0.02334, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02334 to 0.02258, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02258 to 0.02167, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02167 to 0.02067, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02067 to 0.01985, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01985 to 0.01933, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01933 to 0.01810, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01810 to 0.01763, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01763 to 0.01690, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.01720, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01690 to 0.01602, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01670, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01602 to 0.01461, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01461 to 0.01421, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01462, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01421 to 0.01341, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01341 to 0.01294, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01294 to 0.01205, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01205 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01202 to 0.01154, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.01522, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01154 to 0.01115, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01115 to 0.01074, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01074 to 0.01013, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01013 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00966 to 0.00914, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00914 to 0.00879, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00879 to 0.00853, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00853 to 0.00821, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00821 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00960, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00762 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00939, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00724 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00684 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00680 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00661 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00655 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.01441, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.01015, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.01199, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00642 to 0.00636, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00636 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00618 to 0.00609, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00609 to 0.00600, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.01702, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00600 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00794, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_loss improved from 0.00599 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00558 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00556 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00548 to 0.00534, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00534 to 0.00517, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00910, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00517 to 0.00492, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00492 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00658, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00298: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00496, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00986, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00955, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00578, did not improve\n",
      "Epoch 00322: early stopping\n",
      "Using epoch 00262 with val_loss: 0.00490\n",
      "88/88 [==============================] - 0s 154us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00547] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00617]\n",
      " [ 0.00533]\n",
      " [ 0.0049 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# best solution for MLP\n",
    "best_config = {'lr': 0.37364476899503524, 'batch_size': 48}\n",
    "results = m.eval_cv('mlp', configs, Y, cfg=best_cfg, epochs=1000, splits = 3, earlystop=True, \n",
    "                    dropout=False, lr_exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139639964378880\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.05958020697008728, 'l2': 0.00012441938511358324, 'l1': 0.0010959499184547534}\n",
      "create mlp using L1L2 regularisation\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0283] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04038]\n",
      " [ 0.02392]\n",
      " [ 0.0206 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02514] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02384]\n",
      " [ 0.02672]\n",
      " [ 0.02486]]\n",
      "hyperband obj crossval results 0.0283\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 19, 'lr': 0.05982392470292955, 'l2': 0.0035279467257703485, 'l1': 0.000599326734940943}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03408] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04816]\n",
      " [ 0.02858]\n",
      " [ 0.02548]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03285] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02735]\n",
      " [ 0.03488]\n",
      " [ 0.03633]]\n",
      "hyperband obj crossval results 0.03408\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03306] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0468 ]\n",
      " [ 0.02793]\n",
      " [ 0.02445]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03256] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02721]\n",
      " [ 0.0342 ]\n",
      " [ 0.03628]]\n",
      "hyperband obj crossval results 0.03306\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.05782109870299804, 'l2': 0.0023711269546917556, 'l1': 0.001003613674878896}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03322] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04637]\n",
      " [ 0.02843]\n",
      " [ 0.02487]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03257] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02728]\n",
      " [ 0.03458]\n",
      " [ 0.03585]]\n",
      "hyperband obj crossval results 0.03322\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03233] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04368]\n",
      " [ 0.02953]\n",
      " [ 0.02378]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03104] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02667]\n",
      " [ 0.03428]\n",
      " [ 0.03216]]\n",
      "hyperband obj crossval results 0.03233\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03315] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04689]\n",
      " [ 0.02798]\n",
      " [ 0.02458]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03212] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0267 ]\n",
      " [ 0.03415]\n",
      " [ 0.03552]]\n",
      "hyperband obj crossval results 0.03315\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 27, 'lr': 0.08456542078923197, 'l2': 0.00046986297806195807, 'l1': 0.0022887938499136415}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.034] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04703]\n",
      " [ 0.0295 ]\n",
      " [ 0.02548]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03349] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02753]\n",
      " [ 0.03564]\n",
      " [ 0.03729]]\n",
      "hyperband obj crossval results 0.034\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03147] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04471]\n",
      " [ 0.0264 ]\n",
      " [ 0.0233 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03007] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02607]\n",
      " [ 0.03159]\n",
      " [ 0.03254]]\n",
      "hyperband obj crossval results 0.03147\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02576] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0388 ]\n",
      " [ 0.01976]\n",
      " [ 0.01873]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02189] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02362]\n",
      " [ 0.02112]\n",
      " [ 0.02093]]\n",
      "hyperband obj crossval results 0.02576\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02796] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04167]\n",
      " [ 0.02221]\n",
      " [ 0.02001]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02512] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02429]\n",
      " [ 0.02589]\n",
      " [ 0.02517]]\n",
      "hyperband obj crossval results 0.02796\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02099] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03013]\n",
      " [ 0.01709]\n",
      " [ 0.01576]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01721] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01898]\n",
      " [ 0.01703]\n",
      " [ 0.01562]]\n",
      "hyperband obj crossval results 0.02099\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02784] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04454]\n",
      " [ 0.01868]\n",
      " [ 0.02031]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0232] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02624]\n",
      " [ 0.0218 ]\n",
      " [ 0.02154]]\n",
      "hyperband obj crossval results 0.02784\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02776] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04287]\n",
      " [ 0.02069]\n",
      " [ 0.01971]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02463] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02577]\n",
      " [ 0.02562]\n",
      " [ 0.0225 ]]\n",
      "hyperband obj crossval results 0.02776\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01785] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02822]\n",
      " [ 0.0116 ]\n",
      " [ 0.01372]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01427] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01813]\n",
      " [ 0.01192]\n",
      " [ 0.01274]]\n",
      "hyperband obj crossval results 0.01785\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01044] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01458]\n",
      " [ 0.00804]\n",
      " [ 0.00869]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00851] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00939]\n",
      " [ 0.00818]\n",
      " [ 0.00794]]\n",
      "hyperband obj crossval results 0.01044\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.026071750020129045, 'l2': 0.0001408448332681315, 'l1': 0.0003860780520028507}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03257] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04531]\n",
      " [ 0.02806]\n",
      " [ 0.02435]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03181] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02647]\n",
      " [ 0.03368]\n",
      " [ 0.03528]]\n",
      "hyperband obj crossval results 0.03257\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0093] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01171]\n",
      " [ 0.00781]\n",
      " [ 0.00837]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00723] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00752]\n",
      " [ 0.00776]\n",
      " [ 0.00641]]\n",
      "hyperband obj crossval results 0.0093\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 27, 'lr': 0.05899350466011258, 'l2': 0.0029782618629786476, 'l1': 0.001283047465048529}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03304] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04585]\n",
      " [ 0.02838]\n",
      " [ 0.02489]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03202] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02692]\n",
      " [ 0.03408]\n",
      " [ 0.03508]]\n",
      "hyperband obj crossval results 0.03304\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.02153014369878981, 'l2': 0.0010743591528754566, 'l1': 0.0011507205935786594}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03252] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04585]\n",
      " [ 0.02745]\n",
      " [ 0.02426]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03147] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02674]\n",
      " [ 0.03322]\n",
      " [ 0.03446]]\n",
      "hyperband obj crossval results 0.03252\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02784] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04247]\n",
      " [ 0.02155]\n",
      " [ 0.01952]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02452] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02496]\n",
      " [ 0.02532]\n",
      " [ 0.0233 ]]\n",
      "hyperband obj crossval results 0.02784\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0264] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0488 ]\n",
      " [ 0.01395]\n",
      " [ 0.01645]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0193] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0285 ]\n",
      " [ 0.01448]\n",
      " [ 0.01492]]\n",
      "hyperband obj crossval results 0.0264\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0312] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04851]\n",
      " [ 0.02492]\n",
      " [ 0.02016]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02561] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02845]\n",
      " [ 0.02678]\n",
      " [ 0.02161]]\n",
      "hyperband obj crossval results 0.0312\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01633] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01849]\n",
      " [ 0.00755]\n",
      " [ 0.02295]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01757] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01465]\n",
      " [ 0.00937]\n",
      " [ 0.02869]]\n",
      "hyperband obj crossval results 0.01633\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00989] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0129 ]\n",
      " [ 0.00807]\n",
      " [ 0.00871]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0076] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00845]\n",
      " [ 0.00759]\n",
      " [ 0.00676]]\n",
      "hyperband obj crossval results 0.00989\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 29, 'lr': 0.08513416180024498, 'l2': 0.0022384130227352055, 'l1': 0.0027488065028525646}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03834] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04963]\n",
      " [ 0.03753]\n",
      " [ 0.02785]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03648] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02869]\n",
      " [ 0.04216]\n",
      " [ 0.03859]]\n",
      "hyperband obj crossval results 0.03834\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00746] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00891]\n",
      " [ 0.0066 ]\n",
      " [ 0.00687]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00542] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00572]\n",
      " [ 0.00564]\n",
      " [ 0.0049 ]]\n",
      "hyperband obj crossval results 0.00746\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01425] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02365]\n",
      " [ 0.00897]\n",
      " [ 0.01013]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01207] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01722]\n",
      " [ 0.01023]\n",
      " [ 0.00875]]\n",
      "hyperband obj crossval results 0.01425\n",
      "traj {'time_finished': [1.991438388824463, 12.025814056396484, 15.350425481796265, 20.988491535186768, 23.55812168121338, 32.916274309158325, 61.148550033569336], 'losses': [0.0283, 0.02576, 0.02099, 0.01785, 0.01044, 0.0093, 0.00746], 'budgets': [12.5, 25.0, 25.0, 50.0, 50.0, 100.0, 100.0], 'config_ids': [(0, 0, 1), (0, 0, 5), (0, 0, 1), (0, 0, 5), (0, 0, 1), (0, 0, 1), (2, 0, 2)]}\n",
      "best_cfg_id (2, 0, 2)\n",
      "all_configs {(1, 0, 3): {'config_info': {}, 'config': {'batch_size': 27, 'lr': 0.05899350466011258, 'l2': 0.0029782618629786476, 'l1': 0.001283047465048529}}, (0, 0, 7): {'config_info': {}, 'config': {'batch_size': 27, 'lr': 0.08456542078923197, 'l2': 0.00046986297806195807, 'l1': 0.0022887938499136415}}, (2, 0, 3): {'config_info': {}, 'config': {'batch_size': 29, 'lr': 0.08513416180024498, 'l2': 0.0022384130227352055, 'l1': 0.0027488065028525646}}, (0, 0, 2): {'config_info': {}, 'config': {'batch_size': 19, 'lr': 0.05982392470292955, 'l2': 0.0035279467257703485, 'l1': 0.000599326734940943}}, (1, 0, 0): {'config_info': {}, 'config': {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}}, (0, 0, 6): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}}, (2, 0, 2): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}}, (0, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}}, (1, 0, 1): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}}, (0, 0, 5): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}}, (0, 0, 0): {'config_info': {}, 'config': {'batch_size': 30, 'lr': 0.05958020697008728, 'l2': 0.00012441938511358324, 'l1': 0.0010959499184547534}}, (2, 0, 1): {'config_info': {}, 'config': {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}}, (0, 0, 4): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.05782109870299804, 'l2': 0.0023711269546917556, 'l1': 0.001003613674878896}}, (1, 0, 2): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.026071750020129045, 'l2': 0.0001408448332681315, 'l1': 0.0003860780520028507}}, (2, 0, 0): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.02153014369878981, 'l2': 0.0010743591528754566, 'l1': 0.0011507205935786594}}, (0, 0, 3): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}}}\n",
      "return best config:  {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 10, max_budget=100, \n",
    "                       run_name='', earlystop=False, L1L2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139639964378880\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.1874018718283377}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 57305.17739 / 49320.43900\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.35682 / 1.32998\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.63600 / 1.60417\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16441.12438] ***\n",
      "Results validation data of all Folds: \n",
      "[[  4.93204390e+04]\n",
      " [  1.32998000e+00]\n",
      " [  1.60417000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 19102.7234] ***\n",
      "Results training data of all Folds: \n",
      "[[  5.73051774e+04]\n",
      " [  1.35682000e+00]\n",
      " [  1.63600000e+00]]\n",
      "hyperband obj crossval results 16441.12438\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.58379 / 6.78356\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.17556 / 1.32706\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.66769 / 5.89443\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.66835] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 6.78356]\n",
      " [ 1.32706]\n",
      " [ 5.89443]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.47568] ***\n",
      "Results training data of all Folds: \n",
      "[[ 6.58379]\n",
      " [ 1.17556]\n",
      " [ 5.66769]]\n",
      "hyperband obj crossval results 4.66835\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.06374 / 0.08929\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.94452 / 2.82466\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.88062 / 3.28236\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.06544] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.08929]\n",
      " [ 2.82466]\n",
      " [ 3.28236]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96296] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.06374]\n",
      " [ 2.94452]\n",
      " [ 2.88062]]\n",
      "hyperband obj crossval results 2.06544\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 39.15438 / 36.29156\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.85508 / 2.74689\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.94695 / 0.96315\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.33386] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 36.29156]\n",
      " [  2.74689]\n",
      " [  0.96315]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 14.3188] ***\n",
      "Results training data of all Folds: \n",
      "[[ 39.15438]\n",
      " [  2.85508]\n",
      " [  0.94695]]\n",
      "hyperband obj crossval results 13.33386\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 25, 'lr': 0.3835397084896682}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 173541.34126 / 202132.37460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1651.36476 / 1902.48543\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.59737 / 6.48495\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 68013.78166] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.02132375e+05]\n",
      " [  1.90248543e+03]\n",
      " [  6.48495000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 58399.7678] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.73541341e+05]\n",
      " [  1.65136476e+03]\n",
      " [  6.59737000e+00]]\n",
      "hyperband obj crossval results 68013.78166\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.17632548175046464}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1985.91012 / 1775.64284\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 34.40116 / 34.60897\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.14322 / 0.18056\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 603.47746] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.77564284e+03]\n",
      " [  3.46089700e+01]\n",
      " [  1.80560000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 673.48483] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98591012e+03]\n",
      " [  3.44011600e+01]\n",
      " [  1.43220000e-01]]\n",
      "hyperband obj crossval results 603.47746\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.45587 / 2.55588\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.94175 / 6.95478\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.55024 / 3.54978\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.35348] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.55588]\n",
      " [ 6.95478]\n",
      " [ 3.54978]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.31595] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.45587]\n",
      " [ 6.94175]\n",
      " [ 3.55024]]\n",
      "hyperband obj crossval results 4.35348\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 19, 'lr': 0.003469030372105559}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.83829 / 16.07803\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.03017 / 5.83519\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 25.34513 / 25.21779\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 15.71033] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 16.07803]\n",
      " [  5.83519]\n",
      " [ 25.21779]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.73786] ***\n",
      "Results training data of all Folds: \n",
      "[[ 13.83829]\n",
      " [  8.03017]\n",
      " [ 25.34513]]\n",
      "hyperband obj crossval results 15.71033\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.28430749054335513}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3472.04396 / 3523.06327\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 0.63654 / 0.69261\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.49294 / 0.53654\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1174.76414] ***\n",
      "Results validation data of all Folds: \n",
      "[[  3.52306327e+03]\n",
      " [  6.92610000e-01]\n",
      " [  5.36540000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1157.72448] ***\n",
      "Results training data of all Folds: \n",
      "[[  3.47204396e+03]\n",
      " [  6.36540000e-01]\n",
      " [  4.92940000e-01]]\n",
      "hyperband obj crossval results 1174.76414\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.07994 / 5.19139\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98066 / 1.25787\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.47991 / 2.00694\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.81873] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19139]\n",
      " [ 1.25787]\n",
      " [ 2.00694]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.51351] ***\n",
      "Results training data of all Folds: \n",
      "[[ 3.07994]\n",
      " [ 1.98066]\n",
      " [ 2.47991]]\n",
      "hyperband obj crossval results 2.81873\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 53.63178 / 49.70385\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 27.54559 / 24.83507\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.28858 / 1.20521\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 25.24804] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 49.70385]\n",
      " [ 24.83507]\n",
      " [  1.20521]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 27.48865] ***\n",
      "Results training data of all Folds: \n",
      "[[ 53.63178]\n",
      " [ 27.54559]\n",
      " [  1.28858]]\n",
      "hyperband obj crossval results 25.24804\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.78026 / 2.92213\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05201 / 0.04299\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.33999 / 1.72137\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.56216] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.92213]\n",
      " [ 0.04299]\n",
      " [ 1.72137]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.39075] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.78026]\n",
      " [ 0.05201]\n",
      " [ 2.33999]]\n",
      "hyperband obj crossval results 1.56216\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 37.93051 / 35.21599\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 14.08750 / 11.14776\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 10.54999 / 8.86799\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 18.41058] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 35.21599]\n",
      " [ 11.14776]\n",
      " [  8.86799]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 20.856] ***\n",
      "Results training data of all Folds: \n",
      "[[ 37.93051]\n",
      " [ 14.0875 ]\n",
      " [ 10.54999]]\n",
      "hyperband obj crossval results 18.41058\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 44, 'lr': 0.4913996332060787}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1575585.33856 / 1698166.62906\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1294.57183 / 1544.12482\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.51372 / 2.53258\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 566571.09549] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.69816663e+06]\n",
      " [  1.54412482e+03]\n",
      " [  2.53258000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 525627.4747] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.57558534e+06]\n",
      " [  1.29457183e+03]\n",
      " [  2.51372000e+00]]\n",
      "hyperband obj crossval results 566571.09549\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.95017 / 1.32579\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04080 / 0.03309\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04376 / 0.02759\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.46215] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.32579]\n",
      " [ 0.03309]\n",
      " [ 0.02759]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34491] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.95017]\n",
      " [ 0.0408 ]\n",
      " [ 0.04376]]\n",
      "hyperband obj crossval results 0.46215\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.62112 / 4.18508\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04926 / 0.04060\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04391 / 0.02767\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.41778] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 4.18508]\n",
      " [ 0.0406 ]\n",
      " [ 0.02767]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.90476] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.62112]\n",
      " [ 0.04926]\n",
      " [ 0.04391]]\n",
      "hyperband obj crossval results 1.41778\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.30579 / 17.29121\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.52870 / 9.34835\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.08090 / 13.25739\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.29898] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 17.29121]\n",
      " [  9.34835]\n",
      " [ 13.25739]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 13.63846] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.30579]\n",
      " [ 11.5287 ]\n",
      " [ 13.0809 ]]\n",
      "hyperband obj crossval results 13.29898\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.37079 / 0.41493\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04794 / 0.03834\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.68127 / 4.39918\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.61748] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.41493]\n",
      " [ 0.03834]\n",
      " [ 4.39918]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.03333] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.37079]\n",
      " [ 0.04794]\n",
      " [ 5.68127]]\n",
      "hyperband obj crossval results 1.61748\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 853.34644 / 905.47774\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.08386 / 0.08086\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05171 / 0.05630\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 301.87164] ***\n",
      "Results validation data of all Folds: \n",
      "[[  9.05477740e+02]\n",
      " [  8.08600000e-02]\n",
      " [  5.63000000e-02]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 284.494] ***\n",
      "Results training data of all Folds: \n",
      "[[  8.53346440e+02]\n",
      " [  8.38600000e-02]\n",
      " [  5.17100000e-02]]\n",
      "hyperband obj crossval results 301.87164\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.45195 / 0.61088\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04009 / 0.03219\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.13532 / 0.25532\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.29946] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.61088]\n",
      " [ 0.03219]\n",
      " [ 0.25532]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.20912] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.45195]\n",
      " [ 0.04009]\n",
      " [ 0.13532]]\n",
      "hyperband obj crossval results 0.29946\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.79849 / 5.19062\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04337 / 0.04290\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04351 / 0.04174\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.75842] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19062]\n",
      " [ 0.0429 ]\n",
      " [ 0.04174]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96179] ***\n",
      "Results training data of all Folds: \n",
      "[[ 5.79849]\n",
      " [ 0.04337]\n",
      " [ 0.04351]]\n",
      "hyperband obj crossval results 1.75842\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.55753 / 20.22860\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.72184 / 6.69817\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.85380 / 4.63014\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 10.51897] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 20.2286 ]\n",
      " [  6.69817]\n",
      " [  4.63014]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 10.71106] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.55753]\n",
      " [  8.72184]\n",
      " [  6.8538 ]]\n",
      "hyperband obj crossval results 10.51897\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.0012756729206255175}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 88.28766 / 88.52939\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 9.23875 / 6.21818\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 15.96105 / 11.43095\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 35.39284] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 88.52939]\n",
      " [  6.21818]\n",
      " [ 11.43095]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 37.82915] ***\n",
      "Results training data of all Folds: \n",
      "[[ 88.28766]\n",
      " [  9.23875]\n",
      " [ 15.96105]]\n",
      "hyperband obj crossval results 35.39284\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 38.78400 / 41.18006\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.46152 / 2.47339\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 4.26055 / 4.35291\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16.00212] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 41.18006]\n",
      " [  2.47339]\n",
      " [  4.35291]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.16869] ***\n",
      "Results training data of all Folds: \n",
      "[[ 38.784  ]\n",
      " [  2.46152]\n",
      " [  4.26055]]\n",
      "hyperband obj crossval results 16.00212\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.001051445164047007}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 50.41666 / 48.16330\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.99796 / 9.56694\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.71480 / 8.87737\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 22.20254] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 48.1633 ]\n",
      " [  9.56694]\n",
      " [  8.87737]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 25.37647] ***\n",
      "Results training data of all Folds: \n",
      "[[ 50.41666]\n",
      " [ 13.99796]\n",
      " [ 11.7148 ]]\n",
      "hyperband obj crossval results 22.20254\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.84544 / 1.95491\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.26675 / 0.27446\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 1.42394 / 1.42303\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.21747] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.95491]\n",
      " [ 0.27446]\n",
      " [ 1.42303]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.17871] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.84544]\n",
      " [ 0.26675]\n",
      " [ 1.42394]]\n",
      "hyperband obj crossval results 1.21747\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98941 / 1.91761\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.48118 / 6.42872\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 18.31786 / 18.51387\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 8.9534] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.91761]\n",
      " [  6.42872]\n",
      " [ 18.51387]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 8.92948] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98941]\n",
      " [  6.48118]\n",
      " [ 18.31786]]\n",
      "hyperband obj crossval results 8.9534\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.80275 / 1.05999\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05447 / 0.04670\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.19026 / 0.20350\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.43673] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.05999]\n",
      " [ 0.0467 ]\n",
      " [ 0.2035 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34916] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.80275]\n",
      " [ 0.05447]\n",
      " [ 0.19026]]\n",
      "hyperband obj crossval results 0.43673\n",
      "traj {'time_finished': [22.488330125808716, 60.9959557056427, 91.6520767211914, 313.71562457084656, 390.30302476882935, 627.3354201316833], 'losses': [16441.12438, 4.66835, 2.06544, 1.56216, 0.46215, 0.29946], 'budgets': [5.0, 5.0, 5.0, 10.0, 20.0, 20.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 1), (0, 0, 2), (2, 0, 0)]}\n",
      "best_cfg_id (2, 0, 0)\n",
      "all_configs {(1, 0, 3): {'config_info': {}, 'config': {'batch_size': 26, 'lr': 0.1579507660887153}}, (0, 0, 7): {'config_info': {}, 'config': {'batch_size': 19, 'lr': 0.003469030372105559}}, (2, 0, 3): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.06641032495399549}}, (0, 0, 2): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.0076099528036695455}}, (1, 0, 0): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.28430749054335513}}, (0, 0, 6): {'config_info': {}, 'config': {'batch_size': 22, 'lr': 0.0010424757895254506}}, (2, 0, 2): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.001051445164047007}}, (0, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.007725817805663781}}, (1, 0, 1): {'config_info': {}, 'config': {'batch_size': 44, 'lr': 0.4913996332060787}}, (0, 0, 5): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.17632548175046464}}, (0, 0, 0): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.1874018718283377}}, (2, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.0012756729206255175}}, (0, 0, 4): {'config_info': {}, 'config': {'batch_size': 25, 'lr': 0.3835397084896682}}, (1, 0, 2): {'config_info': {}, 'config': {'batch_size': 40, 'lr': 0.002370731368911436}}, (2, 0, 0): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.023123758972112808}}, (0, 0, 3): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.001921554114714757}}}\n",
      "return best config:  {'batch_size': 21, 'lr': 0.023123758972112808}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize([configs,lcs], Y, model_type='multi_lstm', \n",
    "                       min_budget = 4, max_budget=40, run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15458, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15458 to 0.09678, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09678 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.03820, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02604 to 0.01416, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01416 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01231 to 0.00266, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00400, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00266 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00153 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00336, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00143 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00132 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00126 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00122 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00116 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00108 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00105 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00102 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00095 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00087 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00079 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00069 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00065 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00052 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00048 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00066, did not improve\n",
      "Epoch 00144: early stopping\n",
      "Using epoch 00086 with val_loss: 0.00045\n",
      "validate on 30 steps, mse on train / validation data: 0.17828 / 0.28994\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.90476, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 1361.45895, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 510.44960, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 255.13004, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 54.13024, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 21.52839, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.90476 to 4.37317, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.37317 to 1.53281, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.53281 to 0.45816, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45816 to 0.10284, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10284 to 0.10064, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10064 to 0.03229, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03229 to 0.01450, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01450 to 0.00979, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00979 to 0.00822, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00822 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00662 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00488 to 0.00424, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00424 to 0.00388, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00388 to 0.00365, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00365 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00335 to 0.00313, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00313 to 0.00293, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00293 to 0.00274, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00274 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00257 to 0.00243, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00243 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00231 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00220 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00210 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00201 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00193 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00186 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00179 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00172 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00166 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00161 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00157 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00154 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00151 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00148 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00145 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00143 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00140 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00138 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00136 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00134 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00132 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00129 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00126 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00121 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00115 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00105 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00080 to 0.00079, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00136, did not improve\n",
      "Epoch 00184: early stopping\n",
      "Using epoch 00134 with val_loss: 0.00069\n",
      "validate on 30 steps, mse on train / validation data: 1.26030 / 1.59993\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.09429, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.03942, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03170 to 0.02370, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02402, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02370 to 0.02337, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02337 to 0.01970, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01970 to 0.01745, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01745 to 0.01647, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01647 to 0.01517, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01517 to 0.01348, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01348 to 0.01201, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01201 to 0.01066, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01066 to 0.00930, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00930 to 0.00798, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00798 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00680 to 0.00574, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00574 to 0.00484, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00484 to 0.00409, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00409 to 0.00350, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00350 to 0.00307, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00307 to 0.00276, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00276 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00257 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00245 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00239 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00235 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00234 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00234 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00233 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00232 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00232 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00231 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00230 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00230 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00228 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00226 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00226 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00225 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00225 to 0.00224, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_loss improved from 0.00224 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00223 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00222 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00220 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00220 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00219 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00219 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00218 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00217 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00216 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00215 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00215 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00214 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00214 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00213 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00213 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00212 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00211 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00210 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00210 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00209 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00209 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00208 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00208 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00206 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00206 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00205 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00205 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00204 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00201 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00200 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00197 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00196 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00193 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00192 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00176 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00174 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00171 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00169 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00168 to 0.00168, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00159: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00166 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00116 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00139, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.00099 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00383: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00070, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00405: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00077, did not improve\n",
      "Epoch 00432: early stopping\n",
      "Using epoch 00382 with val_loss: 0.00068\n",
      "validate on 30 steps, mse on train / validation data: 0.13800 / 0.08938\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.73713  0.79544  0.92657  0.65975] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.70987  0.61491  0.50921  0.28994]\n",
      " [ 1.45444  1.73461  2.25349  1.59993]\n",
      " [ 0.04709  0.03681  0.01701  0.08938]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.54098  0.66638  0.72073  0.52552] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.52994  0.51729  0.3413   0.17828]\n",
      " [ 1.02344  1.42766  1.79191  1.2603 ]\n",
      " [ 0.06957  0.0542   0.02898  0.138  ]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11671, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12661, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11671 to 0.05024, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05024 to 0.02909, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.03532, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02909 to 0.00954, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00954 to 0.00902, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00902 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00634 to 0.00631, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00631 to 0.00496, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00496 to 0.00286, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00286 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00172 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00073 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00052 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00047 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00506, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00044 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00040, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00416, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00640, did not improve\n",
      "Epoch 00133: early stopping\n",
      "Using epoch 00094 with val_loss: 0.00038\n",
      "validate on 30 steps, mse on train / validation data: 0.01650 / 0.01693\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 378.56074, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 378.56074 to 30.09993, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 34.44132, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 30.09993 to 3.21959, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 4.52868, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.21959 to 0.70476, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 1.93981, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70476 to 0.47590, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47590 to 0.15103, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15103 to 0.06758, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06758 to 0.05877, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05877 to 0.03435, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03435 to 0.03089, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03089 to 0.03066, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03066 to 0.02833, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.02988, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02833 to 0.02818, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.02889, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02818 to 0.02816, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02824, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02830, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00019 with val_loss: 0.02816\n",
      "validate on 30 steps, mse on train / validation data: 0.03174 / 0.02943\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04151 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01769 to 0.00459, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00459 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00235 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00191 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00095 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00046 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00039 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00033 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00029, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00029, did not improve\n",
      "Epoch 00085: early stopping\n",
      "Using epoch 00056 with val_loss: 0.00029\n",
      "validate on 30 steps, mse on train / validation data: 0.01208 / 0.00941\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.08369  0.04828  0.03323  0.01859] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.1925   0.08568  0.04499  0.01693]\n",
      " [ 0.02943  0.02943  0.02943  0.02943]\n",
      " [ 0.02916  0.02972  0.02526  0.00941]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.07582  0.05066  0.03704  0.02011] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.14894  0.07332  0.04103  0.0165 ]\n",
      " [ 0.03174  0.03174  0.03174  0.03174]\n",
      " [ 0.04678  0.04693  0.03834  0.01208]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32145, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32145 to 0.12631, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12631 to 0.10577, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10577 to 0.06052, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06052 to 0.01204, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.01487, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01204 to 0.00730, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00730 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00380, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00228 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00097 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00063 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00054 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00077, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00222, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00017 with val_loss: 0.00048\n",
      "validate on 30 steps, mse on train / validation data: 0.00203 / 0.00196\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05475, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12115, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.08707, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05475 to 0.05174, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05174 to 0.04611, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04611 to 0.02661, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02661 to 0.01495, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01495 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00808 to 0.00483, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00483 to 0.00260, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00260 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00112 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00054 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00045 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00025, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00089 with val_loss: 0.00025\n",
      "validate on 30 steps, mse on train / validation data: 0.00131 / 0.00129\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51855, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51855 to 0.09694, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss is 14.75946, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 275.30684, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 13.29729, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 8.29154, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.53958, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 10.31797, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 1.31900, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.25212, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.24620, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09694 to 0.05898, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05898 to 0.04390, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04390 to 0.03132, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.03260, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03132 to 0.02776, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.02879, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02776 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02769, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02754 to 0.02724, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.02757, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02750, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02734, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02735, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02732, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02730, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02727, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02725, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02724 to 0.02722, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02722 to 0.02720, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02720 to 0.02718, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02718 to 0.02716, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02716 to 0.02714, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.02714 to 0.02711, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02711 to 0.02709, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.02709 to 0.02707, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.02707 to 0.02705, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.02705 to 0.02703, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.02703 to 0.02701, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.02701 to 0.02699, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.02699 to 0.02697, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02697 to 0.02696, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.02696 to 0.02694, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.02694 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.02692 to 0.02690, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.02690 to 0.02689, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.02689 to 0.02687, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.02687 to 0.02686, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02686 to 0.02684, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.02684 to 0.02683, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02683 to 0.02681, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02681 to 0.02680, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02680 to 0.02679, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02679 to 0.02678, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02678 to 0.02677, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02677 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02676 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02676 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02677, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02678, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02680, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02681, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02683, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02686, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02688, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02691, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02695, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02698, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02703, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02707, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02712, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02718, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02724, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02731, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02756, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02765, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02775, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02787, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02799, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02812, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.02857, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.02874, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.02892, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.02912, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.02933, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.02955, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.02979, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.03003, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.03030, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.03058, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.03087, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.03118, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.03151, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.03185, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.03220, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.03257, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.03296, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.03336, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.03377, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.03420, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.03463, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.03508, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.03554, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.03601, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.03648, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.03696, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.03744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.03792, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.03840, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.03888, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.03936, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.03983, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.04029, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.04074, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.04118, did not improve\n",
      "Epoch 00129: early stopping\n",
      "Using epoch 00064 with val_loss: 0.02675\n",
      "validate on 30 steps, mse on train / validation data: 0.02955 / 0.02495\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02429  0.01109  0.0104   0.0094 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04105  0.00511  0.00295  0.00196]\n",
      " [ 0.00689  0.00322  0.00331  0.00129]\n",
      " [ 0.02495  0.02495  0.02495  0.02495]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0211   0.01247  0.01185  0.01096] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02463  0.00444  0.00319  0.00203]\n",
      " [ 0.00913  0.00341  0.00281  0.00131]\n",
      " [ 0.02955  0.02955  0.02955  0.02955]]\n",
      "results validation data \n",
      " [[ 0.73713  0.79544  0.92657  0.65975]\n",
      " [ 0.08369  0.04828  0.03323  0.01859]\n",
      " [ 0.02429  0.01109  0.0104   0.0094 ]]\n",
      "results training data\n",
      " [[ 0.54098  0.66638  0.72073  0.52552]\n",
      " [ 0.07582  0.05066  0.03704  0.02011]\n",
      " [ 0.0211   0.01247  0.01185  0.01096]]\n"
     ]
    }
   ],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=best_cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)\n",
    "# results were far better with lr = 0.002 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                     mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "build lstm with input_dim: 6\n",
      "Train on 200 samples, validate on 65 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0743 - mean_squared_error: 0.0743 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "65/65 [==============================] - 0s 843us/step\n",
      "mse:  0.00873059442697\n"
     ]
    }
   ],
   "source": [
    "# experiment with concatenating the config to each data point of learning curve\n",
    "timesteps = 5\n",
    "configs,lcs,Y = t.load_lstm_data_concat_cfg(timesteps=timesteps)\n",
    "model_type = 'lstm'\n",
    "model = m.lstm(lcs[0][0].shape[0])\n",
    "m.train_lstm(model, lcs, Y, split=200, batch_size=20, epochs=5)\n",
    "mse = m.eval_lstm(model, lcs, Y, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_curves' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-b1ef2fff93a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_curves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subset of learning curves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_curves' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_subset=20\n",
    "n_epochs = 40\n",
    "t_idx = np.arange(1, n_epochs+1)\n",
    "\n",
    "[plt.plot(t_idx, lc) for lc in learning_curves[:n_subset]]\n",
    "plt.title(\"Subset of learning curves\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over the final error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf648b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf6cbbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted = np.sort(lcs[:, -1])   # sorted list of final val error\n",
    "print(len(sorted))\n",
    "h = plt.hist(sorted, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(len(sorted))/float(len(sorted))   # from 0 to 1 in 265 even steps\n",
    "plt.plot(sorted, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over all error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4935a3ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493581f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_values = np.sort(learning_curves.flatten())\n",
    "\n",
    "h = plt.hist(all_values, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(all_values.shape[0])/all_values.shape[0]\n",
    "plt.plot(all_values, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
