{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Learning Curves of Convolutional Neural Network on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tools as t\n",
    "import models as m\n",
    "import hyperband as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n"
     ]
    }
   ],
   "source": [
    "configs,lcs,Y = t.load_data(scale_configs = True)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Testing models (mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/10\n",
      "177/177 [==============================] - 2s 14ms/step - loss: 0.0554 - val_loss: 0.0263\n",
      "Epoch 2/10\n",
      "177/177 [==============================] - 0s 445us/step - loss: 0.0363 - val_loss: 0.0250\n",
      "Epoch 3/10\n",
      "177/177 [==============================] - 0s 446us/step - loss: 0.0353 - val_loss: 0.0234\n",
      "Epoch 4/10\n",
      "177/177 [==============================] - 0s 534us/step - loss: 0.0330 - val_loss: 0.0229\n",
      "Epoch 5/10\n",
      "177/177 [==============================] - 0s 443us/step - loss: 0.0305 - val_loss: 0.0252\n",
      "Epoch 6/10\n",
      "177/177 [==============================] - 0s 596us/step - loss: 0.0287 - val_loss: 0.0227\n",
      "Epoch 7/10\n",
      "177/177 [==============================] - 0s 431us/step - loss: 0.0260 - val_loss: 0.0192\n",
      "Epoch 8/10\n",
      "177/177 [==============================] - 0s 483us/step - loss: 0.0247 - val_loss: 0.0183\n",
      "Epoch 9/10\n",
      "177/177 [==============================] - 0s 683us/step - loss: 0.0224 - val_loss: 0.0185\n",
      "Epoch 10/10\n",
      "177/177 [==============================] - 0s 450us/step - loss: 0.0219 - val_loss: 0.0176\n",
      "mse train: 0.02026, mse validation 0.01757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020264899801071712, 0.017568055259537099)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20}\n",
    "model = m.mlp(cfg)\n",
    "m.train_mlp(model, configs, Y, cfg, split=177, epochs=10)\n",
    "m.eval_mlp(model, configs, Y, split=177, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 10 epochs, eval during training with 10 epochs\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 2s 283ms/step - loss: 0.3323 - mean_squared_error: 0.3323 - val_loss: 0.1488 - val_mean_squared_error: 0.1488\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 90ms/step - loss: 0.1609 - mean_squared_error: 0.1609 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 0.0673 - mean_squared_error: 0.0673 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 1s 113ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 1s 110ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 1s 99ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 1s 103ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 1s 102ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "mse train: 0.00087, mse validation 0.00116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00087328647631683377, 0.001162902298025613)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(10,10), split=150, \n",
    "             batch_size=20, epochs=30, mode='finalstep', verbose=1)\n",
    "m.eval_lstm_direct(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, eval during training with 10 epochs\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 4s 451ms/step - loss: 0.2131 - mean_squared_error: 0.2131 - val_loss: 0.0756 - val_mean_squared_error: 0.0756\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.0630 - mean_squared_error: 0.0630 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 2s 241ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 2s 231ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 2s 305ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 2s 216ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 2s 255ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "mse train: 0.00228, mse validation 0.00233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0022836772330743558, 0.0023286114485302134)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same with training on random lenghts\n",
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,10), split=150, batch_size=20, \n",
    "             epochs=30, mode='finalstep', verbose=1)\n",
    "m.eval_lstm_direct(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 527ms/step - loss: 0.2842 - mean_squared_error: 0.2842 - val_loss: 0.1273 - val_mean_squared_error: 0.1273\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.0610 - mean_squared_error: 0.0610 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 8.6942e-04 - val_mean_squared_error: 8.6942e-04\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 8.6067e-04 - val_mean_squared_error: 8.6067e-04\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 8.0088e-04 - mean_squared_error: 8.0088e-04 - val_loss: 5.3051e-04 - val_mean_squared_error: 5.3051e-04\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 6.5383e-04 - mean_squared_error: 6.5383e-04 - val_loss: 4.8940e-04 - val_mean_squared_error: 4.8940e-04\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 5.9794e-04 - mean_squared_error: 5.9794e-04 - val_loss: 4.1794e-04 - val_mean_squared_error: 4.1794e-04\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 4.8789e-04 - mean_squared_error: 4.8789e-04 - val_loss: 4.4278e-04 - val_mean_squared_error: 4.4278e-04\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 4.7472e-04 - mean_squared_error: 4.7472e-04 - val_loss: 4.8829e-04 - val_mean_squared_error: 4.8829e-04\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 4.7378e-04 - mean_squared_error: 4.7378e-04 - val_loss: 4.6291e-04 - val_mean_squared_error: 4.6291e-04\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 4.5659e-04 - mean_squared_error: 4.5659e-04 - val_loss: 4.7734e-04 - val_mean_squared_error: 4.7734e-04\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 4.7181e-04 - mean_squared_error: 4.7181e-04 - val_loss: 5.0012e-04 - val_mean_squared_error: 5.0012e-04\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 4.7167e-04 - mean_squared_error: 4.7167e-04 - val_loss: 5.0414e-04 - val_mean_squared_error: 5.0414e-04\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 4.9762e-04 - mean_squared_error: 4.9762e-04 - val_loss: 5.1562e-04 - val_mean_squared_error: 5.1562e-04\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 5.0700e-04 - mean_squared_error: 5.0700e-04 - val_loss: 5.6090e-04 - val_mean_squared_error: 5.6090e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 6.1550e-04 - mean_squared_error: 6.1550e-04 - val_loss: 5.7167e-04 - val_mean_squared_error: 5.7167e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 7.3531e-04 - mean_squared_error: 7.3531e-04 - val_loss: 8.5170e-04 - val_mean_squared_error: 8.5170e-04\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 8.7663e-04 - val_mean_squared_error: 8.7663e-04\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 7.7629e-04 - val_mean_squared_error: 7.7629e-04\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 3.5146e-04 - val_mean_squared_error: 3.5146e-04\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 5.8703e-04 - mean_squared_error: 5.8703e-04 - val_loss: 5.1905e-04 - val_mean_squared_error: 5.1905e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 117ms/step - loss: 6.5990e-04 - mean_squared_error: 6.5990e-04 - val_loss: 6.0620e-04 - val_mean_squared_error: 6.0620e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 6.2369e-04 - mean_squared_error: 6.2369e-04 - val_loss: 5.1306e-04 - val_mean_squared_error: 5.1306e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 5.4402e-04 - mean_squared_error: 5.4402e-04 - val_loss: 5.0181e-04 - val_mean_squared_error: 5.0181e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 5.4179e-04 - mean_squared_error: 5.4179e-04 - val_loss: 5.0539e-04 - val_mean_squared_error: 5.0539e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 5.5562e-04 - mean_squared_error: 5.5562e-04 - val_loss: 5.0386e-04 - val_mean_squared_error: 5.0386e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 5.4436e-04 - mean_squared_error: 5.4436e-04 - val_loss: 5.1037e-04 - val_mean_squared_error: 5.1037e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 5.5181e-04 - mean_squared_error: 5.5181e-04 - val_loss: 5.2507e-04 - val_mean_squared_error: 5.2507e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 115ms/step - loss: 5.6531e-04 - mean_squared_error: 5.6531e-04 - val_loss: 5.2816e-04 - val_mean_squared_error: 5.2816e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 5.7088e-04 - mean_squared_error: 5.7088e-04 - val_loss: 5.3062e-04 - val_mean_squared_error: 5.3062e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 5.8097e-04 - mean_squared_error: 5.8097e-04 - val_loss: 5.2733e-04 - val_mean_squared_error: 5.2733e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 5.8217e-04 - mean_squared_error: 5.8217e-04 - val_loss: 5.3640e-04 - val_mean_squared_error: 5.3640e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 6.1269e-04 - mean_squared_error: 6.1269e-04 - val_loss: 5.2189e-04 - val_mean_squared_error: 5.2189e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 6.1096e-04 - mean_squared_error: 6.1096e-04 - val_loss: 5.7000e-04 - val_mean_squared_error: 5.7000e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 7.2944e-04 - mean_squared_error: 7.2944e-04 - val_loss: 5.4656e-04 - val_mean_squared_error: 5.4656e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 110ms/step - loss: 8.0794e-04 - mean_squared_error: 8.0794e-04 - val_loss: 7.6281e-04 - val_mean_squared_error: 7.6281e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 8.4114e-04 - val_mean_squared_error: 8.4114e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 9.5205e-04 - val_mean_squared_error: 9.5205e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 7.7647e-04 - val_mean_squared_error: 7.7647e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 8.4677e-04 - mean_squared_error: 8.4677e-04 - val_loss: 7.4963e-04 - val_mean_squared_error: 7.4963e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 7.4931e-04 - mean_squared_error: 7.4931e-04 - val_loss: 5.9438e-04 - val_mean_squared_error: 5.9438e-04\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 6.7311e-04 - mean_squared_error: 6.7311e-04 - val_loss: 4.7425e-04 - val_mean_squared_error: 4.7425e-04\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 112ms/step - loss: 6.5365e-04 - mean_squared_error: 6.5365e-04 - val_loss: 4.5641e-04 - val_mean_squared_error: 4.5641e-04\n",
      "mse train: 0.06521, mse validation 0.02669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.065214293491303491, 0.026686737793762665)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(10,10), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=10, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 20 epochs, eval during training with 20 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 6s 616ms/step - loss: 0.3187 - mean_squared_error: 0.3187 - val_loss: 0.1667 - val_mean_squared_error: 0.1667\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 0.0876 - mean_squared_error: 0.0876 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0228 - val_mean_squared_error: 0.0228\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 157ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 171ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 153ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 8.5395e-04 - val_mean_squared_error: 8.5395e-04\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 7.7500e-04 - mean_squared_error: 7.7500e-04 - val_loss: 4.8302e-04 - val_mean_squared_error: 4.8302e-04\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 6.5339e-04 - mean_squared_error: 6.5339e-04 - val_loss: 7.3556e-04 - val_mean_squared_error: 7.3556e-04\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 4.7104e-04 - mean_squared_error: 4.7104e-04 - val_loss: 5.0899e-04 - val_mean_squared_error: 5.0899e-04\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 4.3912e-04 - mean_squared_error: 4.3912e-04 - val_loss: 5.1421e-04 - val_mean_squared_error: 5.1421e-04\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 4.4952e-04 - mean_squared_error: 4.4952e-04 - val_loss: 4.1170e-04 - val_mean_squared_error: 4.1170e-04\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 4.4389e-04 - mean_squared_error: 4.4389e-04 - val_loss: 3.6920e-04 - val_mean_squared_error: 3.6920e-04\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 160ms/step - loss: 3.9739e-04 - mean_squared_error: 3.9739e-04 - val_loss: 3.9454e-04 - val_mean_squared_error: 3.9454e-04\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 3.6438e-04 - mean_squared_error: 3.6438e-04 - val_loss: 4.1874e-04 - val_mean_squared_error: 4.1874e-04\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 118ms/step - loss: 3.7776e-04 - mean_squared_error: 3.7776e-04 - val_loss: 4.4277e-04 - val_mean_squared_error: 4.4277e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 3.9011e-04 - mean_squared_error: 3.9011e-04 - val_loss: 4.5991e-04 - val_mean_squared_error: 4.5991e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 3.9392e-04 - mean_squared_error: 3.9392e-04 - val_loss: 4.8633e-04 - val_mean_squared_error: 4.8633e-04\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 4.0337e-04 - mean_squared_error: 4.0337e-04 - val_loss: 5.2011e-04 - val_mean_squared_error: 5.2011e-04\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 118ms/step - loss: 4.1083e-04 - mean_squared_error: 4.1083e-04 - val_loss: 5.4903e-04 - val_mean_squared_error: 5.4903e-04\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 4.1645e-04 - mean_squared_error: 4.1645e-04 - val_loss: 5.8611e-04 - val_mean_squared_error: 5.8611e-04\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 4.2205e-04 - mean_squared_error: 4.2205e-04 - val_loss: 6.2044e-04 - val_mean_squared_error: 6.2044e-04\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 4.2445e-04 - mean_squared_error: 4.2445e-04 - val_loss: 6.4510e-04 - val_mean_squared_error: 6.4510e-04\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 4.2634e-04 - mean_squared_error: 4.2634e-04 - val_loss: 6.5912e-04 - val_mean_squared_error: 6.5912e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 4.2768e-04 - mean_squared_error: 4.2768e-04 - val_loss: 6.5945e-04 - val_mean_squared_error: 6.5945e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 4.2838e-04 - mean_squared_error: 4.2838e-04 - val_loss: 6.4277e-04 - val_mean_squared_error: 6.4277e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 4.3042e-04 - mean_squared_error: 4.3042e-04 - val_loss: 6.1855e-04 - val_mean_squared_error: 6.1855e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 4.3192e-04 - mean_squared_error: 4.3192e-04 - val_loss: 5.8745e-04 - val_mean_squared_error: 5.8745e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 4.3334e-04 - mean_squared_error: 4.3334e-04 - val_loss: 5.6487e-04 - val_mean_squared_error: 5.6487e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 4.3174e-04 - mean_squared_error: 4.3174e-04 - val_loss: 5.4573e-04 - val_mean_squared_error: 5.4573e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 4.2783e-04 - mean_squared_error: 4.2783e-04 - val_loss: 5.4693e-04 - val_mean_squared_error: 5.4693e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 4.2134e-04 - mean_squared_error: 4.2134e-04 - val_loss: 5.4281e-04 - val_mean_squared_error: 5.4281e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 4.1351e-04 - mean_squared_error: 4.1351e-04 - val_loss: 5.6364e-04 - val_mean_squared_error: 5.6364e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 2s 169ms/step - loss: 4.0610e-04 - mean_squared_error: 4.0610e-04 - val_loss: 5.4848e-04 - val_mean_squared_error: 5.4848e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 3.9829e-04 - mean_squared_error: 3.9829e-04 - val_loss: 5.8068e-04 - val_mean_squared_error: 5.8068e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 3.9303e-04 - mean_squared_error: 3.9303e-04 - val_loss: 5.2665e-04 - val_mean_squared_error: 5.2665e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 3.8678e-04 - mean_squared_error: 3.8678e-04 - val_loss: 5.9208e-04 - val_mean_squared_error: 5.9208e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 3.8725e-04 - mean_squared_error: 3.8725e-04 - val_loss: 4.8470e-04 - val_mean_squared_error: 4.8470e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 3.8458e-04 - mean_squared_error: 3.8458e-04 - val_loss: 6.2418e-04 - val_mean_squared_error: 6.2418e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 3.9485e-04 - mean_squared_error: 3.9485e-04 - val_loss: 4.4557e-04 - val_mean_squared_error: 4.4557e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 3.9114e-04 - mean_squared_error: 3.9114e-04 - val_loss: 6.6658e-04 - val_mean_squared_error: 6.6658e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 4.0717e-04 - mean_squared_error: 4.0717e-04 - val_loss: 4.1475e-04 - val_mean_squared_error: 4.1475e-04\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.9160e-04 - mean_squared_error: 3.9160e-04 - val_loss: 6.6236e-04 - val_mean_squared_error: 6.6236e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 4.0314e-04 - mean_squared_error: 4.0314e-04 - val_loss: 3.8992e-04 - val_mean_squared_error: 3.8992e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 3.8044e-04 - mean_squared_error: 3.8044e-04 - val_loss: 5.9433e-04 - val_mean_squared_error: 5.9433e-04\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 3.8620e-04 - mean_squared_error: 3.8620e-04 - val_loss: 3.8484e-04 - val_mean_squared_error: 3.8484e-04\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.7075e-04 - mean_squared_error: 3.7075e-04 - val_loss: 5.0869e-04 - val_mean_squared_error: 5.0869e-04\n",
      "mse train: 0.07631, mse validation 0.03996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.076310423364451249, 0.039959337114572585)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(20,20), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=20, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 510ms/step - loss: 0.2543 - mean_squared_error: 0.2543 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 0.0601 - mean_squared_error: 0.0601 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0117 - val_mean_squared_error: 0.0117\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 9.4203e-04 - val_mean_squared_error: 9.4203e-04\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 9.7494e-04 - val_mean_squared_error: 9.7494e-04\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 9.9589e-04 - val_mean_squared_error: 9.9589e-04\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 7.8937e-04 - val_mean_squared_error: 7.8937e-04\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 9.7421e-04 - mean_squared_error: 9.7421e-04 - val_loss: 7.6363e-04 - val_mean_squared_error: 7.6363e-04\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 9.1752e-04 - mean_squared_error: 9.1752e-04 - val_loss: 6.7368e-04 - val_mean_squared_error: 6.7368e-04\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 8.6539e-04 - mean_squared_error: 8.6539e-04 - val_loss: 6.6338e-04 - val_mean_squared_error: 6.6338e-04\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 9.6177e-04 - mean_squared_error: 9.6177e-04 - val_loss: 7.0253e-04 - val_mean_squared_error: 7.0253e-04\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 8.1121e-04 - val_mean_squared_error: 8.1121e-04\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 8.2198e-04 - val_mean_squared_error: 8.2198e-04\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.2502e-04 - val_mean_squared_error: 8.2502e-04\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 6.6508e-04 - val_mean_squared_error: 6.6508e-04\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 7.6501e-04 - val_mean_squared_error: 7.6501e-04\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 7.4475e-04 - mean_squared_error: 7.4475e-04 - val_loss: 8.0915e-04 - val_mean_squared_error: 8.0915e-04\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 7.8624e-04 - mean_squared_error: 7.8624e-04 - val_loss: 6.1501e-04 - val_mean_squared_error: 6.1501e-04\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 6.5213e-04 - mean_squared_error: 6.5213e-04 - val_loss: 5.7252e-04 - val_mean_squared_error: 5.7252e-04\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 6.2322e-04 - mean_squared_error: 6.2322e-04 - val_loss: 5.3362e-04 - val_mean_squared_error: 5.3362e-04\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 6.2554e-04 - mean_squared_error: 6.2554e-04 - val_loss: 5.4672e-04 - val_mean_squared_error: 5.4672e-04\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 6.5832e-04 - mean_squared_error: 6.5832e-04 - val_loss: 5.5547e-04 - val_mean_squared_error: 5.5547e-04\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 7.0015e-04 - mean_squared_error: 7.0015e-04 - val_loss: 6.4986e-04 - val_mean_squared_error: 6.4986e-04\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 7.6763e-04 - mean_squared_error: 7.6763e-04 - val_loss: 6.2912e-04 - val_mean_squared_error: 6.2912e-04\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 8.4450e-04 - mean_squared_error: 8.4450e-04 - val_loss: 8.5492e-04 - val_mean_squared_error: 8.5492e-04\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 9.5290e-04 - mean_squared_error: 9.5290e-04 - val_loss: 7.2314e-04 - val_mean_squared_error: 7.2314e-04\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 7.3492e-04 - val_mean_squared_error: 7.3492e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 8.2414e-04 - val_mean_squared_error: 8.2414e-04\n",
      "mse train: 7.22660, mse validation 7.20777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.2265960036872281, 7.2077733345183974)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(5,5), split=180, batch_size=20, \n",
    "             epochs=50, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=5, split=180, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, eval during training with 10 epochs\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 0.2961 - mean_squared_error: 0.2961 - val_loss: 0.0789 - val_mean_squared_error: 0.0789\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.0863 - mean_squared_error: 0.0863 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 2s 244ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 2s 283ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "mse train: 0.92659, mse validation 0.73805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.92658545341745213, 0.73805311720421751)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,10), split=150, batch_size=20, \n",
    "             epochs=10, mode='nextstep', verbose=1)\n",
    "m.eval_lstm_stepwise(model, [configs,lcs], Y, steps=10, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.277989 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.2915 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.276354 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.292358 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.274371 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.290001 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.274371 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.287972 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.274371 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.287972 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.274371 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.284571 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.274371 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.281017 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.274371 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.272158 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.269471 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.269471 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.269471 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.269471 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.269471 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.269471 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.269471 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.269471 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.269471 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.269471 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.269471 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.269471 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.269471 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.269471 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.51741443  0.4540801   0.42677908  0.39405802  0.37759711  0.36866406\n",
      "  0.35782395  0.34698384  0.33524039  0.32951922  0.32249322  0.32148951\n",
      "  0.31506574  0.31356018  0.30683529  0.31135201  0.30442638  0.30312155\n",
      "  0.29308442  0.29298404  0.29338553  0.29388738  0.2857573   0.28977216\n",
      "  0.28073873  0.2899729   0.28214393  0.27792833  0.27863094  0.27381311\n",
      "  0.27270902  0.27903242  0.2728094   0.27009937  0.27050085  0.26809194\n",
      "  0.26538191  0.26558265  0.27120345  0.26708823]\n",
      "step nr. 10 prediction / true value for lc number 13 0.343269 / 0.322493222025\n",
      "step nr. 11 prediction / true value for lc number 13 0.345114 / 0.321489512185\n",
      "step nr. 12 prediction / true value for lc number 13 0.345114 / 0.315065743747\n",
      "step nr. 13 prediction / true value for lc number 13 0.339905 / 0.313560176043\n",
      "step nr. 14 prediction / true value for lc number 13 0.346333 / 0.306835287883\n",
      "step nr. 15 prediction / true value for lc number 13 0.346333 / 0.311352006447\n",
      "step nr. 16 prediction / true value for lc number 13 0.345114 / 0.304426380146\n",
      "step nr. 17 prediction / true value for lc number 13 0.346333 / 0.303121549848\n",
      "step nr. 18 prediction / true value for lc number 13 0.346333 / 0.293084416125\n",
      "step nr. 19 prediction / true value for lc number 13 0.346333 / 0.292984041167\n",
      "step nr. 20 prediction / true value for lc number 13 0.346333 / 0.293385528488\n",
      "step nr. 21 prediction / true value for lc number 13 0.346333 / 0.2938873812\n",
      "step nr. 22 prediction / true value for lc number 13 0.346333 / 0.285757301766\n",
      "step nr. 23 prediction / true value for lc number 13 0.346333 / 0.289772155108\n",
      "step nr. 24 prediction / true value for lc number 13 0.346333 / 0.280738732697\n",
      "step nr. 25 prediction / true value for lc number 13 0.346333 / 0.289972896929\n",
      "step nr. 26 prediction / true value for lc number 13 0.346333 / 0.282143932802\n",
      "step nr. 27 prediction / true value for lc number 13 0.346333 / 0.277928333959\n",
      "step nr. 28 prediction / true value for lc number 13 0.346333 / 0.278630935116\n",
      "step nr. 29 prediction / true value for lc number 13 0.346333 / 0.273813112282\n",
      "step nr. 30 prediction / true value for lc number 13 0.346333 / 0.272709022334\n",
      "step nr. 31 prediction / true value for lc number 13 0.346333 / 0.279032418757\n",
      "step nr. 32 prediction / true value for lc number 13 0.346333 / 0.272809395084\n",
      "step nr. 33 prediction / true value for lc number 13 0.346333 / 0.270099369096\n",
      "step nr. 34 prediction / true value for lc number 13 0.346333 / 0.270500852738\n",
      "step nr. 35 prediction / true value for lc number 13 0.346333 / 0.26809193985\n",
      "step nr. 36 prediction / true value for lc number 13 0.346333 / 0.26538191239\n",
      "step nr. 37 prediction / true value for lc number 13 0.346333 / 0.265582654211\n",
      "step nr. 38 prediction / true value for lc number 13 0.346333 / 0.271203452422\n",
      "step nr. 39 prediction / true value for lc number 13 0.346333 / 0.26708822633\n",
      "mse train: 0.08315, mse validation 0.05617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.083146945607023348, 0.056170584855359744)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, \n",
    "       'cols_bt': 0.9376450587145334, 'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "model = m.xgb_next(cfg)\n",
    "m.train_xgb_next(model, [configs,lcs], split = 200)\n",
    "m.eval_xgb_stepwise(model, [configs,lcs], Y, 10, split=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    evaluating models with cross validation (ridge, XGB, mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'alpha': 1.0}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.02977] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03703]\n",
      " [ 0.02671]\n",
      " [ 0.02556]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.02747] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02433]\n",
      " [ 0.02864]\n",
      " [ 0.02943]]\n",
      "mse over all validation data 0.0297968665436\n",
      "path plots/ridge regression_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this mse ridge 0.0297968665436\n",
      "this mse from list ridge 0.0297968665436\n",
      "path plots/comparison models_sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdd39dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdc9dcba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg={'alpha':1.0}\n",
    "res_ridge = m.eval_cv('ridge', configs, Y, cfg=cfg, splits = 3)\n",
    "t.scatter_plot(Y, res_ridge, 'ridge regression')\n",
    "t.box_plot(Y, [res_ridge], ['ridge'], 'comparison models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'n_estimators': 1000, 'gamma': 0.007833441242813044, 'maxdepth': 10, 'cols_bt': 0.9376450587145334}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00728] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00881]\n",
      " [ 0.00549]\n",
      " [ 0.00753]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00059] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00053]\n",
      " [ 0.00061]\n",
      " [ 0.00062]]\n",
      "mse over all validation data 0.00728375623487\n",
      "path plots/xgb_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this mse ridge 0.0297968665436\n",
      "this mse from list ridge 0.0297968665436\n",
      "this mse xgb 0.00728375623487\n",
      "this mse from list xgb 0.00728375623487\n",
      "path plots/comparison models_sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdc7bb0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdc7ce9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "#cfg = {'maxdepth': 4, 'lr': 0.07120217610550672, 'gamma': 0.03393596760993278, 'cols_bt': 0.823494199726015, 'n_estimators': 107, 'subsample': 0.7288741544938715}\n",
    "res_xgb = m.eval_cv('xgb', configs, Y, cfg=cfg, splits = 3)\n",
    "t.scatter_plot(Y, res_xgb, 'xgb')\n",
    "t.box_plot(Y, [res_ridge, res_xgb], ['ridge','xgb'], 'comparison models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 500 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00615] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00644]\n",
      " [ 0.00564]\n",
      " [ 0.00637]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.0005] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00017]\n",
      " [ 0.00077]\n",
      " [ 0.00055]]\n",
      "mse over all validation data 0.00615437846885\n"
     ]
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "res_mlp_500 = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=500, splits = 3, earlystop=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/mlp_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this mse ridge 0.0297968665436\n",
      "this mse from list ridge 0.0297968665436\n",
      "this mse xgb 0.00728375623487\n",
      "this mse from list xgb 0.00728375623487\n",
      "this mse mlp_500epo 0.00615437846885\n",
      "this mse from list mlp_500epo 0.00615437846885\n",
      "path plots/predicting final point directly from config_sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdcf65f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdcf658d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.scatter_plot(Y, res_mlp_500, 'mlp')\n",
    "t.box_plot(Y, [res_ridge, res_xgb, res_mlp_500], ['ridge','xgb','mlp_500epo'], \n",
    "           'predicting final point directly from config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04875, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04875 to 0.04613, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04671, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04613 to 0.04593, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04593 to 0.04239, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.04353, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04239 to 0.03963, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03963 to 0.03916, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03916 to 0.03686, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03686 to 0.03644, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03644 to 0.03381, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03381 to 0.03142, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03142 to 0.02962, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02962 to 0.02731, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02731 to 0.02713, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02713 to 0.02316, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02316 to 0.02113, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02113 to 0.01929, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01929 to 0.01806, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01816, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01806 to 0.01620, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01620 to 0.01465, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.01648, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01465 to 0.01372, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01372 to 0.01237, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01237 to 0.01145, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.01291, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01145 to 0.01099, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01099 to 0.01085, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01085 to 0.01084, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01084 to 0.01035, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01035 to 0.01001, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01001 to 0.00926, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00926 to 0.00920, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00920 to 0.00903, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00974, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00903 to 0.00875, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00875 to 0.00830, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00888, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00878, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00830 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00817 to 0.00784, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00784 to 0.00784, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00784 to 0.00768, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00955, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00768 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00762 to 0.00752, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00752 to 0.00749, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00887, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00749 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00747 to 0.00738, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00738 to 0.00736, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00736 to 0.00734, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00922, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00777, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00142: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00801, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00779, did not improve\n",
      "Epoch 00174: early stopping\n",
      "Using epoch 00099 with val_loss: 0.00734\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03013, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03013 to 0.02939, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02939 to 0.02794, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02794 to 0.02712, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02712 to 0.02585, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02667, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02585 to 0.02313, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02313 to 0.02203, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02203 to 0.02114, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02114 to 0.01806, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01806 to 0.01684, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01684 to 0.01593, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01593 to 0.01527, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01527 to 0.01433, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01433 to 0.01349, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01349 to 0.01253, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01253 to 0.01133, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01133 to 0.01071, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01190, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01071 to 0.00961, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00961 to 0.00952, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00952 to 0.00885, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00885 to 0.00858, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00858 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00837 to 0.00825, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00825 to 0.00765, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00765 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00717 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00873, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00715 to 0.00706, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00706 to 0.00704, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00801, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00704 to 0.00674, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00674 to 0.00666, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00982, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00666 to 0.00660, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00660 to 0.00648, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00892, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.01031, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00850, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00879, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00648 to 0.00648, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00648 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00736, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00114: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00624 to 0.00615, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00615 to 0.00611, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00611 to 0.00603, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00603 to 0.00585, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00585 to 0.00585, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00585 to 0.00577, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00577 to 0.00573, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00573 to 0.00567, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00567 to 0.00564, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00564 to 0.00563, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00563 to 0.00557, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00557 to 0.00543, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00543 to 0.00540, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00540 to 0.00538, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00538 to 0.00524, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00524 to 0.00511, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss is 0.00523, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00511 to 0.00510, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00533, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00267: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00510 to 0.00499, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00523, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00505, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.00499 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00376: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00504, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00498, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00503, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00511, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00430: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00523, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.00506, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00511, did not improve\n",
      "Epoch 00450: early stopping\n",
      "Using epoch 00375 with val_loss: 0.00490\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02666, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02666 to 0.02495, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02495 to 0.02461, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02461 to 0.02340, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02340 to 0.02236, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02236 to 0.02154, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02154 to 0.02006, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02006 to 0.01987, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01987 to 0.01797, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01797 to 0.01709, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.02040, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01709 to 0.01677, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01677 to 0.01617, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01617 to 0.01609, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01609 to 0.01306, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01326, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01306 to 0.01131, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01131 to 0.01061, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01061 to 0.00972, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00972 to 0.00965, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00976, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00965 to 0.00957, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00957 to 0.00863, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00863 to 0.00850, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00850 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01393, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00864, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00837 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00753 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00745 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00717 to 0.00704, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00986, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00704 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00966, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.01020, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00699 to 0.00683, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00683 to 0.00671, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00671 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00655 to 0.00649, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00864, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00649 to 0.00641, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00641 to 0.00636, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00968, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00636 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00592 to 0.00591, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00591 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00850, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00558 to 0.00520, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00542, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00520 to 0.00509, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00509 to 0.00483, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00489, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00499, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00505, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00483 to 0.00481, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00497, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00494, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00498, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00499, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00506, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00504, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00284: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00497, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00496, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00575, did not improve\n",
      "Epoch 00301: early stopping\n",
      "Using epoch 00226 with val_loss: 0.00481\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00568] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00734]\n",
      " [ 0.0049 ]\n",
      " [ 0.00481]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00178] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00275]\n",
      " [ 0.0011 ]\n",
      " [ 0.00149]]\n",
      "mse over all validation data 0.00568764191678\n",
      "path plots/mlp with earlystop_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdead45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "res_mlp_es = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True)\n",
    "t.scatter_plot(Y, res_mlp_es, 'mlp with earlystop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "evaluating with early stopping\n",
      "create mlp using Dropout\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05210, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05210 to 0.04729, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04729 to 0.04595, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04595 to 0.04564, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.04595, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04564 to 0.04303, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04303 to 0.04162, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04162 to 0.04005, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.04128, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04005 to 0.03847, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03847 to 0.03559, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.03796, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03559 to 0.03239, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03239 to 0.03183, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03183 to 0.03084, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.03152, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03084 to 0.02923, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02923 to 0.02588, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02588 to 0.02551, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02551 to 0.02517, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02517 to 0.02293, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.02314, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02293 to 0.02196, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02196 to 0.01942, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.02016, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02048, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01942 to 0.01746, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01746 to 0.01571, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.01838, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01571 to 0.01484, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01484 to 0.01462, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01462 to 0.01306, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01306 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.01324, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.01243, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.01518, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.01253, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.01592, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01224 to 0.01151, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01151 to 0.01075, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.01217, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.01075 to 0.00998, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00998 to 0.00956, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.01163, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00956 to 0.00872, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00962, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.01052, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00980, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01028, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00905, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.01013, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00872 to 0.00868, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00887, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00947, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.01487, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00882, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00868 to 0.00828, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00828 to 0.00820, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00892, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00960, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.01235, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00820 to 0.00814, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00887, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00814 to 0.00810, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00980, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00974, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00993, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.01336, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00810 to 0.00809, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00809 to 0.00807, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.01028, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00923, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00904, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00908, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00142: val_loss is 0.01008, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.01029, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.01020, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00954, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00875, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00807 to 0.00774, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00817, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00998, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00972, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00801, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00803, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00984, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00774 to 0.00772, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00879, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00978, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00852, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00993, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00894, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00940, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00929, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00874, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01002, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00964, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00910, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00984, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00976, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00850, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00814, did not improve\n",
      "Epoch 00247: early stopping\n",
      "Using epoch 00172 with val_loss: 0.00772\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03060, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03060 to 0.02954, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.03105, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02954 to 0.02854, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02854 to 0.02739, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02739 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02692 to 0.02546, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02546 to 0.02425, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02425 to 0.02319, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02319 to 0.02285, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02285 to 0.02072, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02072 to 0.02070, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02070 to 0.01925, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01925 to 0.01842, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01842 to 0.01755, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01755 to 0.01681, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01681 to 0.01644, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01644 to 0.01513, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01513 to 0.01503, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01503 to 0.01415, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01415 to 0.01366, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01366 to 0.01277, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01277 to 0.01275, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01330, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01275 to 0.01130, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01130 to 0.01013, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.01018, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01013 to 0.00999, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00999 to 0.00913, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00913 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00887 to 0.00865, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00865 to 0.00797, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00797 to 0.00791, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00803, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00976, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss is 0.00923, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00791 to 0.00775, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00775 to 0.00760, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00991, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00760 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00753 to 0.00716, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00716 to 0.00695, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00981, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00695 to 0.00682, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00682 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00870, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00833, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00662 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00662 to 0.00644, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00644 to 0.00643, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00643 to 0.00641, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00641 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00618 to 0.00596, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.01009, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00596 to 0.00562, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00864, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00909, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00663, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00196: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00562 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00551 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00548 to 0.00547, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00547 to 0.00531, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00803, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00531 to 0.00521, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00617, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00358: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00521 to 0.00519, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.00519 to 0.00514, storing weights.\n",
      "\n",
      "Epoch 00371: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.00514 to 0.00513, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.00513 to 0.00507, storing weights.\n",
      "\n",
      "Epoch 00376: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.00507 to 0.00505, storing weights.\n",
      "\n",
      "Epoch 00381: val_loss is 0.00521, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00384: val_loss improved from 0.00505 to 0.00496, storing weights.\n",
      "\n",
      "Epoch 00385: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00514, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00596, did not improve\n",
      "Epoch 00459: early stopping\n",
      "Using epoch 00384 with val_loss: 0.00496\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02605, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.02800, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02605 to 0.02551, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02551 to 0.02437, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02468, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02437 to 0.02290, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02290 to 0.02215, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.02314, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02215 to 0.02166, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02166 to 0.01976, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.02044, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01976 to 0.01814, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01814 to 0.01801, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01801 to 0.01641, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01641 to 0.01554, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01554 to 0.01522, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01522 to 0.01459, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01459 to 0.01412, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01412 to 0.01227, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.01245, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01227 to 0.01158, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01158 to 0.01144, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01144 to 0.01096, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01096 to 0.01054, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01054 to 0.01028, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01028 to 0.00971, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01198, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.01026, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00971 to 0.00908, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00908 to 0.00848, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00848 to 0.00846, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00896, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00980, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00846 to 0.00794, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00794 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00728 to 0.00703, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00917, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00703 to 0.00701, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00701 to 0.00667, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00667 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00624 to 0.00617, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00808, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00617 to 0.00607, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00931, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00944, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00607 to 0.00557, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00557 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00550 to 0.00547, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00547 to 0.00535, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00535 to 0.00510, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00920, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00974, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00917, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00719, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00201: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00899, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00715, did not improve\n",
      "Epoch 00210: early stopping\n",
      "Using epoch 00135 with val_loss: 0.00510\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00593] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00772]\n",
      " [ 0.00496]\n",
      " [ 0.0051 ]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00288] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00284]\n",
      " [ 0.00188]\n",
      " [ 0.00391]]\n",
      "mse over all validation data 0.00593283374683\n",
      "path plots/mlp with dropout_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAFQCAYAAAB05S9RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXd4FOX2xz8nYYEgSBSxEBuoV67YcsGKDWwoIIiFpl57++lV9KKgqKgIKNd+sWAvCAHEKCJiAeTaBcGCimKFoIJIECRASM7vj5mNm83O7myyNTmf59knuzPvvHNms/udd8973nNEVTEMwzCyg5x0G2AYhmH4x0TbMAwjizDRNgzDyCJMtA3DMLIIE23DMIwswkTbMAwjizDRrmeIyJMiMsJn2x9E5JgEnTdhfXn0v7OIrBOR3ChtVER2r2X/R4nIstpbaBipwUTbyApU9SdVba6qFQAiMkdEzk+3XclGRHZ1b0aN0m1LvIjIABH5UUT+FJFiEdk6Stv9RWS+iKx3/+4fsk9E5HYRWeU+bhcRCdmv7jnWuY9HQ/bli8hTIrLCfQwPO+9sEVkpIn+IyCci0ivBb0PCMdE2jBhko2CmGxHpADwMnAlsB6wHHvBo2xh4EXgW2Ap4CnjR3Q5wIdAb2A/YF+gJXBTWzX7uTb25qobezO8GmgG7AgcCZ4rIOSH7rwB2UNUt3fM8KyI71OqiU4Wq2iPFD+AHYDDwKfAn8BjOB3sGsBZ4A9gqpP1JwCKgFJgD/D1kXyHwsXtcETARGBGyvwew0D32XWDfMDuO8bDxSZwv2QxgHfAOsD1wD7Aa+AoojNQXMByY4tqz1rVvP4/z3Azc7z4PuO/HGPd1HrAB2BrnS6dAI+A2oMLdtw74r9tegYuBb9zrHQuIx3nz3GtcDXzh/j+WhV3Pte7/aKN73r+773+p+/84Kez9egh43b3mt4BdQvYfCnwErHH/Hur1f3Dfv2fd5z+517XOfRzi4/M1Bxjh/r/XAdOAVsB44A/3/Lu6bQVH2Fa4+z4D9nb3NQH+49rwq3t9eT4/4yOB50Je7wZsAlpEaHscUBL6v3LP2c19/i5wYci+84D3Q14rsLuHHb8BB4S8vg74n0fbA93P1IHp1oio7226DWiID/dL+j6OUBe4X5iPcQS4KTALuMlt+zccITsWR9SuAZYAjd3Hj8Agd9+pQDmuaLv9rQAOAnKBf7rnbhJiRzTR/g3oGGLT98BZbl8jgNlh1xQq2uWuPQHg3+6xgQjn6Qp85j4/FPgW+CBk3yfu813dL2cj9/Uc4PywvhR4GcgHdgZWBr/4Ec47Gvgfzg1hJ+Bzaor2QndfnnsdS9wvfWPXtrXAniHv11rgCByxuxd42923Nc7N4Uwc8e/vvm4V6f9AddGudt0+P19zXFt3A1ri3JS+Bo5xz/808ITb9nhgvvueCc6NaQd3393AS679LXDEf1TIeUqBwzxseBG4NmzbOqBjhLaDgBlh214GrnafrwEOCtnXCVgb9n9fDvwCTMW9Ibn7fiNEhIHrgdURzrXB7edVICfdGhHtYe6R9HG/qv6qqiU44vGBqi5Q1Q3ACziCC9AXmK6qr6tqOc7IJw9H4A7GEZN7VLVcVafgjKKCXAg8rKofqGqFqj6FM2o82KeNL6jq/BCbNqjq0+r4lYtCbIzEfFWd4tp8F47wRzrve8AeItIKR/AeAwpEpDlwJM6INR5Gq2qpqv4EzAb292h3OnCbqv6uqkuB+yK0uU9Vl6pqmWt7c7f/Tao6C+fL3j+k/XRVnauqG3HE4RAR2QnoDnyjqs+o6mZVnYDzS6VnnNcWD0+o6requgbn19K3qvqGqm4GJvPX/64cR5Db44x0v1TVn12f8YXAIPc9Woszeu4XPIGq5qvq2x7nb44jtqGscc8Vb9vw/WuA5iF+7SNxbm7tccT75RCX1qvAEBFp4U5Sn4vjLqlCVXu45zoReE1VKz2uKSMw0U4fv4Y8L4vwurn7vA3OaBoA9wO1FGeE3gYoUXe44PJjyPNdgKtFpDT4wBk5tkmwjZFYGmbzskjndQVxHs4X7wgckX4X6EztRPuXkOfro9jYJtRGqr9vQUL3twGWhn2hf8T5P9Ror6rrgN/d46r9Dz2OTTS+/nfuzee/OK6kFSIyTkS2BFrjiNv8kM/Oq+52P6wDtgzbtiXOr5F424bv3xJYF/zcuzfKTapaiuOjbovziwHgX+71foMz+p+A81mshjvomQEcJyIn+brCNGGinfksxxFfwJlJxxHeEuBnnFGphLTfOeT5UpzRZH7Io5k70ks2O4XYnAPsiHMtkXgLx91QiPNL4S2cn+0HAnM9jqlresqfQ22k+vsW6RzLgZ3cawk9piTkdeg1N8dxKywn7H8Y4dg/qT76297DhqSgqvepakdgLxx33GAct0IZ0CHks9NSVaPdqENZhDNxCICItMNxG33t0XbfsM/xvu72Gn25zxfhjeK4enB/JQxU1e1VtQOO5n0Y5dhGOG6ljMVEO/OZBHQXkaNFJABcjePieBfHtbAZ+JeIBESkD47QBXkEuFhEDnLDprYQke4iEuknaqLpKCJ93J+pV7o2v+/R9i0cX/kXqroJ118NfK+qKz2O+RVoVwf7JgFDRWQrEdkRuDxG+w9wRu7XuO/1UTjujYkhbU4UkcPcqIdbcSbLlgKvAH9zQ+AaiUhfHIF82T1uIdDP7bcTzlxAkJVAZei1hoQB7lqrKw9BRA5wPx/BSeANQKX7i+IR4G4R2dZtWyAix/vsejzQU0QOF5EtgFuAqa6bJZw5OBPL/xKRJiJymbt9lvv3aeAq9/xtcL4DT7o2dXDDBXPdG+WdODfDL939u4lIK3f/CTgunxHuvvYicoKI5Lnv/Rn89WsvYzHRznBUdTFwBnA/zuinJ9DT/Tm4CegDnI3zU7wvzkRM8Nh5wAU4P39X40xOnZ0i01907QlOwPVx/duReBfHTx8cVX+BIx5eo2xwJvpOFZHVIhLJHx2Lm3FcFN8DrwHPRGvsvtc9gRNw/g8PAGep6lchzZ4DbsL5X3TE+b+hqqtwoniuBlbhTCb3UNXf3ONuwBndrXbtei7kvOtxomXecd0UB+OM6H+k+ii/tmyJI86r3T5XAWPcfdfifGbeF5E/cKKa9gwe6MZEHx6pU1VdhBPJMx5nMrwFcGnIsTNE5Dq37SackL6zcCY3zwV6u9vBCR2chhPZ8jkw3d0GzmR+EU7ky3c4vu0eIZ+1ju5xa4FRwEDXNnBG48Nd+1biuFb6qurHMd+1NCLV3aGGUXfcBQy7q+oZ6bYlVYjIkzjRJ8NScK5hwEpVfThmY6PeYYsGDCPLUFVfaQqM+kmDEG3Xp/YATnD/HFUdn2aTDMMwakXWukdE5HEcP+EKVd07ZHs3HH9nLvCoqo4WkTOBUlWdJiJFqto3PVYbhmHUjWyeiHwS6Ba6QZwMcGNxJov2AvqLyF444WbBGNqKFNpoGIaRULLWPaKqcyOEPB0ILFHV7wBEZCLQCyeYfkec0CrPG5WIXIgTEsQWW2zRsX379ok33DCMrKV0fTm//LGB8opKArk5bL9lU/KbBWIfqAo//AC//858+E1V/S5SqkHWirYHBVRfxbYMJ+/GfcB/RaQ7TuhQRFR1HDAOoFOnTjpv3rwkmmoYRoOgvBwGDoSPP4aRI5Hrrou0+tY39U20I6KqfwLnxGxoGIYRg+IFJYyZuZjlpWW0yc9j8PF70rvQIyPBxo3Qty+8+CLceSdcdRVcd12dzl/fRLuE6kuTdyQxCxAMwzAoXlDC4MmfUF7pBHCUlJYxePInADWFe8MGOOUUeOUVuP9+uOyy8O5qRTZPREbiI5yMcW3dpcT9cFJLGoZh1JmhUz+tEuwg5ZXK8JfCUqGsXw8nnQQzZsDDDydMsCGLRVtEJuDk3thTRJaJyHlu2snLgJk4uQcmhSxZNQzDqDXFC0ooK4+ctbW0LCRDw7p10L07vPEGPP44XHhhQu3IWveIqvb32P4KToIewzCMhDFm5uLYjf74A048Ed57D559FgYMSLgdWSvahmEYqWR5aZnnvq2aBaC0FLp1g/nzYeJEOO20pNiRte4RwzCMVNImP89z34jD28DRRzthfVOmJE2wwUTbMAzDF4OP35NArtTYfuHfm9P9yoGwaBEUF0OvXkm1w9wjhmEYfglL1bTD+tVcdstVsPwnmDYNjj026SaYaBuGYfhgzMzF1cL9tlv7G89OvJ7A2lUw8xXo0iUldph7xDAMwwehE5Ft/lhB0XND2Xbd75x1+s0pE2ww0TYMw/BFcCJyx9JfmDR+CFuX/cGZfUewfO9OKbXDRDsMEekpIuPWrFmTblMMw8ggBh+/J+3X/sKk54awxaYyBvS7jcW77MXg4/eMfXACMdEOQ1WnqeqFLVu2TLcphmFkEL2b/sGkCUNpsnkTA/rfxpc77MEpHQu8k0UlCRNtwzCMWHz+ORsOO4KNmzbTr/8ovty2HRWqPD+/hOIFqc1JZ6JtGIYRjQUL4KijWLtZ6dt/FN+03qVqV1l5hb/l7QnEQv4MwzC8+OgjOO44aNGC0064gR+2alOjSbTl7cnARtqGYRiReO89OOYYyM+HuXMpb7tbxGbRlrcnAxNtwzCMcObOdUbY227rPN91VwYfvyd5gdxqzfICuSmPHjH3iGEYRihvvukUMNh5Z+d5G8clEowS8V1qLEmYaBuGEV/dw/rMzJnQuzfsvrtTxGC77dJtUQ1MtA2jgVO8oIShUz+jrLwCcOoeDp36GRCh7mF95uWXnZqOe+0Fr78O22xTbXemvE/m0zaMBs6YmYurhChIOkLZ0soLL0CfPrDvvo5LJEywIXPeJxNtw2jgeIWspTqULW0UFTlFCzp2dFwiW28dsVmmvE8m2obRwPEKWUt1KFtaeOYZp47joYfCa69BlPQV+c0CcW1PFibahtHAyZRQtpTz+OPwz3/CUUfBjBnQokXU5hvCXCOxticLm4g0jAZOpoSypZSHHoJLLnFisYuLIS/2r4qy8sq4ticLE23DMOhdmPpsdWnjvvvgiiuge3enCG/Tpum2KC7MPRKG5dM2jHrMmDGOYJ98MkydmnWCDSbaNbB82oZRTxkxAq65Bvr2dSJGGjeO6/CtPCYcvbYnCxNtwzDqN6pw441www1w5pnw7LMQiF9ob+rZgUCuVNsWyBVu6tkhUZb6wnzahmHUX1Rh6FC4/XY491wYNw5yc2MfF4FMmbA10TYMo36iClddBffcAxdfDGPHQk7dnAuZMGFr7hHDMOoflZVw2WWOYP/rX/DAA3UW7EyhflyFYRhGkMpKuOgiR6gHD3aEWyT2cVmCibZhGPWHigrHd/3oozBsmOPLrkeCDebTNgyjvrB5M5x1FkyYALfc4kSLJJhMyDtuI23DMLKfTZugXz9HsEePTppgD536GSWlZShOPu1BRQsZVvxZws8VDRNtwzCym40b4dRT4fnn4a674Nprk3KaSPm0FRj//k8ULyhJyjkjYaJtGEb2UlbmlAebNs0J6Rs0KGmn8sqbrZDSQggm2oZhZCfr1zsFeGfOhEcegUsvTerpouUXT2UhBBNtwzCyj3Xr4MQTYdYseOIJOP/8pJ+yS/vWnvtSWTDCokcMw8gu1qxxBPuDD5w8Iv37p+S0z89f5rkvlQUjTLQNw8geVq+G44+HBQucTH2nnJKyU0crdpDKsD8TbcMwsoNVq+DYY2HRIidS5KST0m1RWjDRNgwj81mxAo45Br7+2ikPdsIJ6bYobdhEZBhWucYwMoyff3aK7y5ZAi+/nDbBzgtElkuv7cnCRDsMq1xjGBnEsmVw5JHw009OxfRjjkmbKaP67FtDMHPc7anE3COGkUAyITdFveHHH6FrV1i50onF7tw5reZYEQTDqGcEc1MElzqXlJYxdKqTl8KEO06++w66dHHC+954Aw48MN0WAVYEwTDqFZFyU5SVV6R0iXO94Ouv4YgjnAU0s2ZljGBnCjbSNowE4bWUOZVLnLOeL76Ao4928mLPng37ptZfnA3YSNswEoTXUuZULnHOaj791IkSAZgzxwTbAxNtw0gQg4/fk7xA9UrfeYHclC5x9qJ4QQmdR8+i7ZDpdB49K6WpRH3x8ceOD7txY3jrLdhrr3RblLGYe8QwEkSmRBeEk/ETpB9+6CxN33JLx4e9227ptiijMdE2jASSCdEF4USbIE27re++C926wTbbOD7sXXZJrz1ZgIm2YdRzMnaC9K23oHt3aNPGGWHvuGN67fFBJsThm2gbRj2nTX4eJREEOq0TpG++CT17wq67Os932CFq80wQy0xxM9lEpGHUczJugvTVV6FHD9h9dydKxIdghxfUHTr1s5RPpmZKHL6JtmHUc3oXFjCqzz4U5OchQEF+HqP67JMef/a0adCrF/z9744Pe9ttYx6SKWKZKW4mc48YRgMgIyZIn38e+vWDwkInl8hWW/k6LFPE0svNlN8skFI7bKRtGEbymTAB+vZ1lqS//rpvwYbMWbQ0+Pg9CeRKje3rNmxOqavGRNswjOTy9NNwxhlOlr5XX4U40x5nik++d2EBWzSu6Zwor9SUumoalGiLSDsReUxEpqTbFsNoEDz2GJx9trPa8ZVXoEWLuLvIJJ/8mrLyiNtT6apJqk9bRPKBR4G9AQXOVdX3atHP40APYIWq7h22rxtwL5ALPKqqo736UdXvgPNMtA0jBTzwAPzf/zmLZ6ZOhbzauzMywidPZoRPJnukfS/wqqq2B/YDvgzdKSLbikiLsG27R+jnSaBb+EYRyQXGAicAewH9RWQvEdlHRF4Oe8SepjYMIzHcc48j2D17OjUd6yDYmUQmuGqSNtIWkZbAEcDZAKq6CdgU1uxI4GIROVFVN4rIBUAfHBGuQlXnisiuEU5zILDEHUEjIhOBXqo6CmdkXhu7ewI9d9890r3DMIyY3H47DBkCp5wCzz3nJIGqJ2RCfplkukfaAiuBJ0RkP2A+cIWq/hlsoKqTRaQtUCQik4FzgWPjOEcBsDTk9TLgIK/GItIKuA0oFJGhrrhXQ1WnAdM6dep0QRx2GIYBcOutcOONTmjfM89Ao/oXVZxuV00y3SONgH8AD6pqIfAnMCS8kareAWwAHgROUtV1yTJIVVep6sWqulskwTYMo5aowg03OIJ95pnw7LP1UrAzgWSK9jJgmap+4L6egiPi1RCRw3EmKl8AborzHCXATiGvd3S3GYaRKlTh2mthxAg47zx44gnIzY19nFErkibaqvoLsFREgh76o4EvQtuISCEwDugFnAO0EpERcZzmI2APEWkrIo2BfsBLdTbeMJJMxhcl8IsqDBoEY8bAJZfAuHEm2Ekm2dEjlwPjReRTYH9gZNj+ZsDpqvqtqlYCZwE/hnciIhOA94A9RWSZiJwHoKqbgcuAmTiRKZNUdVHSrsYwEkCmJECqM5WVToTIvffClVfC2LGQ06CWfqQFUdV025CRdOrUSefNm5duM4x6SOfRsyLG+hbk5/HOkK5psKgWVFTARRc5i2euuQZGjwapucTbqImIzFfVTrU93m6LhpFiMiUBUq3ZvBnOOccR7BtuMMFOMSbahpFiMiUBUq0oL3fyiDzzjBPed8stJtgpxkTbMFJMJqyqqxWbNjmZ+oqK4I47YNiwdFvUILFASsNIMZmwqi5uNm6EU0+Fl192lqhfcUWtusmEsmHZjom2YaSBdK+qi4uyMjj5ZKdwwQMPOKF9tSBTaixmO+YeMQzDmz//dOo5vvYaPPporQUbMqdsWLZjI23DMCKzdi107w7vvANPPeUsT68DWR81kyHYSNswjJqsWQPHHw/vvutk6qujYEOWR81kECbahmFUZ/VqOPZYmDcPJk1yIkYSQNZGzWQY5h4xDCyqoYrffnME+4svnOrpPXsmrOusjJrJQGKKtohcATwBrMUpHVYIDFHV15Jsm2GkBItqcPn1VzjmGFiyBF580SkTlmCyKmomQ/HjHjlXVf8AjgO2As4EPOswGka2YVENwPLlcNRR8O23Tix2EgTbSAx+3CPBNaonAs+o6iIRW7dq1B8yNaohZS6bpUuha1f45Rd49VWKW+zGmNGzzIWRofgR7fki8hpO+bChbiHeyuSaZRipIxMqbIeTMpfNDz84gr1qFcycSXHeLuYqynD8uEfOwykTdoCqrgca4xQsMIx6QSZGNaTEZfPtt3DkkU60yBtvwKGHcvO0RXGft94UdMgS/Ij266r6saqWglNnEbg7uWYZRuroXVjAqD77UJCfh+DktR7VZ5+0jiy9XDMlpWWJEcXFi+GII5wVj7NmwQEHULyghNXry+Oyp94UdMgiPN0jItIUp7LMNiKyFX/5trfEqYJuGPWGTItq8HLZAL7dFZ4+8UWL4OijnVJhs2fDPvsARB1Nh7qKQvvNEaEirJBKcGSeSe9nfSLaSPsiYD7Q3v0bfLwI/Df5phlGbOrrT/NILpsgftwkXiPgWUWvO1EiOTkwZ06VYEP0idcu7VtH7DdcsP30ZdQNT9FW1XtVtS3wb1Vtp6pt3cd+qlpvRVtEeorIuDVr1qTbFCMG9fmnedBl40UsUYzkE2+3dDEdzzmF9bkB+g4YTdunvqt2o4s28Tr7q5We/UbClqYnj5g+bVW9X0QOFZEBInJW8JEK49KBqk5T1QtbtmyZblOMGNT3+OrehQUU1DJfR7io7798MRMmXs/aQB69Tr2NDxq1qnGjizbxGuzPzwg63ZO49Z2Yoi0izwD/AQ4DDnAftS5KaRiJIlPjqxNJbSNbQkW907JFPFM0jNV5Leg/8Ha+ab5ttbahPuj8vEDU/rxuFrkiGTOJW9/xE6fdCdhLrWy7kWFkYnx1oqltvo7Bx+/J0Kmfsd+3C3hsyi380qIV554xmqV5W0VsH7zRDT+pQ7U4bah+kwj2G77fhDp1+BHtz4HtgZ+TbIthxIWXgGTCT/NErmasbWTLkT8t5O7JN7O05XZccvbtDBp4OGNmLo56o4t1k7CkT+nHj2hvA3whIh8CG4MbVfWkpFllGD7IVAFJdwKq4gUlzLjjMe6bPILvti7gjL4jWJ+XD/i70cW6SWRaeGRDw49oD0+2EYZRWzJRQKJNkKbC1vfufpz7J93K4ta7cGbfWynN2xLc878zpGuVjZl0ozP8E1O0VfWtVBhiGPWFtE6QTp7MiPE3s2i73Tjr9Fv4o2nzGufPxBud4Z9oKyLfVtXDRGQtEDoJKYCq6pZJt84wspC0TZC6ZcG+2Gkvzjj5RtY1aebr/FYAIruItrjmMPdvC1XdMuTRwgTbMLyJFaaXlFWcTz0FZ5wBhx/OT+Ofp6J5C8/zh1KfFyjVV3yVGxOR/YDD3ZdzVfXT5JlkGNlNtAnSpExSPvIIXHQRHH00025+kNFzf6KsvIJcNy9IQZTRc7r970b8+C03dgEw1d00XkTGqer9SbXMMBJEOn7+e/mNayOSUe0fOxYuuwxOOIGXbryfa6d/U9V/hSqCkzfEq++GsECpvuFnpH0ecJCq/gkgIrcD7wEm2kbGk+7wu3CipVxtO2R6DVGOav+cSXDVVdCrFxQVcfvd79S4ISgw/v2f6LTL1hGvtyEsUKpv+MmnLUDoJ6GCv9K0GkZGk2n5SaKJYSSfspf9Pw8d7gj2qafC5MnQpInnDUHxTruaiQUgjOj4Ee0ngA9EZLiI3Ay8DzyWXLMMIzHU5ed/MiYMo6VcDRJ6U6lhpyr/emcCl8x8FPr3hwkTIODkC4l2Q/C63kwsAGFEx0+c9l0iMgcnYZQC56jqgmQbZhiJoLY//5PlVgmfpPRK6BMU2Wr2q/Lv/z3DZe9N4pV/HMeJzzwDuX/dAAYfvyeDihZG7DPa9VrcdnbhZ6QdRML+GkbGU9uf/8l0q/QuLOCdIV25u+/+5Erkr1OOCG2HTGf9ps0EcgRUuW7241z23iQmFXZj07hHqwl2sN+BB+9c4wtq7o76hZ/UrDcCTwFb4eQheUJEhiXbMMNIBLX9+Z/sqIrgSN6r8kuFKgpuzUZl5JxHufCjF3j+4F40fmQcvTvuFPG4Eb334e6++1e73lM6FjBm5uJ6V92noSKxMq6KyGJgP1Xd4L7OAxaqar2+dXfq1EnnzZuXbjOMNNF59KyIbpWC/Lyq/B3J6D8c0UpGvPYAAxe+CoMGwZ13gsfoPBLhbh6wVKrpRkTmq2qtaxL4cY8sB5qGvG4C2K3aqNckO6rCz4g9p7KC22fcx8CFr/LAwafGLdiQedEzRt3xI9prgEUi8qSIPIGTX7tURO4TkfuSa55hpIdkR1VEqwADkFtZwZ3T7+b0z97gns79GX/SxXELNtjimfqIn8U1L7iPIHOSY4phZBaxoirqstLSK6/1KR0LePHDHxlVPIYei9/mjiPO4okj+jOqW/taXYMtnql/+An5eyoVhhhGNlHXkEDP/CQdWnPpfYNps/htbutyLq8cN5BRdVh2n8nVfYzaEXMisqFiE5FGNJIyUblhg7PCcfp0uO8+uPzyOlrpYKlXM4u6TkT6yvJnGEZ1YvmK4xbK9ev59egT2e79t7ju+P/jrT87MHhBSULE1RbPJI5MuAGaaBtGHAS/tF6/T9vk58XvOvnzT1YedRyt573H4BOuYPK+x0KaE1sZNcmU5GPRKtdMA8/PZlYW9hWRdsD1QEtVPTXd9hjZRaSY51CCvuK40q+uXQsnnsjW89/nqh5XUdyhS+xjjLSQKbnHo420/5OIE4hILjAPKFHVHrXs43GgB7BCVfcO29cNuBfIBR5V1dFe/ajqd8B5IjKlNnYYDZtIX9ogApzS0XFDDCpaGLFNDZdKaSmccAJ89BFX9BzMy38/PPYxRtrwWgzlZ5FUIvEU7QQW9L0C+BKoUaJMRLYFylR1bci23VV1SVjTJ4H/Ak+HHZ8LjAWOBZYBH4nISzgCPiqsj3NVdUXdLsVoyEQTUAVmf7US8Blm9/vvcNxx8OmnMHkyCxa3BAvNy2iClYAibU8lfnKP7CEiU0TkCxH5Lvjw07mI7Ah0Bx71aHIkUCwiTdz2FxChuIKqzgV+j3D8gcASVf1OVTcBE4FeqvqZqvYIe/gSbBHpKSLj1qxZ46e50YCIJaBBUY8NyKsbAAAgAElEQVS5mnLlSujaFT77DKZOhZNPtrzWWUC0PDGpxG8+7QeBzUAXnNHusz77vwe4BqiMtFNVJwMzgSIRGQicC5zms2+AAmBpyOtl7raIiEgrEXkIKBSRoR42TVPVC1u2bBmHGUY2UNf82LFyYQcz842ZuZhTOhZEXk35yy/QpQssXgzTpkEPx2Noea0znwKPm7bX9mThJ3okT1XfFBFR1R+B4SIyH7gx2kEiEvRBzxeRo7zaqeodIjIR58awm6qui8P+uFDVVcDFyerfyFwSMfMfbHfztEVu9r3qBEdcJaVlPD+/pKboLl/ujLCXLnVisbtWj+e20LzMJlMWKvkZaW8UkRzgGxG5TEROBpr7OK4zcJKI/IDjtugqIjVG6CJyOLA3zlL5m3xb7lAChOao3BFLZmVEIFGJk3oXFrDgxuO4JyT9aSSfZo2+ly6FI4+EkhJ49dUagm1kPpnya8jPSPsKoBnwL+BWoCvwz1gHqepQYCiAO9L+t6qeEdpGRAqBcTiRId/jVHofoap+83V/BOwhIm1xxLofMMDnsUYDItGJk0JHxW2HTI/e9/ffOyL9++/w2mtwyCHV2mXCgg3DH5nwayjmSFtVP1LVdaq6TFXPUdU+qvp+gs7fDDhdVb9V1UrgLODH8EYiMgGnAvyeIrJMRM5zbdsMXIbjF/8SmKSqixJkm1GP8JpEDC6GqYuv26vvlnkBTv/3M5TsdxB//LqKOQ9MiCjYQ6d+Rolbeiy8sK9hhOOnCMLfgMHALoSMzFW1Xv++s9wj9QuvYgCndCzg+fklNVwn+XkBhp/UwXNUFTo6zm8WYN2GzZRX/vVdCuQI7VYt5ennrqNRxWbO7DuC73fcg1F99gH+ShSV4xFGlqhiC0bmkYrcI5OBh4BHgMgrCwwjQ/ByNXhl1fNaMFNaVs7QqZ8x78ffefmTnyktcyYet2oWoPu+O1QT+tXrywnkCvl5AdaUldMmP48dli7hwfFDQKF//5F83XpXKK/g5mmL2FBeWXWsV7iYLaoxvPAz0p6vqh1TZE/GYCPtzCaSOANxl9ba1cMfXRuqRseffMKqQ45gc24jBvS7jW9bRa7n6Ksvo96RipH2NBG5FCe6Y2Nwo6pGWuxiGEnHK3yvaSDHd26I4gUl3DwtsdMfy0vLYN48OO44NjduQt/TR/DD1vFPWtmiGiMafkQ7GCkyOGSbAu0Sb45hVCfSiNorfM8rL0i4qyFW4qfacuwf38PRA2Drrfnk/uf49YM1EDbqb9Iop8rVEkquCJWqFj1ixMRP5Zq2qTDEMMLxGlHHK7bh0R3REj+FIninuQzf1/nnLxk7ZTjssD3MmsVxO+/MqILqN5wu7Vsz/dOfI/bV/6CdGNF7H59XZDRkoqVm7aqqs0SkT6T9qjo1eWYZhveCGBGINBWTF8gBJOaKNT+TfAWuyBZ9uLRaVAhAIFfoe8BOzP5qJctLy+ix6ivunnQjjXbZGd58k+IVMOa5WVVifXff/YGa/vYgCjw/v4ROu2xtI2wjJtFG2kcAs4CeEfYpYKJtJBUvcfWaO28ayOWmnh08F6rEKmAANScuO+2yNcNfWlTl0mgWyKFJIJfx7/9Em/w8nt15DZ3vHQbt2jmC/XOFb397KJY72/BLNNFe7f59TFXfToUxhhGKV4pTL0rXl3uuWPPjx24WyGFkjGXJ68srWV/u5D/72/y5dHrhNn7eaTd2mDMHWrdmzJOz4vK3h2JhfoYfoq2IPMf9e18qDDGMcLzSlebnBSK2j5Y61Y8fu3Gj3GqCXbyghMGTP4k4cXj81+/y8NTbWNx6V044aTjFyzYBdRNey51t+CGaaH8pIt/gLB3/NOTxmYh8mioDjYaLV4Ke4Sd1iDv3tB8xLS0rr7Z8fMzMxTX82QDdv/wfY4tH8/n2u3FG3xGU5rWoSg7lJbz5eYGoaV0tzM/wS7TKNf1FZHucvB5ZVw/SqB9ES9ATT5Kl/GaBiOlUI/UZ7CeS0PdaNJu7pt/N/IL2nHvqcNY1aVatrVf6zuEndahmc8u8ACKOS8fC/Ix4iBryp6q/APulyBbD8E282db8FhcJFepwn/ppn77O7TPu4/2d9+H8U25gfeO8am2DdoH3DcWE2agrfhbXGEbWsyaCXzoSweozbcJC/gYsnMHImWOZu2shF/a5ng2BplXHhLs2MiF9p1F/8VMEwTCynpYek5fhVKhWpUh9fn4JfQ/ciYs/fYWRM8fy5m4HMPiMWzj18L+lPRG+0XCxkbbRIPAqmC3u0sZIKVLLyivY7pGxXD7jYejdm6OLivigcWNf57PCBkayiLYichreq3hRVZucNFJCIgSw1GsSUuH70d0jVp+59L1JXD73ad7c50gu3v1str3rbV/nTkQ9SsPwIpp75D/AnThlwMpw8mk/AqwDvk2+aYaRuMou0SrX1NivypVvj+eauU9TvNdRXNjtKspzG/k+d6LqURpGJKKF/L0FICJ3huV+nSYilmjaSDjxZPSLtuQ7Uj9eoXhd2rem8+hZlJSWOUmgVLlm7lNc+v4UJu99DNeecDmVOX/FV/tZbp7oepSGEYofn/YWItJOVb8DcIvobpFcs4yGRrwZ/bwEMFI/g4oWojhVZ5o0yqmqLtOlfetqFWhUlWGzH+P8j4p5bv8TuP64S1Cp+WM0lvh6Lb+3FY9GIvATPTIImCMic0TkLWA2cGVyzTIaGl4j6lyPGUQvAYzUT3BiZvX6cjZuruTuvvvzzpCuzP5qZVVb0UqGv/Ew539UzJMde3LdcZdGFOxo5w7itfzeVjwaicBPPu1XRWQPoL276StV3RjtGMOIF6/Ra4UqeYHcmOlWY/UTJNS3HBwNi1Zy28yxDPhkJuMOOJmRXc71DDfxI76xFtgYRl2IKdoi0gy4CthFVS8QkT1EZE9VfTn55hkNBS+XQkGIb9tLAEN92F7VzUMJjebIqazgjhn3cernb/LfQ07nP4ef6SnYBXGIry2wMZKFH5/2E8B84BD3dQlOhXYTbSNhRJooDOQI6zdtZlDRwqpiApFqPYYeF0uwwSntVVZeQW5lBXdOv4veX7zFXYcN5L5D+yEiEeNcrdCukSn48Wnvpqp3AOUAqroep0KSYSSM8Ix++XkBEMcPHS3Uz2/psCB5gVwqVGlUsZl7XxpD7y/e4o4jzuK+zv1BhIEH70wgp/rHO5Ajni6R4gUldB49i7ZDptN59Ky4QxENI178iPYmEcnDnc8Rkd0IqcpuGImid2EB7wzpyveju7NFk0aUV9RcoRge6xxvGF2TRjls21h54MXR9Fj8Nrd2OY8HDjm9av/0T3+uOVr3GKIkKobcMOLBj2gPB14FdhKR8cCbwLXJNMow/MY6xxtGV7b2T+547maO++Z9bjzmIh478ORq+1evLyc8hXZ5hUZcGGOLaIx0EFO0VfU1oA9wNjAB6KSqs5Nsl9HAibWCMUik8DovmpZv4NEpt3DEt/O5tccVvHn06bEPcol0E7FFNEY6iCnaIvKmqq5S1emq+rKq/iYib6bCOKPh4jfWOVJ1mzMO3rnqdZBmm8p4YsrNdP7xE6458Qoe73As7wzp6ntyJtJNxO+NxTASSbSEUU2BZsA2IrIVf3n2tgSyMpZJRNoB1wMtVfXUdNtjeBMr1jlWEqlhxZ8x4YOlVKjSfON6npg8nH8s/4ore17NS3sdRYErrC3zAhFrQIbiFZvttTTeFtEYySRayN9FOCsf2+CE/AVF+w/gv7E6dkV/LtDEPc8UVb2pNkaKyONAD2CFqu4dtq8bcC+QCzyqqqO9+nGX4p8nIlNqY0eqaejpPf1WVi8pLePKooVcWbSQ/LwAHdq04J1vfwdgyw3reGrSTez96xIuP+kaXml/WDVh9UrZGiRXxDNfti2iMdJBtIRR9wL3isjlqnp/LfreCHRV1XUiEgDeFpEZqvp+sIGIbAuUqerakG27q+qSsL6exLlRPB26UURygbHAscAy4CMReQlHwEeF9XGuqq6oxXWkBUvv6eA3iVSQ0rLyKsHOL/uDZ4puYM+VP3Jp76G8vsfBNRbIeKZsdalUjfp+2yIaI9X4iR6pFJH84AsR2UpELo11kDqsc18G3Ef4uoUjgWIRaeL2fQFQ4wahqnOB3yOc5kBgiap+p6qbgIlAL1X9TFV7hD2yRrDBIhPAO6Qu0srJcFr9WcqECdfxt99+4sI+1/P6HgcDziThmJmLq8LyYvmfzT9tZBp+VkReoKpjgy9UdbUrrg/EOtAdCc8HdgfGquoHoftVdbKbNbBIRCYD5+KMmv1SACwNeb0MOCiKPa2A24BCERmqquGjcUSkJ9Bz9913j8OMxNNQIxNiLUn3s5Cm9brVjJ94PTuv+YXzTrmRt9sWVu0LFX+I7JcOEsk/3dBdVkb68SPauSIiqhpcXJML+Kq5pKoVwP7uSP0FEdlbVT8Pa3OHiEwEHsRZfbkuUl+JQFVXARfHaDMNmNapU6cLkmWHH7I1vadfUYvUDoh7SXo42639jecmXs8Oa3/jnFOH894u+0ZsF/zVElyaPmbmYkpKy8h1bxSR8oyYy8rIBPyI9qs4I+GH3dcXudt8o6qlIjIb6AZUE20RORzYG3gBuAm4LI6uS4CdQl7v6G7LerIxMsGvqHm1axrIiWtJejht/ljBcxOup9X6Us46/Rbm7dghavvgrxa/funaFGQwjETjx6d9LU4O7Uvcx5vANbEOEpHWQV+4uwz+WOCrsDaFwDigF3AO0EpERsRh/0fAHiLSVkQaA/2Al+I4PmOJFH+c6VW//frhh7+0KGK71TEmBaOxU+kvTBo/hK3L/uDMviNiCjbE/6ulobqsjMzCTz7tShzXxYNx9r0D8JTrTskBJkVI59oMOF1VvwUQkbNwVl5WQ0QmAEfhxIwvA25S1cdUdbOIXAbMxIkYeVxVF8VpZ8aSbZEJXhOEoduLF5TEjIuOl11/L+G5ideTV76RAf1u4/PtY89HBHK9k0B5ka0uK6N+EW1xzSRVPV1EPiNCVXZVjews/Gv/p0BhjDbvhL0uxykeHN6uf5Q+XgFeiXYeIzXkeuSyDq0+k+jol91+W8pzRdfTqGIzA/rfxlfbtvN13BaNG8V9Q8xGl5VR/4g20r7C/dsjFYYY2Y/XxGHo9kS6Ev628gfGTxwGAv36j+Kb1rv4PnZNLUb7tpjGyASiLa752f37Y+rMMbKZgijVZ4J4uRjipcOv3/JM0Q1sym3EgH4j+a7VjnEdX1uXRra5rIz6h+dEpIisFZE/vB6pNNLIDvwkefJqs0Vjf5n6APb9+Wuem3g9ZY2a0HfA6LgFOxUuDSuOYCSLaCPtFgAicivwM/AMTv6RgTiTjIZRDT/ug0hturRvTdGHS2v0l5sjaKVSGbLtHyVf8uSkmyjNa8GA/iNZ1nI7X7ZFi79ONBbPbSQT0RgLGETkE1XdL9a2+kanTp103rx56TYjY0jmSsDOo2dFdJnk5wUYflIHrixaCMCBSz/n8Sk3s3KLfAb0G8nPW7aO2Xc6ajt6XY/VmTQARGS+qnaq7fF+4rT/FJGBIpIrIjkiMhD4s7YnNLKPZJfV8pqcXFNWTu/CAgry8zj0h4U8Ofkmfmneir79R0cU7PCEfcHCwKl2UVg8t5FM/Ij2AOB04Ff3cZq7zWggJDt5ldekYMu8gHP+Fj/z+PO38FPL7ek3YBQrWrSq0VaAu/vuH3dh4GRgxRGMZOKn3NgPqtpLVbdR1daq2ltVf0iBbUaGkOyR4+Dj96xRAR1g7cbNXD7gFjpecTbft9qR/v1H8tsWW0Xso01+Xq0KAycDv1V3DKM2+Ck39jcReVNEPndf7ysiw5JvmpFOQqMfcjwqBSRq5Ni7sIDmTWvOiR/z1TvcWXQrX7VuS7++t7G6WcuIx0da3eh1Q0lEuGEssjEFgZE9+EkY9QgwGHgYnJWOIvIcEE+OECOLCI9+iLRoJjhyjGeCMrRty7wA5RWV/LkpcoKoHl/O5Z5p/+HTHfbgn6ffwtomW3ja2yhHapzTKx5cXDuSLaAWz20kCz+i3UxVP5Tqo63NSbLHyAC8KsPkilCp6plKNTS0LdhPeFhfeaVzA4iWf+Tkz2fxn1fuYV7B3zn31Jv4s0mzqPaWlVfWEOLBx+/JoKKFNfIvqGuXCaqRrfgR7d9EZDfc/CMicipO3LZRT/FyLVSq8v3o7lWvO4+eFXGCcvhLi9i4ubKamD/7/k++zn3ap69x+4z7eW+XfTi/z42UNW7q67hwIe5dWFAVKhiORXEY2Ywf0f4/nPSp7UWkBPgeZ4GNUU/xm83OS/xqm8Vv4IJXuO21B3ir7T+48OTr2Rho4vvYSLZ4Lau3KA4jm4k6ESkiOUAnVT0GaA20V9XDLB9J/SZW9ENwkjL+ujLenD3vJW577QHe2O0ALuwzLC7BhshCbFEcRn0k6khbVStF5BqcXNi2oCZNpLouYbTl6OGTlIngwg+e57o5T/Dq3w7h8pOuoTw3ENfxXkJsWfmM+oifZeyjgd+AIkJWQqpqpOro9YZMWcYeSSTzArlpCyHzWqJdWy57dyL//t+zTGt/OIN6XM3mXO9xRJNGOWzTvEnMWo6GkcnUdRm7H592X/fv/4VsU8BftnmjTmRaXcKETeKpMujt8Vzx7kSe79CFa068koqc6Jn+Nm6urBpRB0fPhtHQ8FNurG0qDDEik2l5LOLNhy1ATnhFG1WufespLvlgCkX7HMvQbpdRGUOwg0SKTAnNoJdqV5JhpJqYoi0iTYFLgcNwRtj/Ax5S1Q1Jts0gOXUJ6yJskUpuRaOG/arcMOtRzpv3Is/ufwI3HHcJKn5S4DhEikwpK6/g6kmfMO/H33l+fomlRDXqNX6+LU8DHYD7gf+6z59JplHGXyQ6AqKuGfvCl2hv0Ti3Rna9IIFcoUv71lU1IkUrueX1hzhv3os80bEnw467NC7BjkaFKuPf/ympia0MIxPw49PeW1X3Cnk9W0S+SJZBRnUSHQFRWx95rNF58YISBk/+pGrFI0BFhVL00VIqVBGtZOSr/6X/p6/x0IF9GH3UOeCKea4IB7fbive/W+1ZZxKcm1XTQA6r10eOA/c6MtSVZO4TI9vxI9ofi8jBqvo+gIgcBKQ/rKKBkGiR8esjDz1vfrMA6zZsrhLkSG6HMTMXVxNsgEqgskLJqaxgzIx7OeXzWdx3SF/uOvyMKsEGZ5T88U9r6H/QTtWWuoeyVbMAN/XsABB3yGHQlWQVZYz6gJ/fph2Bd0XkBxH5AXgPOEBEPhORT5NqXQMnGcUH/OR6Dj/v6vXlNYQ03O3gdTPIrazg7pfv4pTPZ3HnYQO564gzqwl2aH+zv1oZMdsfQLPGjaqSMI3qs0+VyyWc8K2hrqRk5wU3jFTgR7S7AW2BI91HW3dbD6Bn8kwzkiEyfnzkXgmjwikpLau6gUS6GQQqyrn/xdvp9eVbjD7ybO7v3D9qf8tLyyj1cH2E3hR6FxZw5+n7RbyOgQfv7JkSNdMicQyjNvgJ+bMl62kimsj4dZtEajeqzz5Rj41HxAZP+cT5GxZV0nhzOWNfHMWxSz7k1q7n89gBvWP2ld8sQLPGjXxFy9TG15+MSBzDSDV+fNpGmvASmfxmAV++2Ug+3EFFCxl48M5RC8zGE4tdXqGMmbm4qr/hLy2i7I91PPzCSI76fj7Djr2EZ//RPUYvDus2bKb7vjtUC9uD6MvU4/FFRwpXtFwkRraRmHgrIyl4uTJU8eU2ieTmUGD8+z9F9YtHOm80giPz3oUFNN20gceev4Ujvv+Ya7tdHlGw8wK5NAvU/OiVVyqzv1qZtKovVlHGqA/YSDuD8XIBDPKZJ9rLzRGrEECk8/65cbNnytWge2HaO19zzzPXc8CyL/h39yuZuvfRNdoWxHkNicYqyhjZTsyEUQ2VTEkYFQmvpE1BQQyKbY3l4yEIVCtoEItIcdjgLKAZc+p+9G7XnIX7HMrey77iqh5X89JeR1Zr1yyQwxe3nhDzGvLzAtWWqUNyE2RZ3LaRalKRMMrIMLx8s13at45Z2zFIpMm3cAHr0r41s79aWfW674E78fInP1eNuIOx0713bcbqzkfSoWQxl/W6llf37Fyj75F99vV1DSLerp9Ei6nFbRvZiIl2FuLlNvEbqhdp8i2SgIWWCCspLaPow6WMOW2/6oK2ahWlhx7BFt98xSW9r+ONPQ6KanOsa0il2yTTMigahh9MtLOUSL5ZL8EDx3USzQXgR/DLK5XhLy3669gVK+CYY8hbspgLTx7GnN0i/+LLz4tc1CDSNYyZudgzLC9dq0MNI5Ow6JEsonhBCfvf/Bq7DpnOrkOmU3jLa9WiQGLFGw88eGfAEffOo2dVO9ZviF/VZOTPP8NRR8GSJZx7yk2egh3IEYaf1MFX3+AdMRN0/aR6dahhZBo20k4DoSPGlnkBRKB0fXmNsl7h/uXwvByr15dz9WRncUvvwoKoaVMjuTuC/ltwJiZ9T0kvWwZdu8Ly5TBjBu/MWOfZtO+BO9UYDUcbMcfj+qmrK8Pito1sxKJHPEhW9EisGot5gVxO6VhQY4FJNFHdonEui27pBsCw4s8Y//5PvgW4wB1V+h1pF6xZwbQXbmTr9Wtgxgzo3Jn9b37NMxwwPPKjtuXT2g6ZHvGa4o2CCceiR4xUY9EjWUYs33FZeQUTPlhaI/Ijmgj/uamC4gUl9C4sYPZXK+Oqkh5PFZqdSn9hwoSh5G5czw1X3cutnZ0okeEndYgYDgh/FSgAZxRd2xFzspagW9y2kW2YTzvF+Jnkihaq58WYmYspXlASd9FdrwIG4bT9vYRJ469li00bGNDvNp6p3L7Kn9y7sIAxp+3neWyFKoOKFrLrkOme9pWUltF2yPQavvYgiS4GYRjZiol2ivEzMvRKOxqNcB+1X/zcHnb/7SeKnhtCoHIz/fuPZNH2uwPUWDYfzWo/54k2wWhL0A3DwdwjKSZWjUUvn3ZeIJd/7NySd779PeJxuSJxFQbwS/sV3/Ns0TAqJYd+/UexZJudq/aF/moYM3NxXG6ZaHi5S8yVYRgm2iknPDrCK3qk0y5bR1ydGIm8QG5SBLvDL0t4tugGNjRqzID+I/l+6+qCGfqroTaxzdEmV7MpVtomM41UYqKdBvyMGEPbRIq4CApeaL6ReP3ZWzULsG7jZsorakrnfssX8/SkG1nbpBkD+o3kp612qLY/3J8cTzpXXLvfGdLVMwdJtsRK21J4I9WYTzsDKF5QQufRszwn4rxSrAaFD2D9ps1xnTMvkEv3fXeIONT9x7IvebZoGGuaNqfvgNurBDvos47kT44nnWuo4Gf7BKOVMDNSjY2004xXoYIrixaS77pOvKqPByvYRPKRNwvkUF6h1cLwIo3Ow8P0DvrpMx6fcjO/Nt+aAf1G8suW21TtU/4S1Gi5RKKNuAt8LqbJllGqLYU3Uo0trvEg0YtriheUcPO0RVUCnJ8XYPhJHWrl1ggSbWFMeJrWSGIYvmCl8w8LefT5W1nWclsG9LuNlc239jxvtMo3yVoIk4lES5Mb7T0yGi62uCYLKF5QwuApn1TzHZeWlXsuSPFDIFdiZsWL5TsP9UMf+d18xk0dwXdbF3BG3xGs2iLf87hYo8iGVIvRlsIbqaZB+bRFpJ2IPCYiU1J53jEzF0ec7KutYANVvmgvIcwRibpYBRzBEeDoJR8wbuqtfLPNzvTvPzKqYEc7Z2i/2eynjgeLHzdSTdJG2iKyE/A0sB2OxIxT1Xtr2dfjQA9gharuHbavG3AvkAs8qqqjvfpR1e+A81It2snwb5ZXOgV1veK+g6sqo0Uz9C4sYP3EIk57YSSLtmvHWaffyh9Nm0c9rx/xzXY/dbxY/LiRSpLm0xaRHYAdVPVjEWkBzAd6q+oXIW22BcpUdW3Itt1VdUlYX0cA64CnQ0VbRHKBr4FjgWXAR0B/HAEfFWbSuaq6wj1uiqqeGs3+RPq0vfye4CyKqc2ydfjLRxwaJxytxFj4JCATJsCZZ7Jq70K6drmGNU2a1TgmPy/AFk0aNQjxNYxUUFefdtLcI6r6s6p+7D5fC3wJhH/bjwSKRaQJgIhcANwfoa+5QKSlgAcCS1T1O1XdBEwEeqnqZ6raI+yxwo/dItJTRMatWbPG76XGZPDxexLIrbnIO5AjHNxuq1r3G3RT9C4s4J0hXfl+dHcqo9wAqi0Rf+opOOMM6NyZVv+bxc1nHVrDpSH8lT/77r77886QribYhpFmUuLTFpFdgULgg9DtqjoZmAkUichA4FzgtDi6LgCWhrxeRs0bQ6gdrUTkIaBQRIZGaqOq01T1wpYtW8ZhRnR6FxYw5tT92KrZXxVc8vMCjDltP774eW2UIyE3R2gWqPlv8nJTxPI3l5VX8NWIu+Ccc6BLF3jlFWjRoppvFqqvVkxEwQHDMBJD0qNHRKQ58Dxwpar+Eb5fVe8QkYnAg8BuquqdUb+OqOoq4OJk9R+NSH7P4gUlnjHYQSoqlSZNG9Gn447Viux6uSli5TY54+PpDHn9QejWDaZOhby/RD5oYyR3jtVONIzMIKmiLSIBHMEer6pTPdocDuwNvADcBFwWxylKgJ1CXu/obssK/K6aW72+nOfnl/iKSgjuvzJCKOC5H73IjbMe4e2/H8JhxcXQpEnEPmzBiGFkLklzj4iIAI8BX6rqXR5tCoFxQC/gHKCViIyI4zQfAXuISFsRaQz0A16qm+WpIx4RDBYT8OOi6F1YUOXmCHLx+1O4cdYjzPjboax6crynYIPVTjSMTCaZPu3OwJlAVxFZ6D5ODGvTDDhdVb9V1UrgLODH8I5EZALwHrCniCwTkfMAVHUzzsh8Js5E5yRVXZS8S0os8Ypghapv33JorPTl70xgyFtP8tLfj+C9kf+l14FtfR8bpL7GWRtGtmHL2D1IZMifV+rOWPUivfC7RLr442WU/nsoZ89+lhmFx7Jx3M4EJE8AAAzxSURBVKP07rRzzOOi2WwYRt2wZewZjp/UnUFx9Hv79OVWUaX3xPtg9rNw3nmc8PDDkOsvC1/QNhNpw8g8GtQy9nQQK3VnaIx1uB/ai5huFVUYNAjGjIFLLoFx4+ISbMMwMhcT7SQTTySGn5zUMX3LlZVw6aVw771w5ZUwdizk2L/ZMOoL9m1OMvFEYgQXuHgV9s0ViR72V1EBF1wADz0E114Ld90FtSgSbBhG5mKinWS6tG8d1/behQWeS9ErVb0Fe/NmZ5Xj44/DDTfAqFEm2IZRD7GJyCTjVYzXazv4y0cdGt2xU4sAz80dy46vvQS33grDhtXdcMMwMhIbaSeZ2qwujBUnHYxIKSkto1FFOdc9PZwdX3uJz6+43gTbMOo5JtpJpjarC2Ml1g9GpDTZvIkHXxhJt6/f4+ajL+Ci7a28lWHUd8w9kmRqW44qWpz08tIympRvZNwLt3Hk9x8z7LhLebbwRMRygxhGvcdEO8kko4pLu2Zwy+M3c8iPn3FNt38xab/jAMsNYhgNARPtFJDQ1YVr11JUfCtb/fQ5V3cfxAt7Oy4Ryw1iGA0DE+1sYs0aOOEEtvl0Hh+N/C8fsidiuUEMo0Fhop0trF4Nxx0Hn3wCkyZxQJ8+vJNumwzDSDkm2tnAb7/BscfCF1/A889Dz57ptsgwjDRhop3p/PorHHMMLFkCL77olAkzDKPBYqKdySxfDkcfDT/+CC+/7Dw3DKNBY6KdqSxdCl27wi+/wKuvwhFHpNsiwzAyABPtTOSHHxzBXrUKZs6EQw9Nt0WGYWQIJtqZxrffOoL9xx/wxhtwwAHptsgwjAzCRDuTWLzYEeyNG2HWLCgsTLdFhmFkGCbamcKiRc5EoyrMng377JNuiwzDyEAsy18m8MkncNRRTlmwOXNMsA3D8MREO918/LHjEmnaFN56C/7+93RbZBhGBmOinU4++MAR7BYtHMHeY490W2QYRoZjop0u3n7bWZreqpUj2O3apdsiwzCyABPtdDBnjrMcfYcdYO5c2GWXdFtkGEaWYKKdal5/HU480RHqt96CAkunahiGf0y0U8krrzgZ+vbYwxltb799ui0yDCPLMNFOFS++CL17Q4cOzsKZ1q3TbZFhGFmIiXYqmDwZTj3VWeH45pvO5KNhGEYtMNFONs89B/36wUEHOf7s/Px0W2QYRhZjop1MnnoKzjgDDj/cSa+65ZbptsgwjCzHRDtZPPIInHOOk0/klVegefN0W2QYRj3ARDsZjB0LF17oxGJPmwbNmqXbIsMw6gkm2onmrrvgssugVy944QUnp4hhGEaCMNFOJKNHw9VXO5EikydDkybptsgwjHqGiXYiUIVbboGhQ2HAAJgwAQKBdFtlGEY9xIog1BVVGDYMRo6Ef/4THnsMcnPTbZVhGPUUE+26oAqDB8Odd8IFF8BDDzmFDAzDMJKEKUxtUYUrrnAE+//+zwTbMIyUYCpTGyor4ZJL4P77YdAg568JtmEYKcCUJl4qKuD88+Hhh2HIEGekLZJuqwzDaCCYaMfD5s3OZOMTT8BNNzmTjybYhmGkEJuI9Et5OQwc6MRf33YbXHddui0yDKMBYqLth02boG9fKC6G//zHWUBjGIaRBky0Y7Fhg7PCcfp0uO8+uPzydFtkGEYDxkQ7GuvXw8knw2uvOSF9F12UbosMw2jgmGh7UVkJPXo4tRwff9xJs2oYhpFmTLS9+OYbZ6T99NNOIQPDMIwMwETbi3XroKgITj893ZYYhmFUIaqabhsyEhFZCfyYbjtSREtgTbqNSBKZfG3ptC0V507GORLVZ137qcvxe6pqi9qe2EbaHqhq63TbkCpEZJyqXphuO5JBJl9bOm1LxbmTcY5E9VnXfupyvIjMq+15wVZEGg7T0m1AEsnka0unbak4dzLOkag+69pP2v535h4xDMNIISIyT1U71fZ4G2kbhmGklnF1OdhG2oZhGFmEjbQNwzCyCBNtwzCMLMJE26gzItJORB4TkSnptiUZZPL1ZbJtdaU+X1tdMNHOMkRkJxGZLSJfiMgiEbmiDn09LiIrROTzCPu6ichiEVkiIkOi9aOq36nqebW1I+y8TUXkQxH5xL2+m+vQV1KuT0RyRWSBiLycabbVBRHJF5EpIvKViHwpIofUsp+Mu7Z6haraI4sewA7AP9znLYCvgb3C2mwLtAjbtnuEvo4A/gF8HrY9F/gWaAc0Bj4B9gL2AV4Oe2wbctyUBFyfAM3d5wHgA+DgTLo+4CrgOeDlCOfM5vf+KeB893ljIL++XFumPoAt3Pf9EWCgr2PSbbQ96vxPfxE4NmzbacCbQBP39QXADI/jd43w5ToEmBnyeigw1IctCf1yAc2Aj4GDMuX6gB3dc3f1EO2sfO9xlmV/jxtR5tEmK68t1Q/gcWBFhOvvBiwGlgBD3G1nAj3d50V++jf3SBYjIrsChTij0SpUdTIwEygSkYHAuThfOL8UAEtDXi9zt3nZ0UpEHgIKRWRoHOfx6i9XRBbifPBfV9WMuT5gBnANUBmpbRa/922BlcATruvnURHZIrRBFl9bqnkSR6CrEJFcYCxwAs6vi/4ishfOICD4nlT46dxEO0sRkebA88CVqvpH+H5VvQPYADwInKSq65Jli6quUtWLVXU3VR2VgP4qVHV/nA/0gSKyd4Q2Kb8+4Argf6o6P0b7bHzvG+G4NB5U1ULgT6CGzzlLry2lqOpc4PewzQcCS9Tx028CJgK9cG5cO7ptfOmxiXYWIiIBHMEer6pTPdocDuwNvADcFOcpSoCdQl7v6G5LKapa+v/t3V+IlFUYx/Hvrz/+yYsE01CKDCkrql0rL8ouROsmKqF/GwmVZISRdGNiESJlYVndJAlCN6WIFUaSJGRuZaEpRq62RiIGmWgQFWlSq/66OEd3dth1Z9yx3VPPB4adfec973vOsPPsec/MPA/QStWsBfptfJOAOyX9QHrRTZG0fID0ra/2AfsqrmreIwXxLgod20DQ01XGauBuSUupMZ9JBO3CSBLwJrDL9ms97DOB9FXZacAMYISkhXWcZitwmaRLJQ0C7gfW9K3ntZE0UtLwfH8ocCvwXdU+/TI+20/bvsj22Nxmg+0uFTJKfe5tHwB+lDQ+b5oKtFfuU+rYBjLbh23PsD3L9opa2kTQLs8k0psXUyR9k2+3Ve1zHnCf7T22jwMP0k1ucEkrgU3AeEn7JD0CYPso8ARp/XIX8I7tb8/ckLoYDbRKaiO9yD+2Xf3RuoE8voHct97MBlbk574ZeLHq8ZLH1t8adpURuUdCCKHB8ocEPrR9df79HNLHc6eSgvVW4IHT+acVM+0QQmig7q40GnmVETPtEEIoSMy0QwihIBG0QwihIBG0QwihIBG0QwihIBG0QwihIBG0QwihIBG0QxFygv7Hz+DxB0tan79h2pKz3F11msd6WNKSBvRpjGqo2iLpmb6eK5QjgnYoxXCg26Cdv23WVxMAbDfbXmV7pu323hqdSbb3276nhl0jaP+PRNAOpVgEjMsz4cWSJkvaKGkN0C5pbGV5K0lzJC3I98dJWidpW25zReWBJY0ClgMT8/HHSfpU0g358UOSXlAqgbZZ0oV5+x2Svsr5p9ef2N4TSQskvS1pk6Tdkh7N25XHtFPSDkktefvJMeXZ++o8jt2SXs7bFwFDc79XSBomaW3u684Txwr/HRG0QynmAXvyTPipvO064Enbl/fSdhkw2/b1wBzgjcoHbf8MzCTlym62vaeq/TBgs+0m4HNSxRaAL0il0CaQUrXOrWEc15Kq3twIzJc0BriLlKCpCbgFWCxpdDdtm4EWUnmuFkkX254HHMn9nk5KY7vfdlPOe7Guhj6FgjTisjKE/rLF9t5T7aBULOIm4N2U1RaAwXWe529S3UKAbaR0sZAyta3KAXYQqVxXbz6wfQQ4IqmVlBz/ZmCl7WPAQUmfAROBtqq2n9j+PY+rHbiErjmaAXYAr0p6iZSwaGMd4wwFiJl2KNnhivtH6fr3PCT/PAv4Lc9ET9yurPM8He5M0nOMzsnO68AS29cAj1Wc81Sqk/3Uk/znr4r7lf3oPJj9PekKZAewUNL8Oo4fChBBO5TiD1L1+Z4cBEYp1RUcDNwOkEux7ZV0L5xcP25qUJ/OpzMn8kM1tpkmaYikEcBkUorOjaTljrMljSRVM99SRz86lKoZkZdb/rS9HFhMN9VnQtlieSQUwfYvkr7Mb8x9BKyterxD0nOkYPcTXavdTAeWSnoWOJe0/ry9Ad1aQFp2+RXYQCqO25s2Ugm1C4Dnbe+X9D5pjXs7aeY91/aBnJO5FsuANklfA2+R1sSPAx3ArNqHE0oQqVlD+JfkT7Mcsv1Kf/cllCuWR0IIoSAx0w4hhILETDuEEAoSQTuEEAoSQTuEEAoSQTuEEAoSQTuEEAryDyBoi30eN9uVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdb578dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20} \n",
    "res_mlp_do = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, dropout=True)\n",
    "t.scatter_plot(Y, res_mlp_do, 'mlp with dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "evaluating with early stopping\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08864, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08864 to 0.08346, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08346 to 0.08315, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.08315 to 0.08297, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08297 to 0.08224, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.08275, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08224 to 0.08080, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08080 to 0.07997, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.07997 to 0.07986, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.07986 to 0.07833, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07833 to 0.07699, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07699 to 0.07684, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.07746, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.07684 to 0.07499, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07499 to 0.07344, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07344 to 0.07308, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.07369, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07308 to 0.07151, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07151 to 0.07005, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07005 to 0.06938, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06938 to 0.06800, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06800 to 0.06731, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06731 to 0.06621, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06621 to 0.06579, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06579 to 0.06382, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06382 to 0.06318, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.06318 to 0.06239, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.06239 to 0.06127, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.06127 to 0.05935, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05935 to 0.05868, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05868 to 0.05776, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05776 to 0.05675, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.05688, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05675 to 0.05451, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05451 to 0.05346, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.05346 to 0.05270, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.05270 to 0.05242, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.05242 to 0.05061, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.05061 to 0.05011, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05011 to 0.04980, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04980 to 0.04824, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04824 to 0.04803, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04803 to 0.04677, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.04677 to 0.04634, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.04636, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.04634 to 0.04484, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.04484 to 0.04446, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.04446 to 0.04443, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04443 to 0.04340, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.04340 to 0.04323, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.04323 to 0.04235, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.04243, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.04235 to 0.04103, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.04103 to 0.04074, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.04074 to 0.04061, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.04061 to 0.03990, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.04047, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03990 to 0.03911, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.03926, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.03971, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03911 to 0.03883, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.03883 to 0.03833, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.03833 to 0.03817, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.03817 to 0.03759, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.03896, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.03759 to 0.03723, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03723 to 0.03653, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03653 to 0.03606, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.03662, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.03606 to 0.03561, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03561 to 0.03507, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03507 to 0.03499, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.03499 to 0.03474, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.03474 to 0.03436, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.03436 to 0.03411, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.03445, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.03411 to 0.03390, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.03390 to 0.03372, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.03372 to 0.03323, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.03323 to 0.03312, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.03312 to 0.03302, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.03302 to 0.03256, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.03256 to 0.03241, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.03241 to 0.03239, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.03239 to 0.03217, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.03217 to 0.03208, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.03208 to 0.03191, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.03191 to 0.03134, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.03140, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.03187, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.03134 to 0.03104, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.03104 to 0.03073, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.03073 to 0.03026, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.03078, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.03026 to 0.03005, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.03005 to 0.02981, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02981 to 0.02953, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.02994, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.03015, did not improve\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02953 to 0.02925, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.02947, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.02949, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02925 to 0.02889, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.02911, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.02890, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02889 to 0.02819, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.02837, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.02862, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.02840, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.02819 to 0.02775, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02775 to 0.02736, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.02736 to 0.02726, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss is 0.02763, did not improve\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.02726 to 0.02725, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.02725 to 0.02693, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02693 to 0.02643, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00118: val_loss is 0.02690, did not improve\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.02643 to 0.02623, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02623 to 0.02600, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02600 to 0.02585, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.02585 to 0.02573, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss is 0.02627, did not improve\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.02573 to 0.02564, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.02578, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.02581, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.02620, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.02564 to 0.02512, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss is 0.02527, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.02512 to 0.02486, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.02489, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02486 to 0.02460, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.02461, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.02460 to 0.02421, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.02426, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.02427, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.02421 to 0.02395, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.02395 to 0.02379, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.02379 to 0.02358, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02358 to 0.02344, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss is 0.02350, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.02416, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.02344 to 0.02325, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.02375, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.02325, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.02325 to 0.02292, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.02299, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.02292 to 0.02277, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.02303, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.02293, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.02277 to 0.02270, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.02270 to 0.02239, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.02239 to 0.02233, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.02233 to 0.02203, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss is 0.02214, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.02206, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.02283, did not improve\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.02203 to 0.02178, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.02178 to 0.02176, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.02176 to 0.02149, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.02158, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.02180, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.02155, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.02149 to 0.02104, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.02104 to 0.02096, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.02134, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.02096 to 0.02076, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.02106, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.02078, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.02078, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.02098, did not improve\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.02076 to 0.02045, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.02045 to 0.02021, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.02079, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.02027, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.02021 to 0.02004, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.02004 to 0.01991, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.01991 to 0.01981, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.01981 to 0.01979, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss is 0.01998, did not improve\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.01979 to 0.01975, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01975 to 0.01944, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01944 to 0.01935, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.01935 to 0.01929, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.01940, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01929 to 0.01918, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.01918 to 0.01905, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss is 0.01948, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01913, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.01905 to 0.01889, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.01889 to 0.01880, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.01880 to 0.01866, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.01908, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.01866 to 0.01852, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.01852 to 0.01846, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss is 0.01848, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01851, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.01846 to 0.01825, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.01825 to 0.01819, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss is 0.01843, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.01819 to 0.01803, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.01803 to 0.01795, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.01795 to 0.01788, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.01788 to 0.01785, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.01785 to 0.01783, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.01783 to 0.01763, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.01763 to 0.01751, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss is 0.01782, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01791, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01860, did not improve\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.01751 to 0.01737, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.01737 to 0.01732, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.01751, did not improve\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.01732 to 0.01723, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.01723 to 0.01712, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.01712 to 0.01711, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss is 0.01718, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.01711 to 0.01710, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.01740, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.01710 to 0.01688, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.01688 to 0.01669, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss is 0.01670, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01736, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.01669 to 0.01667, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.01667 to 0.01661, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.01661 to 0.01647, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.01762, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01679, did not improve\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.01647 to 0.01640, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.01640 to 0.01625, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.01625 to 0.01618, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.01618 to 0.01612, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.01684, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01613, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.01612 to 0.01591, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.01616, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01593, did not improve\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.01591 to 0.01581, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss is 0.01627, did not improve\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.01581 to 0.01578, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss is 0.01600, did not improve\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.01578 to 0.01567, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.01567 to 0.01559, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.01559 to 0.01555, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.01555 to 0.01543, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.01543 to 0.01538, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.01552, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00248: val_loss improved from 0.01538 to 0.01534, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.01574, did not improve\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.01534 to 0.01521, storing weights.\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.01521 to 0.01513, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss is 0.01641, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01517, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.01513 to 0.01501, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss is 0.01526, did not improve\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.01501 to 0.01499, storing weights.\n",
      "\n",
      "Epoch 00257: val_loss is 0.01512, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01521, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01619, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.01499 to 0.01490, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.01490 to 0.01487, storing weights.\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.01487 to 0.01475, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.01475 to 0.01463, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss is 0.01480, did not improve\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.01463 to 0.01452, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss is 0.01494, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.01488, did not improve\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.01452 to 0.01449, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.01449 to 0.01439, storing weights.\n",
      "\n",
      "Epoch 00270: val_loss is 0.01467, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.01439 to 0.01426, storing weights.\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.01426 to 0.01423, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01450, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01453, did not improve\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.01423 to 0.01413, storing weights.\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.01413 to 0.01410, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.01438, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01425, did not improve\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.01410 to 0.01401, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.01401 to 0.01393, storing weights.\n",
      "\n",
      "Epoch 00283: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01393, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01426, did not improve\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.01393 to 0.01389, storing weights.\n",
      "\n",
      "Epoch 00287: val_loss is 0.01429, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.01389 to 0.01385, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.01401, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.01387, did not improve\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.01385 to 0.01362, storing weights.\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.01362 to 0.01358, storing weights.\n",
      "\n",
      "Epoch 00293: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01373, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01360, did not improve\n",
      "\n",
      "Epoch 00296: val_loss improved from 0.01358 to 0.01351, storing weights.\n",
      "\n",
      "Epoch 00297: val_loss is 0.01420, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01394, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01434, did not improve\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.01351 to 0.01339, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.01339 to 0.01338, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.01342, did not improve\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.01338 to 0.01321, storing weights.\n",
      "\n",
      "Epoch 00305: val_loss is 0.01478, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01345, did not improve\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.01321 to 0.01318, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.01318 to 0.01317, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.01317 to 0.01312, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.01312 to 0.01304, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.01310, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01387, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01333, did not improve\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.01304 to 0.01299, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.01299 to 0.01297, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.01314, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.01297 to 0.01285, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.01285 to 0.01278, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.01278 to 0.01277, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.01277 to 0.01275, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01289, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01354, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.01275 to 0.01264, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.01264 to 0.01258, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01274, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.01262, did not improve\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.01258 to 0.01255, storing weights.\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.01255 to 0.01250, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss is 0.01253, did not improve\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.01250 to 0.01249, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss is 0.01251, did not improve\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.01249 to 0.01243, storing weights.\n",
      "\n",
      "Epoch 00341: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.01243 to 0.01235, storing weights.\n",
      "\n",
      "Epoch 00343: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.01235 to 0.01233, storing weights.\n",
      "\n",
      "Epoch 00345: val_loss is 0.01254, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.01233 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.01231 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.01224 to 0.01222, storing weights.\n",
      "\n",
      "Epoch 00350: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.01222 to 0.01218, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.01218 to 0.01211, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss is 0.01242, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.01211 to 0.01207, storing weights.\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.01207 to 0.01204, storing weights.\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.01204 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00361: val_loss is 0.01205, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.01202 to 0.01199, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.01199 to 0.01198, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01210, did not improve\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.01198 to 0.01195, storing weights.\n",
      "\n",
      "Epoch 00368: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01272, did not improve\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.01195 to 0.01193, storing weights.\n",
      "\n",
      "Epoch 00371: val_loss is 0.01197, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.01193 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.01185 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01197, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.01185 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.01185 to 0.01179, storing weights.\n",
      "\n",
      "Epoch 00383: val_loss is 0.01193, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00384: val_loss improved from 0.01179 to 0.01165, storing weights.\n",
      "\n",
      "Epoch 00385: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.01165 to 0.01163, storing weights.\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.01163 to 0.01161, storing weights.\n",
      "\n",
      "Epoch 00390: val_loss is 0.01216, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.01161 to 0.01154, storing weights.\n",
      "\n",
      "Epoch 00396: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01198, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01221, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01321, did not improve\n",
      "\n",
      "Epoch 00402: val_loss improved from 0.01154 to 0.01147, storing weights.\n",
      "\n",
      "Epoch 00403: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01223, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00410: val_loss improved from 0.01147 to 0.01137, storing weights.\n",
      "\n",
      "Epoch 00411: val_loss improved from 0.01137 to 0.01132, storing weights.\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.01132 to 0.01128, storing weights.\n",
      "\n",
      "Epoch 00413: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.01128 to 0.01128, storing weights.\n",
      "\n",
      "Epoch 00420: val_loss improved from 0.01128 to 0.01125, storing weights.\n",
      "\n",
      "Epoch 00421: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01131, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.01125 to 0.01122, storing weights.\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.01122 to 0.01121, storing weights.\n",
      "\n",
      "Epoch 00426: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.01121 to 0.01116, storing weights.\n",
      "\n",
      "Epoch 00430: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.01151, did not improve\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.01116 to 0.01110, storing weights.\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.01110 to 0.01105, storing weights.\n",
      "\n",
      "Epoch 00437: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.01105 to 0.01101, storing weights.\n",
      "\n",
      "Epoch 00448: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.01329, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00456: val_loss improved from 0.01101 to 0.01097, storing weights.\n",
      "\n",
      "Epoch 00457: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.01097 to 0.01093, storing weights.\n",
      "\n",
      "Epoch 00459: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00464: val_loss improved from 0.01093 to 0.01093, storing weights.\n",
      "\n",
      "Epoch 00465: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.01171, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.01093 to 0.01088, storing weights.\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.01088 to 0.01084, storing weights.\n",
      "\n",
      "Epoch 00475: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.01084 to 0.01080, storing weights.\n",
      "\n",
      "Epoch 00480: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00483: val_loss improved from 0.01080 to 0.01076, storing weights.\n",
      "\n",
      "Epoch 00484: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00491: val_loss improved from 0.01076 to 0.01075, storing weights.\n",
      "\n",
      "Epoch 00492: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.01084, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.01075 to 0.01073, storing weights.\n",
      "\n",
      "Epoch 00498: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.01073 to 0.01069, storing weights.\n",
      "\n",
      "Epoch 00504: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.01115, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.01069 to 0.01062, storing weights.\n",
      "\n",
      "Epoch 00515: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.01062 to 0.01061, storing weights.\n",
      "\n",
      "Epoch 00521: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00528: val_loss improved from 0.01061 to 0.01059, storing weights.\n",
      "\n",
      "Epoch 00529: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.01059 to 0.01058, storing weights.\n",
      "\n",
      "Epoch 00531: val_loss is 0.01097, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00532: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.01247, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.01058 to 0.01056, storing weights.\n",
      "\n",
      "Epoch 00554: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00575: val_loss improved from 0.01056 to 0.01056, storing weights.\n",
      "\n",
      "Epoch 00576: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.01237, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00601: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00607: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00608: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.01084, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00618: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00619: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00625: val_loss improved from 0.01056 to 0.01049, storing weights.\n",
      "\n",
      "Epoch 00626: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00627: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00628: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00629: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00630: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00631: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00632: val_loss improved from 0.01049 to 0.01046, storing weights.\n",
      "\n",
      "Epoch 00633: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00634: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00635: val_loss is 0.01052, did not improve\n",
      "\n",
      "Epoch 00636: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00637: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00638: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00639: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00640: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00641: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00642: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00643: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00644: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00645: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00646: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00647: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00648: val_loss is 0.01050, did not improve\n",
      "\n",
      "Epoch 00649: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00650: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00651: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00652: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00653: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00654: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00655: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00656: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00657: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00658: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00659: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00660: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00661: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00662: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00663: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00664: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00665: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00666: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00667: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00668: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00669: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00670: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00671: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00672: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00673: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00674: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00675: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00676: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00677: val_loss is 0.01131, did not improve\n",
      "\n",
      "Epoch 00678: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00679: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00680: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00681: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00682: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00683: val_loss is 0.01243, did not improve\n",
      "\n",
      "Epoch 00684: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00685: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00686: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00687: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00688: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00689: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00690: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00691: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00692: val_loss is 0.01072, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00693: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00694: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00695: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00696: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00697: val_loss is 0.01151, did not improve\n",
      "\n",
      "Epoch 00698: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00699: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00700: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00701: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00702: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00703: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00704: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00705: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00706: val_loss is 0.01239, did not improve\n",
      "\n",
      "Epoch 00707: val_loss is 0.01062, did not improve\n",
      "Epoch 00707: early stopping\n",
      "Using epoch 00632 with val_loss: 0.01046\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06741, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06741 to 0.06671, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06671 to 0.06524, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06524 to 0.06461, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06461 to 0.06424, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.06424 to 0.06365, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06365 to 0.06342, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06342 to 0.06202, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06202 to 0.06096, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06096 to 0.06024, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06024 to 0.05951, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05951 to 0.05845, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05845 to 0.05789, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05789 to 0.05672, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05672 to 0.05629, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.05629 to 0.05511, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05511 to 0.05415, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05415 to 0.05347, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05347 to 0.05244, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.05244 to 0.05176, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05176 to 0.05139, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05139 to 0.05025, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.05047, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05025 to 0.04984, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.06437, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.04984 to 0.04711, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.04711 to 0.04635, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.04733, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.04654, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04635 to 0.04411, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04411 to 0.04374, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04374 to 0.04313, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04313 to 0.04274, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04274 to 0.04225, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04225 to 0.04174, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.06660, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04174 to 0.04118, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04118 to 0.04012, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04012 to 0.03950, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.03960, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03950 to 0.03868, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.03868 to 0.03847, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.06111, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03847 to 0.03840, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.04014, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03840 to 0.03714, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03714 to 0.03644, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03644 to 0.03640, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.04262, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.03705, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03640 to 0.03540, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03540 to 0.03486, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03486 to 0.03474, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03474 to 0.03443, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.03683, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.03443 to 0.03400, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.03416, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.03400 to 0.03353, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03353 to 0.03329, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.03329 to 0.03289, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.04438, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.04564, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.03411, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.03477, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.04525, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.03289 to 0.03230, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.03230 to 0.03209, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03209 to 0.03161, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.03161 to 0.03100, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.05525, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.04174, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03100 to 0.03029, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.03029 to 0.03016, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.03016 to 0.02984, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.02984 to 0.02966, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.02966 to 0.02957, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.03972, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.02957 to 0.02942, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.02942 to 0.02920, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02920 to 0.02896, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.02896 to 0.02866, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.02867, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.02866 to 0.02818, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.02909, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02818 to 0.02794, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.02815, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.02794 to 0.02740, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.02740 to 0.02722, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.03138, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02750, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.05152, did not improve\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02722 to 0.02690, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.02690 to 0.02668, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.03311, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.02723, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.02668 to 0.02631, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.02631 to 0.02600, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.02600 to 0.02575, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.02580, did not improve\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.02575 to 0.02520, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.03355, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02520 to 0.02514, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.02514 to 0.02474, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.02477, did not improve\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.02474 to 0.02438, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02438 to 0.02430, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.02855, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.02430 to 0.02375, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.02427, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.02389, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.02375 to 0.02361, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02361 to 0.02342, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00113: val_loss improved from 0.02342 to 0.02326, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss is 0.02372, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.02601, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.02326 to 0.02316, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.02337, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.02316 to 0.02284, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.02388, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02284 to 0.02264, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02264 to 0.02226, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.02226 to 0.02220, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss is 0.02309, did not improve\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.02220 to 0.02215, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.02229, did not improve\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.02215 to 0.02193, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02193 to 0.02170, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.03500, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.02235, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.02170 to 0.02112, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss is 0.02522, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.02117, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.02112 to 0.02092, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.03149, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.03569, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.03318, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.02100, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.02092 to 0.02080, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.02080 to 0.02034, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss is 0.02327, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.02070, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.02034 to 0.02032, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.02032 to 0.01979, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.02008, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.01979 to 0.01965, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.01977, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.04678, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.01977, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.01989, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.01975, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.01965 to 0.01929, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.01929 to 0.01892, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.01892 to 0.01857, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss is 0.02051, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.01878, did not improve\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.01857 to 0.01847, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.01847 to 0.01827, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss is 0.01875, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.01908, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.03995, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01911, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.01860, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.01837, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.01942, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.01842, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.01827 to 0.01822, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.01822 to 0.01792, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.01792 to 0.01759, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.02119, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.04396, did not improve\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.01759 to 0.01735, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.01735 to 0.01717, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.01722, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.01722, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01717 to 0.01693, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.01693 to 0.01693, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss is 0.01724, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.01754, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.02936, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.01695, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01693 to 0.01660, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.01678, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.01682, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01756, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01660 to 0.01637, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.01804, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.01727, did not improve\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.01637 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss is 0.01620, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01700, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.01605 to 0.01600, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.01617, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.04747, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.04374, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01633, did not improve\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.01600 to 0.01577, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss is 0.01593, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.01589, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01578, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.01577 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.01583, did not improve\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.01556 to 0.01517, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss is 0.01535, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01527, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.01517 to 0.01512, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.01512 to 0.01503, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss is 0.01562, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.04707, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01713, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01573, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01650, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.02080, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01520, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.03134, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01524, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01632, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01541, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.01525, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.01646, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.01571, did not improve\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.01503 to 0.01461, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss is 0.01528, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.04053, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01609, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.03252, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01560, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01535, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.01889, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.01515, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.01804, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.01491, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01745, did not improve\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.01461 to 0.01438, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.01438 to 0.01403, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.01414, did not improve\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.01403 to 0.01398, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss is 0.01426, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.01717, did not improve\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.01398 to 0.01379, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss is 0.01387, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01425, did not improve\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.01379 to 0.01356, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss is 0.01411, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.03369, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01638, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01490, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.03611, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.01372, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.04511, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.01356 to 0.01333, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00255: val_loss is 0.01354, did not improve\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.01333 to 0.01326, storing weights.\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.01326 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00258: val_loss is 0.04131, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01418, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01588, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01323, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01320, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.01364, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.01382, did not improve\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.01316 to 0.01299, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.01299 to 0.01297, storing weights.\n",
      "\n",
      "Epoch 00267: val_loss is 0.01332, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.01318, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01328, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01345, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01308, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.01297 to 0.01288, storing weights.\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.01288 to 0.01267, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss is 0.01326, did not improve\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.01267 to 0.01243, storing weights.\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.01243 to 0.01242, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.01242 to 0.01239, storing weights.\n",
      "\n",
      "Epoch 00280: val_loss is 0.01251, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.01302, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01909, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.01288, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01293, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.01239 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.01224 to 0.01215, storing weights.\n",
      "\n",
      "Epoch 00290: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01220, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01522, did not improve\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.01215 to 0.01206, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01281, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01272, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01226, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.05402, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.02358, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01366, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.01313, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.01206 to 0.01195, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss is 0.06415, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01201, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.03386, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01546, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01265, did not improve\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.01195 to 0.01160, storing weights.\n",
      "\n",
      "Epoch 00314: val_loss is 0.01555, did not improve\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.01160 to 0.01146, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.04233, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01771, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01434, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.03882, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01394, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01236, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01312, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01261, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01258, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.01404, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.01259, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01414, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01173, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01232, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.01146 to 0.01133, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss is 0.03861, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01229, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.05035, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01234, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.03363, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01955, did not improve\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.01133 to 0.01117, storing weights.\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.01117 to 0.01095, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.02790, did not improve\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.01095 to 0.01076, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01115, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01295, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01199, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01182, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01137, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01115, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.01438, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01239, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01118, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00409: val_loss is 0.01111, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01115, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.04948, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.01198, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.01612, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.01437, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.02345, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.01239, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.01396, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.01177, did not improve\n",
      "Epoch 00448: early stopping\n",
      "Using epoch 00373 with val_loss: 0.01076\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06286, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.07822, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.07988, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.06549, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06286 to 0.06028, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.07464, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06028 to 0.05866, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05866 to 0.05742, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05742 to 0.05663, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.05843, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05663 to 0.05505, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.05531, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05505 to 0.05399, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.05399 to 0.05336, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.05336 to 0.05269, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.05303, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.05269 to 0.05126, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.05179, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.05126 to 0.05036, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.06398, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05036 to 0.05034, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.05034 to 0.04954, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.04954 to 0.04784, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.04784 to 0.04713, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.04713, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.04713 to 0.04535, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.06239, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.04535 to 0.04515, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.04515 to 0.04424, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.04517, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04424 to 0.04233, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04233 to 0.04170, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04170 to 0.04160, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04160 to 0.04054, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.05521, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.04062, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.04275, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.04157, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04054 to 0.03909, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03909 to 0.03755, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.03776, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.03755 to 0.03745, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03745 to 0.03729, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03729 to 0.03684, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.03695, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03684 to 0.03644, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03644 to 0.03614, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03614 to 0.03602, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03602 to 0.03553, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.03632, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03553 to 0.03544, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03544 to 0.03539, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03539 to 0.03535, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.03535 to 0.03485, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.03485 to 0.03432, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.03432 to 0.03410, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.03463, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.03751, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.03410 to 0.03299, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.03308, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.03299 to 0.03279, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.03378, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.03332, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.03279 to 0.03216, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.03219, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.03216 to 0.03172, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.03215, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03172 to 0.03140, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.03140 to 0.03109, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.03122, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03109 to 0.03092, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.03384, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.03092 to 0.03047, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.03083, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.03062, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.03047 to 0.02988, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.03025, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02990, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.03068, did not improve\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.02988 to 0.02962, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.02962 to 0.02902, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.02902 to 0.02875, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.02934, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02939, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.02875 to 0.02859, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.02917, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.02859 to 0.02819, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.02819 to 0.02804, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.02804 to 0.02780, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.02780 to 0.02761, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.02761 to 0.02737, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.02737 to 0.02712, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.02713, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.02887, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.02712 to 0.02676, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00097: val_loss is 0.02679, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.02772, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.02676 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.02615, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.02670, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.02604 to 0.02598, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.02628, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.02598 to 0.02555, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.02782, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.02555 to 0.02532, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.02557, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.02859, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.02532 to 0.02502, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.02524, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.02539, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02502 to 0.02449, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.03337, did not improve\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.02449 to 0.02385, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss is 0.02556, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.02412, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.02385 to 0.02372, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.02512, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.03332, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.02372 to 0.02330, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.02330 to 0.02299, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.02314, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.02299 to 0.02290, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.02335, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.02429, did not improve\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.02290 to 0.02271, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss is 0.02426, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.02352, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.02271 to 0.02227, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.02233, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.02283, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.02227 to 0.02221, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.02232, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.02285, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.02376, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.02266, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.03213, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.02221 to 0.02103, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.02132, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.02188, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.02152, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.02167, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.02192, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.02214, did not improve\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.02103 to 0.02066, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss is 0.02335, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.02121, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.02066 to 0.02038, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.03632, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.02180, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.02356, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.02264, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.02038 to 0.02022, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.02023, did not improve\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.02022 to 0.02009, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.02009 to 0.02003, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.02122, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.02096, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.02003 to 0.01977, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.01977 to 0.01960, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.01960 to 0.01953, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss is 0.02014, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.05389, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.01994, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.01953 to 0.01921, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.01921 to 0.01831, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.01975, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01888, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.01992, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.01897, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.01831 to 0.01792, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.01855, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.01846, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.02094, did not improve\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.01792 to 0.01771, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss is 0.01856, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01800, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.01771 to 0.01758, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.01809, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.01790, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.01794, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01758 to 0.01721, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.01721 to 0.01718, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.01718 to 0.01716, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.01716 to 0.01698, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss is 0.01884, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.04227, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.01841, did not improve\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.01698 to 0.01676, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss is 0.01684, did not improve\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.01676 to 0.01667, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.01667 to 0.01666, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.03129, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.01691, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.01683, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01730, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01777, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.01666 to 0.01623, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.01836, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01928, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01665, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.01623 to 0.01591, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.01591 to 0.01559, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss is 0.01632, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01649, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.01616, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.05359, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.01605, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01595, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01612, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01640, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01570, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.04092, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01684, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.01629, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01973, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01592, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01656, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.01573, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.01584, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.01704, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.01903, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01701, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.01559 to 0.01544, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.01544 to 0.01500, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss is 0.01595, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01545, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01579, did not improve\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.01500 to 0.01490, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss is 0.01507, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.01511, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.01490 to 0.01437, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.01450, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01489, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.01681, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.01574, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01711, did not improve\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.01437 to 0.01430, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.01430 to 0.01416, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00240: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.01467, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01530, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01508, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01548, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01446, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01626, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.01456, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01440, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01472, did not improve\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.01416 to 0.01388, storing weights.\n",
      "\n",
      "Epoch 00251: val_loss is 0.01545, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.01552, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01649, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.01388 to 0.01370, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.01370 to 0.01358, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.01427, did not improve\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.01358 to 0.01334, storing weights.\n",
      "\n",
      "Epoch 00258: val_loss is 0.01443, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01363, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01422, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.08354, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.01673, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.01447, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.03947, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.01395, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01368, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01378, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01368, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01339, did not improve\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.01334 to 0.01268, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.01458, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01306, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.01322, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.01432, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.01268 to 0.01262, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss is 0.04552, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01399, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.01390, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01440, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01342, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01417, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.01262 to 0.01241, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.01382, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01313, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.02968, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01358, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01516, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01436, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01334, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01260, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.01293, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.01366, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.01460, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01310, did not improve\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.01241 to 0.01230, storing weights.\n",
      "\n",
      "Epoch 00305: val_loss is 0.01262, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01320, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01310, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01641, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01421, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01292, did not improve\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.01230 to 0.01214, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss is 0.01322, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.01214 to 0.01212, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.01212 to 0.01186, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01276, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.01235, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.04189, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.14099, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.01186 to 0.01178, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01231, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01311, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01269, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.07708, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.01385, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01271, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01445, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01394, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.01410, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.02007, did not improve\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.01178 to 0.01138, storing weights.\n",
      "\n",
      "Epoch 00342: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01277, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01259, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01211, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01247, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01232, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.01226, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.01138 to 0.01122, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss is 0.01198, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01653, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.01234, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01314, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01231, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01351, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.03013, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01274, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.01281, did not improve\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.01122 to 0.01122, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.04480, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01318, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01461, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01262, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01707, did not improve\n",
      "\n",
      "Epoch 00385: val_loss improved from 0.01122 to 0.01111, storing weights.\n",
      "\n",
      "Epoch 00386: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01197, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.01111 to 0.01096, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00396: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01163, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01342, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01271, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01385, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01334, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.01181, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.01328, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.01480, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01689, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.01096 to 0.01061, storing weights.\n",
      "\n",
      "Epoch 00447: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.01210, did not improve\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.01061 to 0.01060, storing weights.\n",
      "\n",
      "Epoch 00450: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.01213, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.05473, did not improve\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.01060 to 0.01015, storing weights.\n",
      "\n",
      "Epoch 00454: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.01353, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.01428, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.01015 to 0.01002, storing weights.\n",
      "\n",
      "Epoch 00472: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.01313, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.01273, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.02171, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01332, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.01122, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.01290, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.01567, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.01273, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.01727, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.01137, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.01454, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.01195, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.04627, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01256, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.01356, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.04087, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.01183, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.01034, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.04612, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01149, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.04818, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.01292, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.01237, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.01249, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.01182, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.01382, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.01216, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.01221, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.03125, did not improve\n",
      "Epoch 00546: early stopping\n",
      "Using epoch 00471 with val_loss: 0.01002\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00628] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00699]\n",
      " [ 0.0059 ]\n",
      " [ 0.00595]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.0034] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00297]\n",
      " [ 0.0042 ]\n",
      " [ 0.00303]]\n",
      "mse over all validation data 0.00628485497844\n",
      "path plots/mlp_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effddc86198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'l1': 0.0005, 'l2': 0.0005} \n",
    "# config found by hyperband on L1L2 case\n",
    "cfg = {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
    "res_mlp_l1l2 = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, L1L2=True)\n",
    "t.scatter_plot(Y, res_mlp_l1l2, 'mlp L1L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/mlp L1L2_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effde0a4be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.scatter_plot(Y, res_mlp_l1l2, 'mlp L1L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20, 'l2': 0.0005, 'l1': 0.0005}\n",
      "evaluating with early stopping\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13878, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13878 to 0.13441, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13441 to 0.13311, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13311 to 0.12898, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12898 to 0.12612, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12612 to 0.12332, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12332 to 0.12179, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12179 to 0.11723, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11723 to 0.11458, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11458 to 0.11094, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.11159, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11094 to 0.10704, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.10704 to 0.10298, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.10298 to 0.09987, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09987 to 0.09763, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09763 to 0.09566, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09566 to 0.09190, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09190 to 0.09067, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09067 to 0.08468, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08468 to 0.08131, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08131 to 0.07876, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.07936, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.07876 to 0.07391, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.07391 to 0.07179, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.07274, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07179 to 0.06694, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.06719, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.06694 to 0.06254, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.06254 to 0.06189, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.06189 to 0.05762, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05762 to 0.05643, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05643 to 0.05382, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.05598, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05382 to 0.05181, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05181 to 0.04973, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04973 to 0.04836, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04836 to 0.04614, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.04641, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04614 to 0.04497, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.04497 to 0.04257, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.04257 to 0.04066, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.04066 to 0.04062, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.04062 to 0.03852, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03852 to 0.03753, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03753 to 0.03734, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03734 to 0.03710, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03710 to 0.03538, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03538 to 0.03534, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.03559, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03534 to 0.03295, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03295 to 0.03158, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03158 to 0.03145, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03145 to 0.03029, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.03115, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.03029 to 0.02868, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02868 to 0.02785, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.02840, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02815, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02785 to 0.02677, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02677 to 0.02572, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.02631, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02598, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02740, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02572 to 0.02377, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02377 to 0.02319, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.02319 to 0.02265, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.02307, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02290, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.02265 to 0.02206, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.02206 to 0.02113, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.02147, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02171, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.02113 to 0.02090, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.02291, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.02090 to 0.01963, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.01964, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02112, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.01963 to 0.01910, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.01910 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.01869 to 0.01820, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss is 0.01830, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02027, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.01820 to 0.01791, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.02002, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.01833, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.01791 to 0.01723, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.01740, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.01840, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.01723 to 0.01676, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.01676 to 0.01643, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.01728, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.01645, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.01643 to 0.01583, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.02016, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.01583 to 0.01551, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.01596, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.01743, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01563, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.01725, did not improve\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.01551 to 0.01509, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.01509 to 0.01475, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.01476, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.01776, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.01508, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.01489, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.01479, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.01475 to 0.01474, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.01537, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.01474 to 0.01414, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.01414 to 0.01404, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss is 0.01612, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.01517, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.01417, did not improve\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.01404 to 0.01401, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.01401 to 0.01387, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.01409, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.01387 to 0.01374, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.01374 to 0.01353, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.01400, did not improve\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.01353 to 0.01332, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss is 0.01391, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss is 0.01502, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.01332 to 0.01315, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.01315 to 0.01315, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss is 0.01368, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.01610, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.01481, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.01406, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.01569, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.01315 to 0.01286, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.01411, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.01777, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.01511, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.01295, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.02091, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.01286 to 0.01257, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.01482, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.01341, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.01439, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.01933, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.02194, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.01257 to 0.01229, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.01229 to 0.01227, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss is 0.01237, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.01336, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.01261, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.01227 to 0.01214, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.01226, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01243, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.01220, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.01214 to 0.01203, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.01532, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.01455, did not improve\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.01203 to 0.01201, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01426, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.01370, did not improve\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.01201 to 0.01193, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01205, did not improve\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.01193 to 0.01187, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.01320, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.01210, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01235, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01187 to 0.01183, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.01183 to 0.01178, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.01268, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01182, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.01231, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.01636, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01539, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.01441, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01358, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.01178 to 0.01157, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.01173, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.01157 to 0.01152, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.01228, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.01152 to 0.01148, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.01148 to 0.01144, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.01257, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01148, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.01171, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01293, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.01144 to 0.01142, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.01142 to 0.01133, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.01133 to 0.01132, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.01293, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.01132 to 0.01128, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01706, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.01128 to 0.01121, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.01121 to 0.01118, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss is 0.01423, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01316, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01651, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01295, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.01118 to 0.01115, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss is 0.01369, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.01288, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.01683, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01137, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01719, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01210, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.01115 to 0.01113, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss is 0.01231, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01151, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.01113 to 0.01110, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01405, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01242, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01147, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00274: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.01110 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00277: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.01106 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.01394, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01115, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01604, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.01349, did not improve\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.01106 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00290: val_loss is 0.01300, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.01110, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01450, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01677, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01264, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.01356, did not improve\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.01106 to 0.01096, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.01096 to 0.01095, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.01095 to 0.01095, storing weights.\n",
      "\n",
      "Epoch 00304: val_loss is 0.01981, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01111, did not improve\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.01095 to 0.01091, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss is 0.01337, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.01091 to 0.01089, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.01089 to 0.01080, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.01080 to 0.01076, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss is 0.01253, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.01190, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.01076 to 0.01065, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.01065 to 0.01057, storing weights.\n",
      "\n",
      "Epoch 00320: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01624, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01385, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01369, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01255, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.01057 to 0.01053, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss is 0.01204, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01440, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01211, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.01053 to 0.01048, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.01048 to 0.01045, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.01045 to 0.01043, storing weights.\n",
      "\n",
      "Epoch 00356: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.01061, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01504, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01251, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01910, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01221, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.01275, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01372, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01316, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01072, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01120, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.01065, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01638, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01289, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01624, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01302, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01092, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01852, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01046, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.02943, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01550, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01326, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01239, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.01363, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01067, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01247, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01142, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.01198, did not improve\n",
      "Epoch 00430: early stopping\n",
      "Using epoch 00355 with val_loss: 0.01043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12250, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12250 to 0.11789, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11789 to 0.11508, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11508 to 0.11202, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.11202 to 0.10928, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10928 to 0.10618, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10618 to 0.10392, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10392 to 0.10027, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10027 to 0.09770, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09770 to 0.09396, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09396 to 0.09140, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09140 to 0.08981, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08981 to 0.08741, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08741 to 0.08393, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08393 to 0.08024, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08024 to 0.07783, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07783 to 0.07630, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07630 to 0.07607, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07607 to 0.07209, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07209 to 0.06926, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06926 to 0.06766, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06766 to 0.06521, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06521 to 0.06398, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06398 to 0.06104, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06104 to 0.05944, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.05944 to 0.05823, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05823 to 0.05553, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05553 to 0.05381, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05381 to 0.05215, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05215 to 0.05101, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05101 to 0.04905, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04905 to 0.04766, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04766 to 0.04687, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04687 to 0.04581, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04581 to 0.04356, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.04358, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04356 to 0.04128, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.04128 to 0.04052, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04052 to 0.03897, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03897 to 0.03829, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03829 to 0.03733, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.03789, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.03733 to 0.03664, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03664 to 0.03407, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03407 to 0.03296, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.03384, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03296 to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03170 to 0.03066, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03066 to 0.03055, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03055 to 0.02932, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.02947, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02932 to 0.02831, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.02858, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02831 to 0.02647, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02647 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02604 to 0.02563, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02563 to 0.02486, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02486 to 0.02427, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.02432, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02427 to 0.02311, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02311 to 0.02242, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02242 to 0.02225, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02225 to 0.02174, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02174 to 0.02121, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02121 to 0.02080, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.02123, did not improve\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.02080 to 0.02019, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss is 0.02079, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02141, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.02019 to 0.01936, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.01936 to 0.01854, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.01854 to 0.01836, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.01836 to 0.01822, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.01860, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.01854, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.01822 to 0.01720, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.01720 to 0.01681, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.01689, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.01681 to 0.01667, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.01667 to 0.01628, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss is 0.01945, did not improve\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.01628 to 0.01571, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.01571 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.01556 to 0.01507, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.01812, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.01575, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.01507 to 0.01471, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.01511, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.01471 to 0.01456, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.01523, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.01557, did not improve\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.01456 to 0.01383, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.01383 to 0.01379, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.01418, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.01397, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.01379 to 0.01357, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.01377, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01457, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.01357 to 0.01313, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.01330, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.01313 to 0.01255, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.01263, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.01320, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.01273, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.01403, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.01340, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.01255 to 0.01238, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.01473, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.01256, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.01238 to 0.01198, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.01730, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.01351, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.01198 to 0.01182, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.01182 to 0.01132, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.01295, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.01183, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.01300, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.01254, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00125: val_loss improved from 0.01132 to 0.01121, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.01352, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.01376, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.01192, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.01311, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.01121 to 0.01099, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.01295, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.01289, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.01306, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.01099 to 0.01086, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.01112, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.01086 to 0.01063, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.01813, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.01088, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.01063 to 0.01048, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.01048 to 0.01025, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.01576, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.01111, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.01160, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.01080, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.01549, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.01025 to 0.01022, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.01263, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01035, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.01042, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.01479, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.01147, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.01105, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.01022 to 0.01022, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.01226, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.01176, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.01036, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01595, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.01359, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.01744, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.01187, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01222, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.02155, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.01043, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.01040, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.01152, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01127, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01570, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01404, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.01032, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01689, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01134, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01530, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.01022 to 0.01017, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.01043, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.01096, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01393, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.01274, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.01077, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01364, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01155, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.01258, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.01017 to 0.01005, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss is 0.01220, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.01546, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.01091, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.01030, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01086, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01375, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.01387, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.01027, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01028, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.01243, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.01019, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01315, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01238, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.01047, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01100, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01589, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.01156, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00283: val_loss is 0.01232, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01027, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01084, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.01079, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.01033, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01797, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01245, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.01266, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01988, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01261, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.01224, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.01320, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01020, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01174, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.01054, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01068, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01695, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01308, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01076, did not improve\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.01005 to 0.00991, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss is 0.01008, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.01553, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01124, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01572, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01521, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01030, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01135, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01116, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01070, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01322, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01376, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.01296, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.01223, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01154, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01030, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01047, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01155, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.01539, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.01178, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01234, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.02002, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01205, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01596, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01008, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.01533, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.01163, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.01166, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01291, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01375, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01016, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01209, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01136, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01447, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01669, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01323, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01455, did not improve\n",
      "Epoch 00390: early stopping\n",
      "Using epoch 00315 with val_loss: 0.00991\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11707, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11707 to 0.11430, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11430 to 0.11198, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11198 to 0.10855, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10855 to 0.10712, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10712 to 0.10297, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10297 to 0.10029, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10029 to 0.09819, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09819 to 0.09519, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09519 to 0.09318, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09318 to 0.09009, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09009 to 0.08674, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08674 to 0.08487, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08487 to 0.08283, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08283 to 0.08057, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08057 to 0.07802, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07802 to 0.07573, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07573 to 0.07488, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.07488 to 0.07132, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07132 to 0.06967, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06967 to 0.06738, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.06738 to 0.06543, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06543 to 0.06351, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06351 to 0.06146, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.06176, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06146 to 0.05801, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05801 to 0.05633, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.05642, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05633 to 0.05234, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05234 to 0.05164, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05164 to 0.04956, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.04956 to 0.04943, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04943 to 0.04755, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.04755 to 0.04546, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04546 to 0.04476, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04476 to 0.04307, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.04307 to 0.04171, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss improved from 0.04171 to 0.04092, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.04092 to 0.03992, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03992 to 0.03974, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03974 to 0.03969, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.03969 to 0.03732, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.03942, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.03732 to 0.03666, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.03666 to 0.03535, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.03535 to 0.03395, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.03395 to 0.03266, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03266 to 0.03244, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03244 to 0.03143, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03143 to 0.03022, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03022 to 0.02944, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.02983, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.02944 to 0.02899, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.02979, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02899 to 0.02715, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02715 to 0.02668, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02668 to 0.02613, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.02639, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02613 to 0.02483, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02483 to 0.02466, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.02608, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02466 to 0.02427, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02427 to 0.02403, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02403 to 0.02349, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.02349 to 0.02217, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.02370, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02228, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.02217 to 0.02063, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.02225, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.02063 to 0.02051, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.02517, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.02051 to 0.02032, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.02032 to 0.01954, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.01954 to 0.01949, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.01961, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.01949 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.02301, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.01904, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.01896, did not improve\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.01869 to 0.01814, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.01814 to 0.01752, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.01855, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.01780, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.01752 to 0.01652, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.01955, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.01710, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.01652 to 0.01615, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.01615 to 0.01607, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.01612, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.01607 to 0.01601, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.02163, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.01669, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02016, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.01601 to 0.01601, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.01601 to 0.01571, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.01571 to 0.01506, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.01843, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01647, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.01506 to 0.01505, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.01505 to 0.01408, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.01765, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.01437, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.01526, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.01408 to 0.01387, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.01424, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.01417, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.01397, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.01495, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.01407, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.01500, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.01423, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.01517, did not improve\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.01387 to 0.01355, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.01355 to 0.01309, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.01385, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.01442, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.01309 to 0.01248, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.01270, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.01564, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.01326, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.01588, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.01444, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.01248 to 0.01223, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss is 0.01234, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.01359, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.01241, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.01235, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.01448, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.01235, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.01236, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.01483, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.02067, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.01223 to 0.01192, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.01214, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.01319, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.01471, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.01500, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.01256, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.01505, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.01336, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.01546, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.01217, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.01306, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.01372, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.02052, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.01289, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.01192 to 0.01176, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss is 0.01220, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.01176 to 0.01162, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.01288, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.01254, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.01189, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.01865, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.01244, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.01237, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.01616, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.01566, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01512, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.01305, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00180: val_loss is 0.01209, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.01296, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.01162 to 0.01096, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.01176, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.01258, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.01280, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.01286, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.01322, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.01332, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.01248, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.01629, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.01310, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.01369, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.01576, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.01793, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.01374, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.01291, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.01162, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.01131, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.01159, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.01565, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.01658, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.01096 to 0.01083, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.02030, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.01225, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.01812, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.02266, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.01131, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.01161, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.01262, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.01313, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.01669, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.01298, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.01271, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.01359, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.01104, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.01319, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.01220, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.01197, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.01170, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.01373, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.01198, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.01329, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.01128, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.01452, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.02440, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.01246, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.01353, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.01447, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.02132, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.01258, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.01403, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.01318, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.01255, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.01340, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.01285, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.01631, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.01249, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.01381, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.01125, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.01481, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.01252, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.02302, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.01083 to 0.01079, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss is 0.01561, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.01355, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.01197, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.01215, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.01688, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01332, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.01216, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.01380, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.01157, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.01728, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.01181, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.01245, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.01144, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.01216, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01273, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.01201, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.01388, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.01325, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.01294, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.01188, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.01130, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.01670, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.01865, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.01140, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.01474, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.01407, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.01313, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.01402, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.01218, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.01267, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.01276, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.01253, did not improve\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.01079 to 0.01067, storing weights.\n",
      "\n",
      "Epoch 00330: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.01307, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.01259, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.01119, did not improve\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.01067 to 0.01056, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss is 0.01302, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.01181, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.01410, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00339: val_loss improved from 0.01056 to 0.01036, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.01535, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.01103, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.01193, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.01415, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.01516, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.01639, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.01321, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.01363, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.01359, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.01555, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.01414, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.01364, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.01245, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.02210, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.01419, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.01594, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.01233, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.01783, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.01205, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.01434, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.01212, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.01175, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.01063, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.01473, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.01093, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.01704, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.01085, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.01141, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.01254, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.01146, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.01087, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.01533, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.01251, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.01829, did not improve\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.01036 to 0.01031, storing weights.\n",
      "\n",
      "Epoch 00396: val_loss is 0.01619, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.01126, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.01090, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.01659, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.01438, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.01709, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.01934, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.01129, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.01973, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.01491, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.01419, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.01309, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.01083, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.01368, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.01073, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.01150, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.01177, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.01462, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.01359, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.02092, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.01055, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.01287, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.01094, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.01075, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.01723, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.01036, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.01321, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.01708, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.01035, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.01389, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.01279, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.01464, did not improve\n",
      "\n",
      "Epoch 00439: val_loss improved from 0.01031 to 0.01025, storing weights.\n",
      "\n",
      "Epoch 00440: val_loss is 0.01227, did not improve\n",
      "\n",
      "Epoch 00441: val_loss improved from 0.01025 to 0.00998, storing weights.\n",
      "\n",
      "Epoch 00442: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.01074, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.01078, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.01288, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.01331, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.01854, did not improve\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.00998 to 0.00992, storing weights.\n",
      "\n",
      "Epoch 00451: val_loss is 0.01156, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.01513, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.01196, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.01401, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.01167, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.01123, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.01616, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.01318, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.01764, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.01121, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.01017, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.01230, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.01247, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.01774, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.01006, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.01410, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.01066, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.01277, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.01013, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.01169, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.01139, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.01045, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.01250, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.01172, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.01089, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.01345, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.01053, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.03615, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.01469, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.01145, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.01060, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.01138, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.01153, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.01282, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.01164, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.01507, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.00992 to 0.00950, storing weights.\n",
      "\n",
      "Epoch 00498: val_loss is 0.01106, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.01171, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00500: val_loss is 0.01102, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.01612, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.01158, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.01206, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.01018, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.01007, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.01069, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.01041, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.01438, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.01219, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.01236, did not improve\n",
      "\n",
      "Epoch 00513: val_loss improved from 0.00950 to 0.00921, storing weights.\n",
      "\n",
      "Epoch 00514: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.01327, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.01057, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.01817, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.01293, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.01255, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.01022, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.01346, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.01059, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.01245, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00980, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.01336, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.01504, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.01242, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.01269, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.01859, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.01071, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.01111, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.01016, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.02437, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.01109, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.01042, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.01342, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.01757, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00956, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.01232, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.01042, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.01199, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.01031, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.01607, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.01180, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.01420, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.01185, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00999, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.01101, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.01265, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.01249, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.01173, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.01034, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00946, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00990, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.01108, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.00982, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.01011, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.01604, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.01407, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00978, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.01062, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.01007, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.01207, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.01400, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.01181, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.01314, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.01191, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.01118, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.01278, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.01303, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.01449, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00989, did not improve\n",
      "Epoch 00588: early stopping\n",
      "Using epoch 00513 with val_loss: 0.00921\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00588] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0067 ]\n",
      " [ 0.00563]\n",
      " [ 0.00531]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00402] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00376]\n",
      " [ 0.00366]\n",
      " [ 0.00464]]\n",
      "mse over all validation data 0.00588251943148\n",
      "path plots/mlp L1L2_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effde8c1a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'l1': 0.0005, 'l2': 0.0005} \n",
    "# config found by hyperband on L1L2 case\n",
    "# cfg = {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
    "res_mlp_l1l2 = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, L1L2=True)\n",
    "t.scatter_plot(Y, res_mlp_l1l2, 'mlp L1L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this mse mlp_500 0.00615437846885\n",
      "this mse from list mlp_500 0.00615437846885\n",
      "this mse earlystop 0.00568764191678\n",
      "this mse from list earlystop 0.00568764191678\n",
      "this mse dropout 0.00593283374683\n",
      "this mse from list dropout 0.00593283374683\n",
      "this mse l1l2 0.00588251943148\n",
      "this mse from list l1l2 0.00588251943148\n",
      "path plots/means of regularisation for mlp_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdcdbf780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.box_plot(Y, [res_mlp_500, res_mlp_es, res_mlp_do, res_mlp_l1l2],\n",
    "           ['mlp_500', 'earlystop', 'dropout', 'l1l2'], 'means of regularisation for mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [0] steps\n",
      "config {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
      "evaluating with early stopping\n",
      "evaluating with exponential decay\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04899, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04899 to 0.04654, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04964, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.04700, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04654 to 0.04416, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.04463, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04416 to 0.04257, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.04594, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04257 to 0.04139, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04139 to 0.04068, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.04068 to 0.03866, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03866 to 0.03788, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03788 to 0.03738, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03738 to 0.03540, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03540 to 0.03295, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.03356, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03295 to 0.03015, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.03015 to 0.02963, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02963 to 0.02765, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02765 to 0.02690, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02690 to 0.02543, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02543 to 0.02477, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02477 to 0.02266, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02266 to 0.02076, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02076 to 0.02001, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02001 to 0.01893, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01893 to 0.01862, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01862 to 0.01677, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01677 to 0.01665, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01665 to 0.01543, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01543 to 0.01462, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01462 to 0.01421, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01421 to 0.01251, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01251 to 0.01200, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01200 to 0.01104, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.01104 to 0.01060, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.01143, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.01098, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01060 to 0.00971, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.01045, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00971 to 0.00938, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00938 to 0.00920, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00923, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00920 to 0.00867, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00986, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00906, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00867 to 0.00841, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00884, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00841 to 0.00828, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00828 to 0.00811, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00811 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00951, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00850, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00808 to 0.00788, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00788 to 0.00773, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00773 to 0.00771, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00771 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00758 to 0.00754, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00754 to 0.00751, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00751 to 0.00748, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00748 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00747 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00741 to 0.00735, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00737, did not improve\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00735 to 0.00734, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00734 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00732 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00728 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00728 to 0.00725, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00725 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00758, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00133: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00724 to 0.00721, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00721 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00717 to 0.00717, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00717 to 0.00714, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00714 to 0.00711, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00711 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00709 to 0.00703, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00703 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00702 to 0.00700, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00700 to 0.00695, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00695 to 0.00694, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00694 to 0.00690, storing weights.\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00690 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00689 to 0.00688, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00688 to 0.00681, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00681 to 0.00681, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00681 to 0.00678, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00678 to 0.00676, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00676 to 0.00669, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00684, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00285: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.00669 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00292: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00670, did not improve\n",
      "new lr:  0.048749254134\n",
      "\n",
      "Epoch 00301: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00668 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.00668 to 0.00667, storing weights.\n",
      "\n",
      "Epoch 00305: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.00667 to 0.00667, storing weights.\n",
      "\n",
      "Epoch 00314: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00667 to 0.00666, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00666 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.00664 to 0.00663, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00663 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.00661 to 0.00660, storing weights.\n",
      "\n",
      "Epoch 00366: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.00660 to 0.00659, storing weights.\n",
      "\n",
      "Epoch 00372: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00373: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.00659 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.00658 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00379: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.00658 to 0.00654, storing weights.\n",
      "\n",
      "Epoch 00396: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00667, did not improve\n",
      "new lr:  0.0294396361375\n",
      "\n",
      "Epoch 00401: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.00654 to 0.00654, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00441: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00442: val_loss improved from 0.00654 to 0.00652, storing weights.\n",
      "\n",
      "Epoch 00443: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00448: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.00652 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.00651 to 0.00649, storing weights.\n",
      "\n",
      "Epoch 00452: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00478: val_loss improved from 0.00649 to 0.00648, storing weights.\n",
      "\n",
      "Epoch 00479: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.00648 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00497: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00650, did not improve\n",
      "new lr:  0.0177785730532\n",
      "\n",
      "Epoch 00501: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00516: val_loss improved from 0.00647 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00517: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00520: val_loss improved from 0.00647 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00521: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00525: val_loss improved from 0.00647 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00526: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.00647 to 0.00646, storing weights.\n",
      "\n",
      "Epoch 00531: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00546: val_loss improved from 0.00646 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00547: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00571: val_loss improved from 0.00645 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00572: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00577: val_loss improved from 0.00645 to 0.00644, storing weights.\n",
      "\n",
      "Epoch 00578: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00581: val_loss improved from 0.00644 to 0.00643, storing weights.\n",
      "\n",
      "Epoch 00582: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00591: val_loss improved from 0.00643 to 0.00643, storing weights.\n",
      "\n",
      "Epoch 00592: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.00645, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00599: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.00644, did not improve\n",
      "new lr:  0.0107364662501\n",
      "\n",
      "Epoch 00601: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00606: val_loss improved from 0.00643 to 0.00643, storing weights.\n",
      "\n",
      "Epoch 00607: val_loss improved from 0.00643 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00608: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00616: val_loss improved from 0.00642 to 0.00641, storing weights.\n",
      "\n",
      "Epoch 00617: val_loss improved from 0.00641 to 0.00641, storing weights.\n",
      "\n",
      "Epoch 00618: val_loss improved from 0.00641 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00619: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00625: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00626: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00627: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00628: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00629: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00630: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00631: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00632: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00633: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00634: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00635: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00636: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00637: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00638: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00639: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00640: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00641: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00642: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00643: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00644: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00645: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00646: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00647: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00648: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00649: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00650: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00651: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00652: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00653: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00654: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00655: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00656: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00657: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00658: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00659: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00660: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00661: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00662: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00663: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00664: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00665: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00666: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00667: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00668: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00669: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00670: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00671: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00672: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00673: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00674: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00675: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00676: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00677: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00678: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00679: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00680: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00681: val_loss improved from 0.00640 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00682: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00683: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00684: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00685: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00686: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00687: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00688: val_loss improved from 0.00640 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00689: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00690: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00691: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00692: val_loss improved from 0.00640 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00693: val_loss is 0.00640, did not improve\n",
      "Epoch 00693: early stopping\n",
      "Using epoch 00692 with val_loss: 0.00640\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03466, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03466 to 0.02952, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02952 to 0.02898, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.03419, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02898 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02918, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02813 to 0.02643, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02643 to 0.02504, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02504 to 0.02488, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02488 to 0.02294, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02294 to 0.02191, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02191 to 0.02131, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.02182, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02131 to 0.01983, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.02117, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01983 to 0.01823, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01823 to 0.01779, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01803, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01779 to 0.01696, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01696 to 0.01634, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01634 to 0.01504, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01504 to 0.01445, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01445 to 0.01401, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01435, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01401 to 0.01264, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01264 to 0.01214, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.01240, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01214 to 0.01134, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01134 to 0.01119, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01119 to 0.01040, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01040 to 0.01000, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01000 to 0.00982, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.01029, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00982 to 0.00929, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00929 to 0.00889, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00889 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00882 to 0.00846, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00846 to 0.00843, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00843 to 0.00836, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00836 to 0.00835, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00835 to 0.00809, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00809 to 0.00801, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00803, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00801 to 0.00776, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00776 to 0.00774, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00774 to 0.00771, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00937, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00771 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00766 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00745 to 0.00744, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00744 to 0.00734, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00734 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00733 to 0.00716, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00716 to 0.00713, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00713 to 0.00712, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00712 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00702 to 0.00701, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00701 to 0.00698, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00698 to 0.00692, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00692 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00713, did not improve\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00684 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00684 to 0.00682, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00682 to 0.00677, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00677 to 0.00676, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00676 to 0.00667, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00667 to 0.00667, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00667 to 0.00657, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00657 to 0.00656, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00656 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00651 to 0.00646, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00646 to 0.00637, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00637 to 0.00635, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00635 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00633 to 0.00630, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss is 0.00673, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00197: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00630 to 0.00621, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00628, did not improve\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00621 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00620 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00620 to 0.00619, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00619 to 0.00614, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00614 to 0.00613, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00613 to 0.00610, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00610 to 0.00610, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00610 to 0.00604, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00604 to 0.00603, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00603 to 0.00598, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00598 to 0.00594, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.00594 to 0.00593, storing weights.\n",
      "\n",
      "Epoch 00277: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.00593 to 0.00591, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.00591 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.00582 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00287: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00289: val_loss improved from 0.00582 to 0.00581, storing weights.\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00581 to 0.00580, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00592, did not improve\n",
      "new lr:  0.048749254134\n",
      "\n",
      "Epoch 00301: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00580 to 0.00580, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00580 to 0.00579, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00579 to 0.00576, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00576, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00349: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.00576 to 0.00574, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.00574 to 0.00573, storing weights.\n",
      "\n",
      "Epoch 00360: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.00573 to 0.00572, storing weights.\n",
      "\n",
      "Epoch 00362: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00572 to 0.00567, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00570, did not improve\n",
      "new lr:  0.0294396361375\n",
      "\n",
      "Epoch 00401: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.00567 to 0.00566, storing weights.\n",
      "\n",
      "Epoch 00410: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.00566 to 0.00565, storing weights.\n",
      "\n",
      "Epoch 00420: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.00565 to 0.00563, storing weights.\n",
      "\n",
      "Epoch 00424: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.00563 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00448: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.00560 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00460: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00473: val_loss improved from 0.00560 to 0.00559, storing weights.\n",
      "\n",
      "Epoch 00474: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.00559 to 0.00559, storing weights.\n",
      "\n",
      "Epoch 00476: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.00559 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00478: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00489: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00490: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00559, did not improve\n",
      "new lr:  0.0177785730532\n",
      "\n",
      "Epoch 00501: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00503: val_loss improved from 0.00558 to 0.00558, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00504: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00519: val_loss improved from 0.00558 to 0.00557, storing weights.\n",
      "\n",
      "Epoch 00520: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00533: val_loss improved from 0.00557 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00534: val_loss improved from 0.00556 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00535: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00553: val_loss improved from 0.00556 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00554: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00556: val_loss improved from 0.00556 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00557: val_loss improved from 0.00554 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00558: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00569: val_loss improved from 0.00554 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00570: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00578: val_loss improved from 0.00554 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00579: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00588: val_loss improved from 0.00554 to 0.00553, storing weights.\n",
      "\n",
      "Epoch 00589: val_loss improved from 0.00553 to 0.00553, storing weights.\n",
      "\n",
      "Epoch 00590: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.00553 to 0.00553, storing weights.\n",
      "\n",
      "Epoch 00594: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.00553, did not improve\n",
      "new lr:  0.0107364662501\n",
      "\n",
      "Epoch 00601: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00607: val_loss improved from 0.00553 to 0.00552, storing weights.\n",
      "\n",
      "Epoch 00608: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00613: val_loss improved from 0.00552 to 0.00552, storing weights.\n",
      "\n",
      "Epoch 00614: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00618: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00619: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00625: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00626: val_loss improved from 0.00552 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00627: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00628: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00629: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00630: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00631: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00632: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00633: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00634: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00635: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00636: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00637: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00638: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00639: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00640: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00641: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00642: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00643: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00644: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00645: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00646: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00647: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00648: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00649: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00650: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00651: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00652: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00653: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00654: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00655: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00656: val_loss improved from 0.00551 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00657: val_loss improved from 0.00551 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00658: val_loss improved from 0.00551 to 0.00550, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00659: val_loss improved from 0.00550 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00660: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00661: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00662: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00663: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00664: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00665: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00666: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00667: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00668: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00669: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00670: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00671: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00672: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00673: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00674: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00675: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00676: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00677: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00678: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00679: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00680: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00681: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00682: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00683: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00684: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00685: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00686: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00687: val_loss improved from 0.00550 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00688: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00689: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00690: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00691: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00692: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00693: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00694: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00695: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00696: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00697: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00698: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00699: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00700: val_loss is 0.00551, did not improve\n",
      "new lr:  0.00648374350381\n",
      "\n",
      "Epoch 00701: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00702: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00703: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00704: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00705: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00706: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00707: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00708: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00709: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00710: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00711: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00712: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00713: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00714: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00715: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00716: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00717: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00718: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00719: val_loss improved from 0.00550 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00720: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00721: val_loss improved from 0.00550 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00722: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00723: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00724: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00725: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00726: val_loss improved from 0.00550 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00727: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00728: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00729: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00730: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00731: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00732: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00733: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00734: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00735: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00736: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00737: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00738: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00739: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00740: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00741: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00742: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00743: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00744: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00745: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00746: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00747: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00748: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00749: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00750: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00751: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00752: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00753: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00754: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00755: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00756: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00757: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00758: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00759: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00760: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00761: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00762: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00763: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00764: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00765: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00766: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00767: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00768: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00769: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00770: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00771: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00772: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00773: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00774: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00775: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00776: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00777: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00778: val_loss improved from 0.00549 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00779: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00780: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00781: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00782: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00783: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00784: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00785: val_loss improved from 0.00549 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00786: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00787: val_loss improved from 0.00548 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00788: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00789: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00790: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00791: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00792: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00793: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00794: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00795: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00796: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00797: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00798: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00799: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00800: val_loss improved from 0.00548 to 0.00548, storing weights.\n",
      "new lr:  0.00391552759015\n",
      "\n",
      "Epoch 00801: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00802: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00803: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00804: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00805: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00806: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00807: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00808: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00809: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00810: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00811: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00812: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00813: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00814: val_loss is 0.00548, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00815: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00816: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00817: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00818: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00819: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00820: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00821: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00822: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00823: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00824: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00825: val_loss is 0.00548, did not improve\n",
      "Epoch 00825: early stopping\n",
      "Using epoch 00800 with val_loss: 0.00548\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "new lr:  0.221347482799\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02643, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02643 to 0.02582, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02582 to 0.02525, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.02621, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02525 to 0.02418, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02418 to 0.02345, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.02471, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02345 to 0.02199, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.02403, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02199 to 0.02085, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02085 to 0.02056, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02056 to 0.01973, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01973 to 0.01884, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01884 to 0.01831, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.01932, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01831 to 0.01718, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01718 to 0.01603, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01610, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01603 to 0.01561, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01561 to 0.01398, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01398 to 0.01337, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01337 to 0.01314, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01314 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01202 to 0.01162, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01162 to 0.01145, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01145 to 0.01039, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01039 to 0.01012, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01012 to 0.00973, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00973 to 0.00855, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00855 to 0.00852, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00852 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00984, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00803 to 0.00796, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00796 to 0.00780, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00780 to 0.00769, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00769 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00762 to 0.00722, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00722 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00715 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00668 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00668 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00661 to 0.00648, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00842, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00648 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00640 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00640 to 0.00621, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00621 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00620 to 0.00601, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00633, did not improve\n",
      "new lr:  0.133671570351\n",
      "\n",
      "Epoch 00101: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00601 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00707, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00599 to 0.00584, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00613, did not improve\n",
      "new lr:  0.0807241559483\n",
      "\n",
      "Epoch 00201: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00602, did not improve\n",
      "Epoch 00223: early stopping\n",
      "Using epoch 00148 with val_loss: 0.00584\n",
      "MSE on validation data on [0] steps: means over folds: *** [ 0.00591] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0064 ]\n",
      " [ 0.00548]\n",
      " [ 0.00584]]\n",
      "MSE on train data on [0] steps: means over folds: *** [ 0.00159] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00104]\n",
      " [ 0.00132]\n",
      " [ 0.00239]]\n",
      "mse over all validation data 0.00590785560253\n",
      "path plots/mlp with exponential decay_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "box_plot() missing 1 required positional argument: 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-ccca12155d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_mlp_ed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mlp with exponential decay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m t.box_plot(Y, [res_mlp_es, res_mlp_ed],\n\u001b[0;32m----> 6\u001b[0;31m            ['lr 0.22', 'exponential decay'])\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: box_plot() missing 1 required positional argument: 'title'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdcc3a0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate mlp via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928} \n",
    "res_mlp_ed = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, lr_exp_decay=True)\n",
    "t.scatter_plot(Y, res_mlp_ed, 'mlp with exponential decay')\n",
    "t.box_plot(Y, [res_mlp_es, res_mlp_ed],\n",
    "           ['lr 0.22', 'exponential decay'], 'with and without exponential decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this mse lr 0.22 0.00568764191678\n",
      "this mse from list lr 0.22 0.00568764191678\n",
      "this mse exponential decay 0.00590785560253\n",
      "this mse from list exponential decay 0.00590785560253\n",
      "path plots/with and without exponential decay_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdcda4f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.box_plot(Y, [res_mlp_es, res_mlp_ed],\n",
    "           ['lr 0.22', 'exponential decay'], 'with and without exponential decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03841, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03841 to 0.02144, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02144 to 0.01620, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01620 to 0.00893, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00893 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00655 to 0.00385, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00385 to 0.00355, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00355 to 0.00288, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00288 to 0.00283, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00283 to 0.00255, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00343, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00373, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00255 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00244 to 0.00236, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00236 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00232 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00231 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00229 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00226 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00224 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00221 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00219 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00218 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00216 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00215 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00212 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00210 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00206 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00202 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00196 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00190 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00183 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00175 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00165 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00156 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00145 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00135 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00125 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00116 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00108 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00103 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00096 to 0.00095, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00063 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00039 to 0.00038, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00253: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00047, did not improve\n",
      "Epoch 00325: early stopping\n",
      "Using epoch 00253 with val_loss: 0.00038\n",
      "validate on 5 steps, mse on train / validation data: 0.03367 / 0.04668\n",
      "validate on 10 steps, mse on train / validation data: 0.03294 / 0.04512\n",
      "validate on 20 steps, mse on train / validation data: 0.04143 / 0.05570\n",
      "validate on 30 steps, mse on train / validation data: 0.03833 / 0.05190\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02232, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03457, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.02233, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02232 to 0.00722, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00722 to 0.00278, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00457, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00278 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00186 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00117 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00101 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00088 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00079 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00066 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00058 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00053 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00049 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00047 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00068, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00053, did not improve\n",
      "Epoch 00120: early stopping\n",
      "Using epoch 00045 with val_loss: 0.00041\n",
      "validate on 5 steps, mse on train / validation data: 0.04070 / 0.03240\n",
      "validate on 10 steps, mse on train / validation data: 0.04060 / 0.03231\n",
      "validate on 20 steps, mse on train / validation data: 0.03865 / 0.03078\n",
      "validate on 30 steps, mse on train / validation data: 0.05149 / 0.04087\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09558, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09558 to 0.08155, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08155 to 0.02731, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02731 to 0.00597, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00597 to 0.00526, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00526 to 0.00377, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00377 to 0.00273, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00273 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00160 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00134 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00116 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00107 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00103 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00095 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00092 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00086 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00083 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00080 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00077 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00075 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00044 to 0.00044, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00073: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00043, did not improve\n",
      "Epoch 00171: early stopping\n",
      "Using epoch 00147 with val_loss: 0.00042\n",
      "validate on 5 steps, mse on train / validation data: 0.04613 / 0.02783\n",
      "validate on 10 steps, mse on train / validation data: 0.04523 / 0.02721\n",
      "validate on 20 steps, mse on train / validation data: 0.04275 / 0.02565\n",
      "validate on 30 steps, mse on train / validation data: 0.03125 / 0.01846\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.03564  0.03488  0.03738  0.03708] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04668  0.04512  0.0557   0.0519 ]\n",
      " [ 0.0324   0.03231  0.03078  0.04087]\n",
      " [ 0.02783  0.02721  0.02565  0.01846]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04017  0.03959  0.04094  0.04036] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.03367  0.03294  0.04143  0.03833]\n",
      " [ 0.0407   0.0406   0.03865  0.05149]\n",
      " [ 0.04613  0.04523  0.04275  0.03125]]\n",
      "mse over all validation data 0.0371339693018\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07340, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07340 to 0.02909, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02909 to 0.01916, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 0.01916 to 0.00486, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00486 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00320, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00213 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00180 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00168 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00141 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00127 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00104 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00086 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00080 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00075 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00072 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00069 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00061 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00058 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00084, did not improve\n",
      "Epoch 00101: early stopping\n",
      "Using epoch 00026 with val_loss: 0.00054\n",
      "validate on 5 steps, mse on train / validation data: 0.05636 / 0.08935\n",
      "validate on 10 steps, mse on train / validation data: 0.05785 / 0.09236\n",
      "validate on 20 steps, mse on train / validation data: 0.06913 / 0.10966\n",
      "validate on 30 steps, mse on train / validation data: 0.05808 / 0.09241\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09279, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09279 to 0.04862, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04862 to 0.02798, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02798 to 0.01432, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01432 to 0.01242, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01242 to 0.00604, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00604 to 0.00296, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00296 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00166 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00071 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00039, did not improve\n",
      "Epoch 00154: early stopping\n",
      "Using epoch 00099 with val_loss: 0.00039\n",
      "validate on 5 steps, mse on train / validation data: 0.04324 / 0.03465\n",
      "validate on 10 steps, mse on train / validation data: 0.04351 / 0.03491\n",
      "validate on 20 steps, mse on train / validation data: 0.04192 / 0.03361\n",
      "validate on 30 steps, mse on train / validation data: 0.02009 / 0.01593\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11866, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11866 to 0.04006, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04006 to 0.00835, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00835 to 0.00698, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00698 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00228 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00154 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00082 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00067, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 0.00053 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00049 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00037, did not improve\n",
      "Epoch 00141: early stopping\n",
      "Using epoch 00082 with val_loss: 0.00031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 0.05284 / 0.03300\n",
      "validate on 10 steps, mse on train / validation data: 0.05236 / 0.03228\n",
      "validate on 20 steps, mse on train / validation data: 0.05860 / 0.03601\n",
      "validate on 30 steps, mse on train / validation data: 0.03803 / 0.02312\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.05233  0.05318  0.05976  0.04382] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.08935  0.09236  0.10966  0.09241]\n",
      " [ 0.03465  0.03491  0.03361  0.01593]\n",
      " [ 0.033    0.03228  0.03601  0.02312]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.05081  0.05124  0.05655  0.03873] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.05636  0.05785  0.06913  0.05808]\n",
      " [ 0.04324  0.04351  0.04192  0.02009]\n",
      " [ 0.05284  0.05236  0.0586   0.03803]]\n",
      "mse over all validation data 0.0440058553659\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02917, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02917 to 0.01587, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01587 to 0.00757, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00757 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00117 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00069 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00062 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00060, did not improve\n",
      "Epoch 00083: early stopping\n",
      "Using epoch 00008 with val_loss: 0.00042\n",
      "validate on 5 steps, mse on train / validation data: 0.02300 / 0.03649\n",
      "validate on 10 steps, mse on train / validation data: 0.00734 / 0.00916\n",
      "validate on 20 steps, mse on train / validation data: 0.00518 / 0.00438\n",
      "validate on 30 steps, mse on train / validation data: 0.00274 / 0.00227\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09216, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09216 to 0.05531, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05531 to 0.00721, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.00942, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00721 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00197 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00077 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00050 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00034 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00029 to 0.00029, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00028, did not improve\n",
      "Epoch 00155: early stopping\n",
      "Using epoch 00091 with val_loss: 0.00025\n",
      "validate on 5 steps, mse on train / validation data: 0.01124 / 0.00828\n",
      "validate on 10 steps, mse on train / validation data: 0.00351 / 0.00239\n",
      "validate on 20 steps, mse on train / validation data: 0.00359 / 0.00386\n",
      "validate on 30 steps, mse on train / validation data: 0.00170 / 0.00137\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11981, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11981 to 0.07597, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07597 to 0.01432, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 0.01432 to 0.00532, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00532 to 0.00487, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00487 to 0.00314, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00314 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00106 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00080 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00068 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00056 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00053 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00050 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00048 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00029 to 0.00029, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00116: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00036, did not improve\n",
      "Epoch 00202: early stopping\n",
      "Using epoch 00172 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.02791 / 0.02917\n",
      "validate on 10 steps, mse on train / validation data: 0.01271 / 0.01067\n",
      "validate on 20 steps, mse on train / validation data: 0.00325 / 0.00335\n",
      "validate on 30 steps, mse on train / validation data: 0.00112 / 0.00149\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02465  0.00741  0.00386  0.00171] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03649  0.00916  0.00438  0.00227]\n",
      " [ 0.00828  0.00239  0.00386  0.00137]\n",
      " [ 0.02917  0.01067  0.00335  0.00149]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02072  0.00785  0.00401  0.00186] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.023    0.00734  0.00518  0.00274]\n",
      " [ 0.01124  0.00351  0.00359  0.0017 ]\n",
      " [ 0.02791  0.01271  0.00325  0.00112]]\n",
      "mse over all validation data 0.00171396584175\n",
      "results validation data \n",
      " [[ 0.03564  0.03488  0.03738  0.03708]\n",
      " [ 0.05233  0.05318  0.05976  0.04382]\n",
      " [ 0.02465  0.00741  0.00386  0.00171]]\n",
      "results training data\n",
      " [[ 0.04017  0.03959  0.04094  0.04036]\n",
      " [ 0.05081  0.05124  0.05655  0.03873]\n",
      " [ 0.02072  0.00785  0.00401  0.00186]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.2\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "res_lstm_ns = []   # evaluating with next steps\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                    steps=(train_steps,[5,10,20,30]), \n",
    "                    cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "    res_train[i], res_val[i] = res['trn_means'], res['val_means']\n",
    "    res_lstm_ns.append(res)\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)\n",
    "t.pickle_to_file(res_lstm_ns, 'res_lstm_nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08584, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08584 to 0.07439, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07439 to 0.02369, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02369 to 0.01386, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01386 to 0.01289, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01289 to 0.00480, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00480 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00335 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00121 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00099 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00063 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00046 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00059, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00036, did not improve\n",
      "Epoch 00160: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.01471 / 0.02526\n",
      "validate on 10 steps, mse on train / validation data: 0.01031 / 0.01362\n",
      "validate on 20 steps, mse on train / validation data: 0.00436 / 0.00365\n",
      "validate on 30 steps, mse on train / validation data: 0.00166 / 0.00143\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60643, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60643 to 0.04722, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04722 to 0.04653, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.06111, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04653 to 0.03205, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03300, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03205 to 0.02693, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02693 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01068 to 0.00309, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00309 to 0.00285, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00285 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00163 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00095 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00060 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00055 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00041 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00035 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00026 to 0.00020, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00081 with val_loss: 0.00020\n",
      "validate on 5 steps, mse on train / validation data: 0.16159 / 0.08641\n",
      "validate on 10 steps, mse on train / validation data: 0.12393 / 0.07440\n",
      "validate on 20 steps, mse on train / validation data: 0.03269 / 0.02646\n",
      "validate on 30 steps, mse on train / validation data: 0.00476 / 0.00441\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11001, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11001 to 0.08327, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08327 to 0.03608, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03608 to 0.01065, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01065 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00554 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00247 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00202 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00197 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00120 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00058 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00043 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00031 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "Epoch 00119: early stopping\n",
      "Using epoch 00055 with val_loss: 0.00025\n",
      "validate on 5 steps, mse on train / validation data: 0.02207 / 0.03434\n",
      "validate on 10 steps, mse on train / validation data: 0.01448 / 0.02045\n",
      "validate on 20 steps, mse on train / validation data: 0.00771 / 0.00859\n",
      "validate on 30 steps, mse on train / validation data: 0.00305 / 0.00316\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04867  0.03615  0.0129   0.003  ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02526  0.01362  0.00365  0.00143]\n",
      " [ 0.08641  0.0744   0.02646  0.00441]\n",
      " [ 0.03434  0.02045  0.00859  0.00316]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.06613  0.04957  0.01492  0.00315] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01471  0.01031  0.00436  0.00166]\n",
      " [ 0.16159  0.12393  0.03269  0.00476]\n",
      " [ 0.02207  0.01448  0.00771  0.00305]]\n",
      "results training data\n",
      " [ 0.06612556  0.04957401  0.01491995  0.00315308]\n",
      "results validation data \n",
      " [ 0.04866848  0.03615473  0.01290235  0.00300086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train with random lenghts\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])\n",
    "t.pickle_to_file(res, 'res_lstm_nextstep_random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11554, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11554 to 0.04302, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04302 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01769 to 0.01097, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01097 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00882 to 0.00847, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00847 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00606 to 0.00438, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00438 to 0.00315, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.01499, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.01350, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00410, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00315 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00251 to 0.00250, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00250 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00220 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00181 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00160 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00137 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00362, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00150, did not improve\n",
      "Epoch 00123: early stopping\n",
      "Using epoch 00049 with val_loss: 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 459us/step\n",
      "mse:  0.00131853502845\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 870us/step\n",
      "mse:  0.00133469417448\n",
      "validate on 5 steps, mse on train / validation data: 0.00133 / 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.001162388689\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00171085036444\n",
      "validate on 10 steps, mse on train / validation data: 0.00171 / 0.00116\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00321647753217\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00388603562235\n",
      "validate on 20 steps, mse on train / validation data: 0.00389 / 0.00322\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00459700725512\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.00526945944875\n",
      "validate on 30 steps, mse on train / validation data: 0.00527 / 0.00460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34632, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34632 to 0.06494, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06494 to 0.05340, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05340 to 0.02735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02735 to 0.01483, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.02008, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01483 to 0.01388, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01388 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01224 to 0.00833, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00833 to 0.00687, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00687 to 0.00294, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00460, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00359, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00294 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00487, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00441, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00466, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00401, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00226 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00220 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00216 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00214 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00211 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00211 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00209 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00206 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00201 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00197 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00193 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00176 to 0.00175, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00168 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00158 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00155 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00152 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00150 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00148 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00144 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00142 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00131 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00126, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00122 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00120 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00119 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.00118 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00116, did not improve\n",
      "Epoch 00328: early stopping\n",
      "Using epoch 00323 with val_loss: 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00115162648779\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 903us/step\n",
      "mse:  0.0012707624862\n",
      "validate on 5 steps, mse on train / validation data: 0.00127 / 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00158448308833\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0011413448456\n",
      "validate on 10 steps, mse on train / validation data: 0.00114 / 0.00158\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0039176084207\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.00348516953004\n",
      "validate on 20 steps, mse on train / validation data: 0.00349 / 0.00392\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0050399990075\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.00497433785639\n",
      "validate on 30 steps, mse on train / validation data: 0.00497 / 0.00504\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.99042, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.99042 to 1.47941, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 2.96802, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47941 to 0.63913, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63913 to 0.51863, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51863 to 0.08843, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08843 to 0.05703, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05703 to 0.04830, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04830 to 0.03768, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03768 to 0.03411, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03411 to 0.02496, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02528, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02496 to 0.02449, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02449 to 0.02441, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02461, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02456, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02441 to 0.02439, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02445, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02451, did not improve\n",
      "Epoch 00093: early stopping\n",
      "Using epoch 00018 with val_loss: 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 765us/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 776us/step\n",
      "mse:  0.0299257414868\n",
      "validate on 5 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 10 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 20 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 30 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00895  0.00904  0.01051  0.01134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00132  0.00116  0.00322  0.0046 ]\n",
      " [ 0.00115  0.00158  0.00392  0.00504]\n",
      " [ 0.02439  0.02439  0.02439  0.02439]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01084  0.01093  0.01243  0.01339] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00133  0.00171  0.00389  0.00527]\n",
      " [ 0.00127  0.00114  0.00349  0.00497]\n",
      " [ 0.02993  0.02993  0.02993  0.02993]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12057, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12057 to 0.06355, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06355 to 0.03759, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03759 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00790 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00766 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00323 to 0.00304, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00304 to 0.00301, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00301 to 0.00289, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00289 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00259 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00227 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00199 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00178 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00152 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00122 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00120 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00104 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00149, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00089 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00084, did not improve\n",
      "Epoch 00120: early stopping\n",
      "Using epoch 00088 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 481us/step\n",
      "mse:  0.0608066631418\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 855us/step\n",
      "mse:  0.0477681301365\n",
      "validate on 5 steps, mse on train / validation data: 0.04777 / 0.06081\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000824270867403\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000763803920894\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00128009282\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00143760890404\n",
      "validate on 20 steps, mse on train / validation data: 0.00144 / 0.00128\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00213678000401\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.0023780453167\n",
      "validate on 30 steps, mse on train / validation data: 0.00238 / 0.00214\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26239, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26239 to 0.07691, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.12018, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07691 to 0.05873, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05873 to 0.02621, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03345, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02621 to 0.02494, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02494 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01605 to 0.01107, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01107 to 0.00395, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00395 to 0.00253, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00253 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00169 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00079 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00064 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00065, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00068, did not improve\n",
      "Epoch 00106: early stopping\n",
      "Using epoch 00083 with val_loss: 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 590us/step\n",
      "mse:  0.0690140077336\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 744us/step\n",
      "mse:  0.0738325941319\n",
      "validate on 5 steps, mse on train / validation data: 0.07383 / 0.06901\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000604276981903\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000690422686459\n",
      "validate on 10 steps, mse on train / validation data: 0.00069 / 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.00140974406068\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0012300674403\n",
      "validate on 20 steps, mse on train / validation data: 0.00123 / 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00224876565732\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00229339740215\n",
      "validate on 30 steps, mse on train / validation data: 0.00229 / 0.00225\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12354, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12354 to 0.04591, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04597, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04591 to 0.02748, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02748 to 0.01963, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01963 to 0.01252, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01252 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00803 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00658 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00560 to 0.00375, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00375 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00194 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00174 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00172 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00153 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00150 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00140 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00110 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00108 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00104 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00095 to 0.00094, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "Epoch 00104: early stopping\n",
      "Using epoch 00104 with val_loss: 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 526us/step\n",
      "mse:  0.789464980364\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 710us/step\n",
      "mse:  0.750682481601\n",
      "validate on 5 steps, mse on train / validation data: 0.75068 / 0.78946\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000914607464272\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000559727040101\n",
      "validate on 10 steps, mse on train / validation data: 0.00056 / 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.980713968927\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.812348923104\n",
      "validate on 20 steps, mse on train / validation data: 0.81235 / 0.98071\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.750668579882\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "mse:  0.836846585664\n",
      "validate on 30 steps, mse on train / validation data: 0.83685 / 0.75067\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.30643  0.00078  0.3278   0.25168] ***\n",
      "Results validation data of all Folds: \n",
      "[[  6.08100000e-02   8.20000000e-04   1.28000000e-03   2.14000000e-03]\n",
      " [  6.90100000e-02   6.00000000e-04   1.41000000e-03   2.25000000e-03]\n",
      " [  7.89460000e-01   9.10000000e-04   9.80710000e-01   7.50670000e-01]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.29076  0.00067  0.27167  0.28051] ***\n",
      "Results training data of all Folds: \n",
      "[[  4.77700000e-02   7.60000000e-04   1.44000000e-03   2.38000000e-03]\n",
      " [  7.38300000e-02   6.90000000e-04   1.23000000e-03   2.29000000e-03]\n",
      " [  7.50680000e-01   5.60000000e-04   8.12350000e-01   8.36850000e-01]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14798, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14798 to 0.04405, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04405 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00993 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00817 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00592 to 0.00324, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00324 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00244 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00093 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00059 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00055 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00050 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "Epoch 00116: early stopping\n",
      "Using epoch 00041 with val_loss: 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 641us/step\n",
      "mse:  0.0258297447869\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 860us/step\n",
      "mse:  0.0263428334147\n",
      "validate on 5 steps, mse on train / validation data: 0.02634 / 0.02583\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.0107730397683\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00910780364013\n",
      "validate on 10 steps, mse on train / validation data: 0.00911 / 0.01077\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000469433154199\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000500161555961\n",
      "validate on 20 steps, mse on train / validation data: 0.00050 / 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.000675560233544\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.000807838148665\n",
      "validate on 30 steps, mse on train / validation data: 0.00081 / 0.00068\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17151 to 1.22318, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22318 to 0.66304, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.96452, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66304 to 0.11085, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11085 to 0.07196, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07196 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03687 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03687 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00837 to 0.00530, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00530 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00227 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00175 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00161 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00113 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00107 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00103 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00100 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00070 to 0.00069, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00060 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00051 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00051, did not improve\n",
      "Epoch 00223: early stopping\n",
      "Using epoch 00156 with val_loss: 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 665us/step\n",
      "mse:  0.0250337426974\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 799us/step\n",
      "mse:  0.0298722941786\n",
      "validate on 5 steps, mse on train / validation data: 0.02987 / 0.02503\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  4.91890081492\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  5.00050854009\n",
      "validate on 10 steps, mse on train / validation data: 5.00051 / 4.91890\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000351062770113\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000462018283239\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  3.78144966472\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  3.87956652129\n",
      "validate on 30 steps, mse on train / validation data: 3.87957 / 3.78145\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11390, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11390 to 0.05036, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05036 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01418, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01418 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00294, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00257 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00224 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00218 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00158 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00129 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00113 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00096 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00081 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.00073, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00212 with val_loss: 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 614us/step\n",
      "mse:  0.00897776255045\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 853us/step\n",
      "mse:  0.0123261121959\n",
      "validate on 5 steps, mse on train / validation data: 0.01233 / 0.00898\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00932959319008\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0085558105197\n",
      "validate on 10 steps, mse on train / validation data: 0.00856 / 0.00933\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000731091864344\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000461046307382\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000526615978742\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.000265021034481\n",
      "validate on 30 steps, mse on train / validation data: 0.00027 / 0.00053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [  1.99500000e-02   1.64633000e+00   5.20000000e-04   1.26088000e+00] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.58300000e-02   1.07700000e-02   4.70000000e-04   6.80000000e-04]\n",
      " [  2.50300000e-02   4.91890000e+00   3.50000000e-04   3.78145000e+00]\n",
      " [  8.98000000e-03   9.33000000e-03   7.30000000e-04   5.30000000e-04]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [  2.28500000e-02   1.67272000e+00   4.70000000e-04   1.29355000e+00] ***\n",
      "Results training data of all Folds: \n",
      "[[  2.63400000e-02   9.11000000e-03   5.00000000e-04   8.10000000e-04]\n",
      " [  2.98700000e-02   5.00051000e+00   4.60000000e-04   3.87957000e+00]\n",
      " [  1.23300000e-02   8.56000000e-03   4.60000000e-04   2.70000000e-04]]\n",
      "[[  1.08437327e-02   1.09259789e-02   1.24323155e-02   1.33898463e-02]\n",
      " [  2.90761069e-01   6.71317882e-04   2.71672200e-01   2.80506009e-01]\n",
      " [  2.28470799e-02   1.67272405e+00   4.74408716e-04   1.29354646e+00]]\n",
      "[[  8.95270427e-03   9.04494103e-03   1.05073458e-02   1.13416525e-02]\n",
      " [  3.06428550e-01   7.81051771e-04   3.27801269e-01   2.51684709e-01]\n",
      " [  1.99470833e-02   1.64633448e+00   5.17195930e-04   1.26088395e+00]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.3 base line training with fixed lenghts (on final epoch)\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "res_lstm_finalstep = []   # evaluating directly on final steps\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                     steps=(train_steps,[5,10,20,30]), \n",
    "                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                     mode='finalstep')\n",
    "    res_train[i], res_val[i] = res['trn_means'], res['val_means']\n",
    "    res_lstm_finalstep.append(res)    \n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)\n",
    "t.pickle_to_file(res_lstm_finalstep, 'res_lstm_finalstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06410, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06410 to 0.03095, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03095 to 0.00941, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.01184, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00941 to 0.00702, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00702 to 0.00428, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00428 to 0.00373, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00373 to 0.00358, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00617, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00358 to 0.00316, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00437, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00316 to 0.00297, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00297 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00180 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00426, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00432, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00333, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00167 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00277, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00387, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00408, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00366, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00453, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00162 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00150 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00434, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00360, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00143 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00121 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00101 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00360, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00083 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00075 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00101, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_loss is 0.00392, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00336, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00068 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00062 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00086, did not improve\n",
      "Epoch 00301: early stopping\n",
      "Using epoch 00235 with val_loss: 0.00059\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00130 / 0.00146\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00073 / 0.00066\n",
      "evaluate lstm with consideration of configs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00058 / 0.00057\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00128 / 0.00111\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09386, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09386 to 0.05620, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05620 to 0.01881, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01881 to 0.00845, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.01113, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00845 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00803 to 0.00357, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00357 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00228 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00137 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00122 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00103 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00087 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00076 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00071 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00067 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00063 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00063, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00050 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00045 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00043 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "Epoch 00253: early stopping\n",
      "Using epoch 00253 with val_loss: 0.00041\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00144 / 0.00070\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00065 / 0.00049\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00053 / 0.00042\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00082 / 0.00061\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18725, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18725 to 0.08344, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08344 to 0.01869, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01869 to 0.01259, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01259 to 0.00759, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00759 to 0.00330, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00434, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00375, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00344, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00330 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00219 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00208 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00128 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00170, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss is 0.00295, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00111 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00087 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00124, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00190: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00096, did not improve\n",
      "Epoch 00226: early stopping\n",
      "Using epoch 00151 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00157 / 0.00258\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00053 / 0.00090\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00042 / 0.00059\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00049 / 0.00077\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00158  0.00068  0.00053  0.00083] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00146  0.00066  0.00057  0.00111]\n",
      " [ 0.0007   0.00049  0.00042  0.00061]\n",
      " [ 0.00258  0.0009   0.00059  0.00077]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00144  0.00064  0.00051  0.00086] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0013   0.00073  0.00058  0.00128]\n",
      " [ 0.00144  0.00065  0.00053  0.00082]\n",
      " [ 0.00157  0.00053  0.00042  0.00049]]\n",
      "results validation data \n",
      " [ 0.00158  0.00068  0.00053  0.00083]\n",
      "results training data\n",
      " [ 0.00144  0.00064  0.00051  0.00086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train train using final points with random lenghts\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='finalstep')\n",
    "\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])\n",
    "t.pickle_to_file(res, 'res_lstm_finalstep_random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'n_estimators': 1000, 'gamma': 0.007833441242813044, 'maxdepth': 10, 'cols_bt': 0.9376450587145334}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.584156378552\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.578395060919\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.527057613488\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.500720170913\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.494855966833\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.454835391707\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.458230453509\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.465432094203\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.472633742624\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.44094650061\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.398662551686\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.417901235598\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.432510285466\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.37952675422\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.399382723702\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.371913578775\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.374074074957\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.378189303257\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.351543205756\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.330349789725\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.318209884343\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.36121398652\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.320370373902\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.34917695434\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.318004115864\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.313580245883\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.321296292323\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.32448559558\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.34002057049\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.311934159862\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.339094645447\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.330349800763\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.306172834502\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.32294238276\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.302572014155\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.326982 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.318048 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.325919 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308534 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.317099 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.306027 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.311122 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.301471 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.301088 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.296533 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.296533 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.296533 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.296533 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.296533 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.296533 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.296533 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.296533 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.296533 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.296533 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.296533 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.296533 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.296533 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.296533 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.296533 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.296533 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.296533 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.296533 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.296533 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.296533 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.296533 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.296533 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.296533 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.827147 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.792634 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.71391 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.742944 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.870816 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.838877 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.817926 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.842618 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.874557 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.874557 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.874557 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.874557 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.874557 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.874557 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.874557 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.874557 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.874557 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.874557 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.874557 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.874557 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.874557 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.874557 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.874557 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.874557 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.874557 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.874557 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.874557 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.874557 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.874557 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.874557 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.874557 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.874557 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.874557 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.874557 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.874557 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.281761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293365 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.273654 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.287996 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27613 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.285244 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.273654 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.283982 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.273654 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281323 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.273654 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27613 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.273654 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.273654 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.273654 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.273654 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.273654 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.273654 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.273654 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.273654 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.273654 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.273654 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.273654 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.273654 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.273654 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.273654 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.273654 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.273654 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.273654 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.273654 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.542816 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.474905 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.501156 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.474768 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.559699 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.470011 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.501156 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.474768 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.584386 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470011 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.501156 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.474768 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.692753 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470011 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.501156 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.50006 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.771477 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.479863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.501156 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.504955 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.792428 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.479863 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.501156 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.504955 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.792428 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.479863 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.501156 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.504955 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.792428 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.479863 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.240653 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.251472 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.244716 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.239961 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.244298 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.244716 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.244298 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.244298 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.244298 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.244298 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.244298 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.244298 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.244298 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.244298 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.244298 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.244298 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.244298 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.244298 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.244298 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.244298 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.387555 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.383345 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.377112 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.367352 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.374395 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.365674 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.374395 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.35863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.369658 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.354651 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.365404 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.354651 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.354376 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.354651 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.350397 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.350397 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.348069 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.348069 / 0.32294238276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 38 prediction / true value for lc number 13 0.348069 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.348069 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221099 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.221099 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.214998 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.216762 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.216762 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.214998 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.214998 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.214998 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.214998 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.214998 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.316017 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.320824 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.311122 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311122 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.306027 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.306027 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.301471 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.330041154667\n",
      "validate on 30 steps, mse on train / validation data: 0.00049 / 0.00105\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.806425 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.756053 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.741435 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.735141 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.860325 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.814813 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.810539 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.829144 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.878929 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.878929 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.878929 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.878929 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.878929 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.878929 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.878929 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.878929 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.878929 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.878929 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.878929 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.878929 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.878929 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.878929 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.878929 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.878929 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.878929 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.878929 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.878929 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.878929 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.878929 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.878929 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.878929 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.878929 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.878929 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.878929 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.878929 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.33173 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.308723 / 0.275050303766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 7 prediction / true value for lc number 13 0.323158 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308723 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.311356 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.308723 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.307247 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.307247 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.307247 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.307247 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.307247 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.307247 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.307247 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.307247 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.307247 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.307247 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.307247 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.307247 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.307247 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.307247 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.307247 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.307247 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.307247 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.307247 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.307247 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.307247 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.307247 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.307247 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.307247 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.307247 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.518932 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.496154 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.47981 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.470411 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.518932 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.478753 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.478264 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.470411 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.518932 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470411 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.478264 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.470411 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.518932 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470411 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.478264 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.470411 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.518932 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.470411 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.478264 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.470411 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.518932 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.470411 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.478264 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.470411 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.518932 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.470411 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.478264 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.470411 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.518932 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.470411 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.284761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293934 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.292961 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295669 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.291778 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.301144 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.295669 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.301144 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.301144 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.301144 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.301144 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.301144 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.301144 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.301144 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.301144 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.301144 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.301144 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.301144 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.301144 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.301144 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.301144 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.301144 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.301144 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.301144 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.301144 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.301144 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.301144 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.301144 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.301144 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.301144 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.402062 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.379441 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.384904 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.374249 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.384904 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.371541 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.376876 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.366349 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.376876 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.363279 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.366349 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.359094 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.360761 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.359094 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.356575 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.351391 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.356575 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.351391 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.349468 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.344566 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.238221 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.243812 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.231052 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.238221 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.231052 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.231052 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.231052 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.231052 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.231052 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.231052 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.231052 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.231052 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.231052 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.231052 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.231052 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.231052 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.231052 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.231052 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.231052 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.231052 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.31241 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.329096 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.308723 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311356 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.308723 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221494 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.216509 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.212515 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.214149 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.209929 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.209929 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.209929 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.207869 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.207869 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.207869 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00050 / 0.00036\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 5 prediction / true value for lc number 13 0.485691 / 0.421210902518\n",
      "step nr. 6 prediction / true value for lc number 13 0.437519 / 0.407979146417\n",
      "step nr. 7 prediction / true value for lc number 13 0.424689 / 0.396952684712\n",
      "step nr. 8 prediction / true value for lc number 13 0.4422 / 0.393143543139\n",
      "step nr. 9 prediction / true value for lc number 13 0.451538 / 0.383019244234\n",
      "step nr. 10 prediction / true value for lc number 13 0.421922 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.419173 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.43516 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.424689 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.415883 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.415883 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.419563 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.409141 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.40588 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.406599 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.397296 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.397296 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.397296 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.394435 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.394435 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.394435 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.394435 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.389773 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.389773 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.386784 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.376037 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.373892 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.373892 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.373892 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.373892 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.363841 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.361281 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.361281 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.35565 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.35565 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.311133 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.311415 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.308342 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.29057 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.305116 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.305116 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.300425 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.289175 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295496 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.289611 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.28329 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.281266 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 10 prediction / true value for lc number 13 0.394435 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.391446 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.379026 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.384639 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.383023 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.376037 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.373892 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.373892 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.373892 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.373892 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.363841 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.361281 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.361281 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.35565 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.35565 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.280891 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.289175 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.277666 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.297836 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27627 / 0.252716300743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.27627 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.27627 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 20 prediction / true value for lc number 13 0.342239 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.342239 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.342239 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.342239 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.342239 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.228495 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.228228 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.227044 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.227044 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.227044 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.227044 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.227044 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.227044 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.227044 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.227044 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.227044 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.227044 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.227044 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.227044 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.227044 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.227044 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.227044 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.227044 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.227044 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.227044 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 30 prediction / true value for lc number 13 0.324573 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.324573 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.324573 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.324573 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.324573 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.324573 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.324573 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.324573 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.324573 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.324573 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.211502 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.211502 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.211502 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.211502 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.211502 / 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 35 prediction / true value for lc number 13 0.211502 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.211502 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.211502 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.211502 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.211502 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00048 / 0.00101\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04854  0.01058  0.00153  0.00081] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04451  0.0149   0.00263  0.00105]\n",
      " [ 0.06447  0.01171  0.00082  0.00036]\n",
      " [ 0.03663  0.00513  0.00113  0.00101]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04393  0.01114  0.00172  0.00049] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0508   0.01723  0.00177  0.00049]\n",
      " [ 0.04719  0.01064  0.00247  0.0005 ]\n",
      " [ 0.03379  0.00554  0.00093  0.00048]]\n",
      "results validation data \n",
      " [ 0.04854  0.01058  0.00153  0.00081]\n",
      "results training data\n",
      " [ 0.04393  0.01114  0.00172  0.00049]\n"
     ]
    }
   ],
   "source": [
    "# task 3.4 \n",
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "res = m.eval_cv('xgb_next', [configs, lcs], Y, steps=(0,[5,10,20,30]), cfg=cfg)\n",
    "print(\"results validation data \\n\", res['val_means'])  \n",
    "print(\"results training data\\n\", res['trn_means'])\n",
    "t.pickle_to_file(res, 'res_xgb_next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03983, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.04641, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03983 to 0.02769, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02769 to 0.02545, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02545 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01185 to 0.00602, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00602 to 0.00379, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00379 to 0.00338, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00352, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00338 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00322, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00257 to 0.00250, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00250 to 0.00243, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00243 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00235 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00229 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00229 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00227 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00222 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00213 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00201 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00187 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00172 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00156 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00143 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00133 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00126 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00121 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00116 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00112 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00106 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00101 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00098 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00094 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00090 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00088 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00081 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00077 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00074 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00071 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00067 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00065 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00061 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00058, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00055 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00053 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00047 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00054, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00293: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00054, did not improve\n",
      "Epoch 00306: early stopping\n",
      "Using epoch 00231 with val_loss: 0.00037\n",
      "validate on 5 steps, mse on train / validation data: 0.03678 / 0.06393\n",
      "validate on 10 steps, mse on train / validation data: 0.03618 / 0.06301\n",
      "validate on 20 steps, mse on train / validation data: 0.03513 / 0.06105\n",
      "validate on 30 steps, mse on train / validation data: 0.05573 / 0.09788\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02589, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03315, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02589 to 0.02352, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02352 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00620 to 0.00426, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00472, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00426 to 0.00334, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00334 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00201 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00142 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00120 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00108 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00097 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00089 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00078 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00075 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00070 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00061 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00056, did not improve\n",
      "Epoch 00104: early stopping\n",
      "Using epoch 00034 with val_loss: 0.00054\n",
      "validate on 5 steps, mse on train / validation data: 0.04268 / 0.03428\n",
      "validate on 10 steps, mse on train / validation data: 0.04122 / 0.03310\n",
      "validate on 20 steps, mse on train / validation data: 0.02830 / 0.02259\n",
      "validate on 30 steps, mse on train / validation data: 0.07370 / 0.05921\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train on nextstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05688, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.06073, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05688 to 0.02991, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02991 to 0.01288, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01288 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00606 to 0.00370, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00370 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00220 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00158 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00134 to 0.00117, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_loss improved from 0.00117 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00109 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00107 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00099 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00096 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00089 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00079 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00047, did not improve\n",
      "Epoch 00200: early stopping\n",
      "Using epoch 00125 with val_loss: 0.00042\n",
      "validate on 5 steps, mse on train / validation data: 0.03702 / 0.02668\n",
      "validate on 10 steps, mse on train / validation data: 0.03432 / 0.02501\n",
      "validate on 20 steps, mse on train / validation data: 0.04954 / 0.03747\n",
      "validate on 30 steps, mse on train / validation data: 0.08313 / 0.05717\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04163  0.04037  0.04037  0.07142] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.06393  0.06301  0.06105  0.09788]\n",
      " [ 0.03428  0.0331   0.02259  0.05921]\n",
      " [ 0.02668  0.02501  0.03747  0.05717]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.03882  0.03724  0.03766  0.07086] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.03678  0.03618  0.03513  0.05573]\n",
      " [ 0.04268  0.04122  0.0283   0.0737 ]\n",
      " [ 0.03702  0.03432  0.04954  0.08313]]\n",
      "mse over all validation data 0.0417128187686\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02700, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.02911, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02700 to 0.01662, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01662 to 0.01296, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01296 to 0.00999, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00999 to 0.00681, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00681 to 0.00260, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00260 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00229 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00179 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00142 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00116 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00107 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00107 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00104 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00086 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00066 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00062 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00059 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00056 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00052 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00049 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00068, did not improve\n",
      "Epoch 00112: early stopping\n",
      "Using epoch 00039 with val_loss: 0.00047\n",
      "validate on 5 steps, mse on train / validation data: 0.04494 / 0.07442\n",
      "validate on 10 steps, mse on train / validation data: 0.04512 / 0.07480\n",
      "validate on 20 steps, mse on train / validation data: 0.04242 / 0.07009\n",
      "validate on 30 steps, mse on train / validation data: 0.01981 / 0.03091\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08473, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08473 to 0.05859, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05859 to 0.03176, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03176 to 0.01476, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01476 to 0.01150, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01150 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00222 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00138 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00117 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00057 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00037 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00034 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00043, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00089: val_loss is 0.00043, did not improve\n",
      "Epoch 00089: early stopping\n",
      "Using epoch 00014 with val_loss: 0.00031\n",
      "validate on 5 steps, mse on train / validation data: 0.04378 / 0.03667\n",
      "validate on 10 steps, mse on train / validation data: 0.57215 / 0.46554\n",
      "validate on 20 steps, mse on train / validation data: 0.21907 / 0.19990\n",
      "validate on 30 steps, mse on train / validation data: 0.01453 / 0.01357\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train on nextstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20228, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20228 to 0.05469, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05469 to 0.01029, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.02629, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01029 to 0.00850, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00850 to 0.00353, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00353 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00158 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00079 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00075 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00028 to 0.00028, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00110: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00030, did not improve\n",
      "Epoch 00185: early stopping\n",
      "Using epoch 00125 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.05310 / 0.03295\n",
      "validate on 10 steps, mse on train / validation data: 0.05314 / 0.03264\n",
      "validate on 20 steps, mse on train / validation data: 0.05576 / 0.03396\n",
      "validate on 30 steps, mse on train / validation data: 0.03334 / 0.02003\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04801  0.19099  0.10132  0.0215 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.07442  0.0748   0.07009  0.03091]\n",
      " [ 0.03667  0.46554  0.1999   0.01357]\n",
      " [ 0.03295  0.03264  0.03396  0.02003]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04727  0.22347  0.10575  0.02256] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.04494  0.04512  0.04242  0.01981]\n",
      " [ 0.04378  0.57215  0.21907  0.01453]\n",
      " [ 0.0531   0.05314  0.05576  0.03334]]\n",
      "mse over all validation data 0.048112381203\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04050, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04050 to 0.01561, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01561 to 0.00609, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00609 to 0.00281, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00499, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00281 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00109 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00097 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00089 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00095, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00051: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00064 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00060 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00057 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00051 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00047 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00042 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00040, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00208: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00039, did not improve\n",
      "Epoch 00212: early stopping\n",
      "Using epoch 00137 with val_loss: 0.00038\n",
      "validate on 5 steps, mse on train / validation data: 0.02881 / 0.03179\n",
      "validate on 10 steps, mse on train / validation data: 0.01279 / 0.01417\n",
      "validate on 20 steps, mse on train / validation data: 0.00608 / 0.00506\n",
      "validate on 30 steps, mse on train / validation data: 0.00197 / 0.00161\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07372, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07372 to 0.03704, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03704 to 0.01551, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01551 to 0.00685, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00685 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00180 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00150 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00098 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00064 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00041 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00034 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00032, did not improve\n",
      "Epoch 00096: early stopping\n",
      "Using epoch 00044 with val_loss: 0.00030\n",
      "validate on 5 steps, mse on train / validation data: 0.00885 / 0.00764\n",
      "validate on 10 steps, mse on train / validation data: 0.00240 / 0.00274\n",
      "validate on 20 steps, mse on train / validation data: 0.00297 / 0.00369\n",
      "validate on 30 steps, mse on train / validation data: 0.00143 / 0.00151\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train on nextstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06087, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06087 to 0.03003, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03003 to 0.01326, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01326 to 0.00638, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00638 to 0.00320, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00320 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00128 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00069 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00056 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00050 to 0.00050, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00048 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00037 to 0.00037, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00037, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00149 with val_loss: 0.00037\n",
      "validate on 5 steps, mse on train / validation data: 0.11840 / 0.10535\n",
      "validate on 10 steps, mse on train / validation data: 0.04972 / 0.03776\n",
      "validate on 20 steps, mse on train / validation data: 0.00290 / 0.00274\n",
      "validate on 30 steps, mse on train / validation data: 0.00074 / 0.00089\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04826  0.01823  0.00383  0.00134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03179  0.01417  0.00506  0.00161]\n",
      " [ 0.00764  0.00274  0.00369  0.00151]\n",
      " [ 0.10535  0.03776  0.00274  0.00089]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.05202  0.02164  0.00398  0.00138] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02881  0.01279  0.00608  0.00197]\n",
      " [ 0.00885  0.0024   0.00297  0.00143]\n",
      " [ 0.1184   0.04972  0.0029   0.00074]]\n",
      "mse over all validation data 0.048198652182\n",
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on nextstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02917, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03013, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02917 to 0.00950, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00950 to 0.00452, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00452 to 0.00286, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00286 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00187 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00152 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00129 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00123 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00089 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00082 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00069 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00057 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00190, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00051: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00053 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00282, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00050 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00043 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00040 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00041, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00209: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00059, did not improve\n",
      "Epoch 00267: early stopping\n",
      "Using epoch 00192 with val_loss: 0.00035\n",
      "validate on 5 steps, mse on train / validation data: 1.67110 / 1.68997\n",
      "validate on 10 steps, mse on train / validation data: 1.39958 / 1.49843\n",
      "validate on 20 steps, mse on train / validation data: 0.14926 / 0.04802\n",
      "validate on 30 steps, mse on train / validation data: 0.00533 / 0.00379\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on nextstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15735, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15735 to 0.06568, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06568 to 0.01930, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01930 to 0.01436, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01436 to 0.00721, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00721 to 0.00500, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00500 to 0.00344, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00344 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00141 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00107 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00083 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00057 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00051 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00035 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00030 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00028 to 0.00024, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00042, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00024 to 0.00018, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00036, did not improve\n",
      "Epoch 00163: early stopping\n",
      "Using epoch 00088 with val_loss: 0.00018\n",
      "validate on 5 steps, mse on train / validation data: 0.30328 / 0.23316\n",
      "validate on 10 steps, mse on train / validation data: 0.18824 / 0.21108\n",
      "validate on 20 steps, mse on train / validation data: 0.02106 / 0.02609\n",
      "validate on 30 steps, mse on train / validation data: 0.00290 / 0.00335\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on nextstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17776, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17776 to 0.07077, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07077 to 0.01854, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01854 to 0.00415, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.00440, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00415 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00139 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00103 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00093 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00045 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00040, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00034 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00028 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00062, did not improve\n",
      "Epoch 00178: early stopping\n",
      "Using epoch 00103 with val_loss: 0.00026\n",
      "validate on 5 steps, mse on train / validation data: 0.78670 / 0.75063\n",
      "validate on 10 steps, mse on train / validation data: 0.96466 / 0.55230\n",
      "validate on 20 steps, mse on train / validation data: 0.06822 / 0.03191\n",
      "validate on 30 steps, mse on train / validation data: 0.00416 / 0.00345\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.89125  0.75394  0.03534  0.00353] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.68997  1.49843  0.04802  0.00379]\n",
      " [ 0.23316  0.21108  0.02609  0.00335]\n",
      " [ 0.75063  0.5523   0.03191  0.00345]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.92036  0.85082  0.07951  0.00413] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.6711   1.39958  0.14926  0.00533]\n",
      " [ 0.30328  0.18824  0.02106  0.0029 ]\n",
      " [ 0.7867   0.96466  0.06822  0.00416]]\n",
      "mse over all validation data 0.894266560734\n",
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train on finalstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06183, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06183 to 0.03056, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03056 to 0.02724, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02724 to 0.02007, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02007 to 0.01511, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01511 to 0.01443, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01443 to 0.01076, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01076 to 0.01047, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01047 to 0.01022, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01022 to 0.00984, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00998, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00984 to 0.00922, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00948, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00922 to 0.00906, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00925, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00906 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00891, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00930, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00960, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: val_loss is 0.00971, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00965, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00887 to 0.00878, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00878 to 0.00835, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00835 to 0.00816, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00816 to 0.00816, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00816 to 0.00789, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00789 to 0.00720, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00720 to 0.00683, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00683 to 0.00681, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00681 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00668 to 0.00593, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00593 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00551 to 0.00550, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00550 to 0.00516, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00516 to 0.00470, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00470 to 0.00457, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00470, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00486, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00478, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00457 to 0.00445, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00445 to 0.00412, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00412 to 0.00400, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00406, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00414, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00407, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00400 to 0.00385, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00385 to 0.00365, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00365 to 0.00358, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00358 to 0.00351, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00351 to 0.00336, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00336 to 0.00325, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00325 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00323 to 0.00321, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00321 to 0.00315, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00315 to 0.00303, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00303 to 0.00291, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00291 to 0.00282, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00282 to 0.00277, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00277 to 0.00274, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00274 to 0.00268, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00268 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00259 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00251 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00245 to 0.00241, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00241 to 0.00236, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00236 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00229 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00222 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00219 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00212 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00204 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00200 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00192 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00186 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00179 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00173 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00165 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00159 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00155 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00151 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00147 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00143 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00140 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00137 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00132 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00127 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00249, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00404, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00441, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00433, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00309, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00192, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00155: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00146, did not improve\n",
      "Epoch 00208: early stopping\n",
      "Using epoch 00133 with val_loss: 0.00125\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00129 / 0.00125\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.01069 / 0.01907\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.03512 / 0.06764\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.04695 / 0.09167\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train on finalstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12103, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12103 to 0.03794, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03794 to 0.01839, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01839 to 0.01414, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.01731, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 0.01870, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01414 to 0.00935, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00935 to 0.00627, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00627 to 0.00534, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00534 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00490 to 0.00466, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00466 to 0.00461, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00461 to 0.00351, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00351 to 0.00264, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00264 to 0.00246, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00246 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00245 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00196 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00510, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00182 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00160 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00144 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00125 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00288, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00138, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00130, did not improve\n",
      "Epoch 00123: early stopping\n",
      "Using epoch 00048 with val_loss: 0.00117\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00164 / 0.00117\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00694 / 0.00559\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00760 / 0.00632\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00847 / 0.00694\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train on finalstep considering 5 epochs, eval during training with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14638, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14638 to 0.05223, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05223 to 0.02116, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02116 to 0.01973, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01973 to 0.00650, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00650 to 0.00616, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00616 to 0.00538, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00538 to 0.00507, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00512, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00507 to 0.00468, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00468 to 0.00385, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00385 to 0.00381, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00430, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00479, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00470, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00381 to 0.00374, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00374 to 0.00308, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00368, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00376, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00308 to 0.00297, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00297 to 0.00262, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00283, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00369, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00293, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00262 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00251 to 0.00240, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00314, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00305, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00240 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00227 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00225 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00253, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00218 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00213 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00212 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00209 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00209, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00208, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00206 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00206 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00205 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00205 to 0.00204, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00204 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00201 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00200 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00197 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00196 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00193 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00192 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00181 to 0.00181, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00204: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00176 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00174 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00171 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00169 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00164 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.00162 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00155 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.00152 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00150, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00344: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00433: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00447: val_loss is 0.00169, did not improve\n",
      "Epoch 00447: early stopping\n",
      "Using epoch 00376 with val_loss: 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00063 / 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00088 / 0.00111\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00310 / 0.00315\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00609 / 0.00691\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00128  0.00859  0.02571  0.03517] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00125  0.01907  0.06764  0.09167]\n",
      " [ 0.00117  0.00559  0.00632  0.00694]\n",
      " [ 0.00141  0.00111  0.00315  0.00691]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00118  0.00617  0.01528  0.0205 ] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00129  0.01069  0.03512  0.04695]\n",
      " [ 0.00164  0.00694  0.0076   0.00847]\n",
      " [ 0.00063  0.00088  0.0031   0.00609]]\n",
      "mse over all validation data 0.00128039351097\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train on finalstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02434, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02434 to 0.01472, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01472 to 0.01133, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01133 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00699 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00556 to 0.00537, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00537 to 0.00460, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00460 to 0.00409, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00533, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00503, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00505, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00487, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00484, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00485, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00484, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_loss is 0.00472, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00466, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00444, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00436, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00409 to 0.00404, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00404 to 0.00394, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00394 to 0.00383, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00383 to 0.00372, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00372 to 0.00360, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00360 to 0.00348, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00348 to 0.00336, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00336 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00323 to 0.00311, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00311 to 0.00299, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00299 to 0.00287, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00287 to 0.00274, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00274 to 0.00263, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00263 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00251 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00239 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00227 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00216 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00204 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00193 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00181 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00170 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00159 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00149 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00138 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00129 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00121 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00115 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00110 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00288, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00106 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00103 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00103 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00098 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00089 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00076 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00082, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00162: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00090, did not improve\n",
      "Epoch 00230: early stopping\n",
      "Using epoch 00155 with val_loss: 0.00066\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.01259 / 0.01481\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00069 / 0.00066\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00162 / 0.00202\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00260 / 0.00297\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train on finalstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02652, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02652 to 0.01662, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01662 to 0.00415, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.01097, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 0.01365, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.01284, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00415 to 0.00411, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00411 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00180 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00178 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00143 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00120 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00117 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00303, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00102 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00096 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00082 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00079 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00077 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00073 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00067 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00060 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00107, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00054 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00048 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00059, did not improve\n",
      "Epoch 00201: early stopping\n",
      "Using epoch 00126 with val_loss: 0.00046\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00731 / 0.00548\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00070 / 0.00046\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00175 / 0.00193\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00316 / 0.00313\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train on finalstep considering 10 epochs, eval during training with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10276, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10276 to 0.03699, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03699 to 0.02008, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 0.02008 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00558 to 0.00425, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00425 to 0.00285, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00285 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00169 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00293, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00159 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00133 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00119 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00115 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00106 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00095 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00091 to 0.00091, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00135: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00087 to 0.00087, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00247: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00272: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00277: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00284: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00286: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00293: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00295: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00296: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00304: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00305: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00309: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00326: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00333: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00343: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00345: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00347: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00370: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00372: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.00085 to 0.00085, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00374: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00379: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00380: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00382: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00388: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00389: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00391: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00392: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00393: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00395: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00396: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00397: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00398: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00400: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00402: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00403: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00404: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00405: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00406: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00410: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00428: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00431: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00434: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00437: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00438: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00440: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00441: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00445: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00446: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00447: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00448: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00449: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00451: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00452: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00458: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00459: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00460: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00461: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00462: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00463: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00464: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00465: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00466: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00479: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00480: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00486: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00487: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00488: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00490: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00491: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00498: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00500: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00501: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00504: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00505: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00514: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00515: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00076, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00519: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00530: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00531: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00552: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00553: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00554: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00555: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00568: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00569: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00572: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00573: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00574: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00590: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00591: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00592: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00593: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00594: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00595: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00596: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00597: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00598: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00599: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00600: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00601: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00602: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00603: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00604: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00605: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00606: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00607: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00608: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00609: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00610: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00611: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00612: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00613: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00614: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00615: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00616: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00617: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00618: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00619: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00620: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00621: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00622: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00623: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00624: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00625: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00626: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00627: val_loss is 0.00080, did not improve\n",
      "Epoch 00627: early stopping\n",
      "Using epoch 00554 with val_loss: 0.00074\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.01470 / 0.01651\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00036 / 0.00074\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00100 / 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00257 / 0.00257\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01227  0.00062  0.0017   0.00289] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01481  0.00066  0.00202  0.00297]\n",
      " [ 0.00548  0.00046  0.00193  0.00313]\n",
      " [ 0.01651  0.00074  0.00115  0.00257]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01154  0.00058  0.00145  0.00278] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01259  0.00069  0.00162  0.0026 ]\n",
      " [ 0.00731  0.0007   0.00175  0.00316]\n",
      " [ 0.0147   0.00036  0.001    0.00257]]\n",
      "mse over all validation data 0.0122754472262\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train on finalstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07938, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07938 to 0.01862, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01862 to 0.01029, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01029 to 0.00370, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00370 to 0.00288, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00288 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00212 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00164, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00100 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00096 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00096 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00094 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00091 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00086 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00082 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00077 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00073 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00067 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00063 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00057 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00049 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00046 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00065, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00168: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00056, did not improve\n",
      "Epoch 00216: early stopping\n",
      "Using epoch 00143 with val_loss: 0.00042\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.02813 / 0.02763\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.01690 / 0.02214\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00041 / 0.00042\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00062 / 0.00063\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train on finalstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00965, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.04867, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.02499, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.01200, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00965 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00741 to 0.00313, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00313 to 0.00253, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00253 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00152 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00069 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00039 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00035 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00035, did not improve\n",
      "Epoch 00089: early stopping\n",
      "Using epoch 00014 with val_loss: 0.00031\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.02318 / 0.02422\n",
      "evaluate lstm with consideration of configs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00739 / 0.00588\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00054 / 0.00031\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00058 / 0.00045\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train on finalstep considering 20 epochs, eval during training with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03717, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03717 to 0.01672, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01672 to 0.01205, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01205 to 0.00682, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00682 to 0.00299, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00299 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00164 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00144 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00117 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00097 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00088 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00084 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00081 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00078 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00064 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00063 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00058 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00056, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00116: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00059, did not improve\n",
      "Epoch 00158: early stopping\n",
      "Using epoch 00092 with val_loss: 0.00055\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.03258 / 0.03222\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00885 / 0.00923\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00040 / 0.00055\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00091 / 0.00116\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02802  0.01242  0.00043  0.00075] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02763  0.02214  0.00042  0.00063]\n",
      " [ 0.02422  0.00588  0.00031  0.00045]\n",
      " [ 0.03222  0.00923  0.00055  0.00116]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02796  0.01105  0.00045  0.0007 ] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02813  0.0169   0.00041  0.00062]\n",
      " [ 0.02318  0.00739  0.00054  0.00058]\n",
      " [ 0.03258  0.00885  0.0004   0.00091]]\n",
      "mse over all validation data 0.0280209583798\n",
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03373, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03373 to 0.01372, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01372 to 0.01258, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.01419, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01258 to 0.00952, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00952 to 0.00869, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00869 to 0.00449, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00449 to 0.00369, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00863, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00449, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00400, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00430, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00402, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00373, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00369 to 0.00338, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00338 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00307, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00370, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00418, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00424, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00231 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00195 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00305, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00173 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00462, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00397, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00253, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00364, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00160 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00249, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00417, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00152 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00282, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00249, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00137, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00300, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00263, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00127 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00210, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00100 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00415, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00095 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00091 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00087 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00056 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00327, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00144, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00240: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00175, did not improve\n",
      "Epoch 00251: early stopping\n",
      "Using epoch 00176 with val_loss: 0.00052\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00171 / 0.00181\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00075 / 0.00055\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00056 / 0.00048\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00107 / 0.00081\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09866, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09866 to 0.03749, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03749 to 0.03127, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03127 to 0.01241, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01241 to 0.00890, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00890 to 0.00443, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00443 to 0.00381, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00381 to 0.00283, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00283 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00175 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00332, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00214, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00157, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00080 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00077 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00064 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00055 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00054, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00126: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00052 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00044 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00039 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00050, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00285: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00061, did not improve\n",
      "Epoch 00308: early stopping\n",
      "Using epoch 00233 with val_loss: 0.00037\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00173 / 0.00087\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00070 / 0.00055\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00042 / 0.00033\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00068 / 0.00046\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on finalstep with random nr. of epochs, eval during training with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07467, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07467 to 0.03938, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03938 to 0.03119, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03119 to 0.01042, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01042 to 0.00477, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00477 to 0.00365, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00365 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00259 to 0.00238, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00238 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00404, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00208 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00186 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00163 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00206, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00110 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00105 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00092 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00107, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00115: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00082 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00102, did not improve\n",
      "Epoch 00198: early stopping\n",
      "Using epoch 00123 with val_loss: 0.00075\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 5 steps, mse on train / validation data: 0.00165 / 0.00278\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 10 steps, mse on train / validation data: 0.00053 / 0.00098\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 20 steps, mse on train / validation data: 0.00042 / 0.00059\n",
      "evaluate lstm with consideration of configs\n",
      "evaluate lstm with consideration of configs\n",
      "validate on 30 steps, mse on train / validation data: 0.00067 / 0.00099\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00182  0.0007   0.00047  0.00076] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00181  0.00055  0.00048  0.00081]\n",
      " [ 0.00087  0.00055  0.00033  0.00046]\n",
      " [ 0.00278  0.00098  0.00059  0.00099]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0017   0.00066  0.00047  0.00081] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00171  0.00075  0.00056  0.00107]\n",
      " [ 0.00173  0.0007   0.00042  0.00068]\n",
      " [ 0.00165  0.00053  0.00042  0.00067]]\n",
      "mse over all validation data 0.00182137785973\n",
      "cross validate 0 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'gamma': 0.007833441242813044, 'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'cols_bt': 0.9376450587145334, 'maxdepth': 10, 'n_estimators': 1000}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.584156378552\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.578395060919\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.527057613488\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.500720170913\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.494855966833\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.454835391707\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.458230453509\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.465432094203\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.472633742624\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.44094650061\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.398662551686\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.417901235598\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.432510285466\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.37952675422\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.399382723702\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.371913578775\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.374074074957\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.378189303257\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.351543205756\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.330349789725\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.318209884343\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.36121398652\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.320370373902\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.34917695434\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.318004115864\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.313580245883\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.321296292323\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.32448559558\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.34002057049\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.311934159862\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.339094645447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 36 true value for curve no. 13 (example) 0.330349800763\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.306172834502\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.32294238276\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.302572014155\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.328041 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.319333 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.323845 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.307771 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.32238 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.307232 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.312327 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.302676 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.305282 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.300237 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.296431 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.295153 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.291875 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.291875 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.286506 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.286506 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.286506 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.286506 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.285248 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.285248 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.285248 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.285248 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.285248 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.285248 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.285248 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.285248 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.285248 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.285248 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.285248 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.285248 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.285248 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.285248 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.285248 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.285248 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.285248 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.829519 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.790915 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.714749 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.743645 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.874133 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.844852 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.819811 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.840803 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.874133 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.874133 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.874133 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.874133 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.874133 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.874133 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.874133 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.874133 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.874133 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.874133 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.874133 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.874133 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.874133 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.874133 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.874133 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.874133 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.874133 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.874133 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.874133 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.874133 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.874133 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.874133 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.874133 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.874133 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.874133 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.874133 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.874133 / 0.330041154667\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fa8460c89607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n\u001b[1;32m     38\u001b[0m        'n_estimators': 1000, 'subsample': 0.7946631901813815}\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xgb_next'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res_xgb_next'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL Theory/project/models.py\u001b[0m in \u001b[0;36meval_cv\u001b[0;34m(model_type, X, Y, steps, cfg, epochs, splits, lr_exp_decay, earlystop, dropout, L1L2, mode)\u001b[0m\n\u001b[1;32m    641\u001b[0m                     \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pred_xgb_stepwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mtrn_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pred_xgb_stepwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'finalstep'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "res_lstm_nextstep = []   # evaluating with next steps\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                    steps=(train_steps,[5,10,20,30]), \n",
    "                    cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "    res_lstm_nextstep.append(res)\n",
    "t.pickle_to_file(res_lstm_nextstep, 'res_lstm_nextstep')    \n",
    "\n",
    "# 3.3 train with random lenghts\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='nextstep')\n",
    "t.pickle_to_file(res, 'res_lstm_nextstep_random')\n",
    "\n",
    "# task 3.3 base line training with fixed lenghts (on final epoch)\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "res_lstm_finalstep = []   # evaluating directly on final steps\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                     steps=(train_steps,[5,10,20,30]), \n",
    "                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                     mode='finalstep')\n",
    "    res_train[i], res_val[i] = res['trn_means'], res['val_means']\n",
    "    res_lstm_finalstep.append(res)    \n",
    "t.pickle_to_file(res_lstm_finalstep, 'res_lstm_finalstep')\n",
    "\n",
    "\n",
    "# 3.3 train train using final points with random lenghts\n",
    "res = m.eval_cv('multi_lstm', [configs, lcs], Y, steps=(0,[5,10,20,30]), \n",
    "                cfg=cfg, epochs=1000, earlystop=True, mode='finalstep')\n",
    "t.pickle_to_file(res, 'res_lstm_finalstep_random')\n",
    "\n",
    "# task 3.4 \n",
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "res = m.eval_cv('xgb_next', [configs, lcs], Y, steps=(0,[5,10,20,30]), cfg=cfg)\n",
    "t.pickle_to_file(res, 'res_xgb_next')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.2378107 ,  0.34378031,  0.24267757,  0.87413275,  0.69131762,\n",
      "        0.69131762,  0.28524768,  0.47180626,  0.33312452,  0.33312452,\n",
      "        0.87413275,  0.27171901,  0.27171901,  0.28524768,  0.87413275,\n",
      "        0.27171901,  0.51169223,  0.87413275,  0.28524768,  0.69131762,\n",
      "        0.28524768,  0.87413275,  0.69131762,  0.3486748 ,  0.69131762,\n",
      "        0.87413275,  0.3486748 ,  0.47180626,  0.27171901,  0.87413275,\n",
      "        0.47180626,  0.28524768,  0.69131762,  0.87413275,  0.87413275,\n",
      "        0.87413275,  0.87413275,  0.27171901,  0.3486748 ,  0.87413275,\n",
      "        0.28524768,  0.2378107 ,  0.28524768,  0.26240456,  0.34378031,\n",
      "        0.58625638,  0.28524768,  0.87413275,  0.47180626,  0.58625638,\n",
      "        0.28524768,  0.3486748 ,  0.33312452,  0.87413275,  0.87413275,\n",
      "        0.28524768,  0.87413275,  0.87413275,  0.66242188,  0.28524768,\n",
      "        0.26240456,  0.28524768,  0.27171901,  0.3486748 ,  0.28524768,\n",
      "        0.86212778,  0.28524768,  0.27171901,  0.3486748 ,  0.28524768,\n",
      "        0.47180626,  0.39525521,  0.50287592,  0.39525521,  0.28524768,\n",
      "        0.3486748 ,  0.46063739,  0.33312452,  0.87413275,  0.34378031,\n",
      "        0.33312452,  0.87413275,  0.87413275,  0.28524768,  0.27171901,\n",
      "        0.87413275,  0.3486748 ,  0.47180626,  0.26240456,  0.34043393,\n",
      "        0.30652297,  0.30652297,  0.4729549 ,  0.46169966,  0.43106872,\n",
      "        0.69172424,  0.87920594,  0.30652297,  0.34043393,  0.34043393,\n",
      "        0.30652297,  0.34043393,  0.87920594,  0.34043393,  0.6429975 ,\n",
      "        0.63411283,  0.87920594,  0.50840509,  0.85505164,  0.3366034 ,\n",
      "        0.34043393,  0.87920594,  0.4729549 ,  0.48080721,  0.34043393,\n",
      "        0.87920594,  0.34043393,  0.87920594,  0.27173233,  0.30187929,\n",
      "        0.87920594,  0.48080721,  0.30652297,  0.34043393,  0.30652297,\n",
      "        0.34043393,  0.48080721,  0.34043393,  0.28759301,  0.30187929,\n",
      "        0.34043393,  0.65761477,  0.34043393,  0.34043393,  0.34043393,\n",
      "        0.22891951,  0.27173233,  0.30652297,  0.85505164,  0.70699394,\n",
      "        0.4729549 ,  0.34043393,  0.83518541,  0.30652297,  0.87920594,\n",
      "        0.30187929,  0.48080721,  0.85505164,  0.85505164,  0.87920594,\n",
      "        0.68283957,  0.48080721,  0.30652297,  0.30652297,  0.34043393,\n",
      "        0.30652297,  0.30652297,  0.30187929,  0.30652297,  0.28759301,\n",
      "        0.25348228,  0.30652297,  0.27173233,  0.87920594,  0.46169966,\n",
      "        0.6429975 ,  0.87920594,  0.34043393,  0.30652297,  0.30652297,\n",
      "        0.34043393,  0.30652297,  0.34043393,  0.87920594,  0.87920594,\n",
      "        0.34043393,  0.85505164,  0.44599783,  0.88002437,  0.85370159,\n",
      "        0.340974  ,  0.44599783,  0.27476174,  0.27476174,  0.340974  ,\n",
      "        0.340974  ,  0.44599783,  0.340974  ,  0.340974  ,  0.27476174,\n",
      "        0.39665261,  0.85370159,  0.25630426,  0.44599783,  0.27476174,\n",
      "        0.88002437,  0.27476174,  0.27476174,  0.44599783,  0.27476174,\n",
      "        0.27476174,  0.27476174,  0.27476174,  0.74292231,  0.63675761,\n",
      "        0.340974  ,  0.44599783,  0.85370159,  0.39665261,  0.340974  ,\n",
      "        0.88002437,  0.27476174,  0.27476174,  0.340974  ,  0.88002437,\n",
      "        0.88002437,  0.27476174,  0.39665261,  0.85370159,  0.27476174,\n",
      "        0.88002437,  0.27476174,  0.39665261,  0.88002437,  0.340974  ,\n",
      "        0.340974  ,  0.340974  ,  0.25630426,  0.88002437,  0.340974  ,\n",
      "        0.340974  ,  0.39665261,  0.88002437,  0.340974  ,  0.340974  ,\n",
      "        0.27476174,  0.27476174,  0.44599783,  0.340974  ,  0.340974  ,\n",
      "        0.69596994,  0.27476174,  0.25630426,  0.340974  ,  0.88002437,\n",
      "        0.88002437,  0.27476174,  0.27476174,  0.27476174,  0.27476174,\n",
      "        0.39665261,  0.27476174,  0.340974  ,  0.27476174,  0.39665261,\n",
      "        0.340974  ,  0.27476174,  0.88002437,  0.39665261,  0.340974  ,\n",
      "        0.340974  ,  0.340974  ,  0.27476174,  0.66377175,  0.340974  ]), array([ 0.21452314,  0.28524768,  0.2378107 ,  0.87413275,  0.58625638,\n",
      "        0.58625638,  0.2378107 ,  0.3486748 ,  0.26240456,  0.28524768,\n",
      "        0.87413275,  0.2378107 ,  0.2378107 ,  0.27171901,  0.87413275,\n",
      "        0.2566148 ,  0.47180626,  0.42806965,  0.27171901,  0.69131762,\n",
      "        0.27171901,  0.87413275,  0.58625638,  0.28524768,  0.58625638,\n",
      "        0.87413275,  0.28524768,  0.3486748 ,  0.2378107 ,  0.34378031,\n",
      "        0.47180626,  0.28524768,  0.69131762,  0.33761159,  0.87413275,\n",
      "        0.87413275,  0.87413275,  0.2378107 ,  0.28524768,  0.79496002,\n",
      "        0.26240456,  0.21452314,  0.2378107 ,  0.23412678,  0.27171901,\n",
      "        0.47180626,  0.23412678,  0.39525521,  0.39525521,  0.47180626,\n",
      "        0.2378107 ,  0.28524768,  0.27171901,  0.39525521,  0.33761159,\n",
      "        0.28524768,  0.87413275,  0.87413275,  0.58625638,  0.2378107 ,\n",
      "        0.2378107 ,  0.21452314,  0.2378107 ,  0.3486748 ,  0.23412678,\n",
      "        0.86212778,  0.28524768,  0.2378107 ,  0.33761159,  0.26240456,\n",
      "        0.39525521,  0.2566148 ,  0.27171901,  0.34378031,  0.2378107 ,\n",
      "        0.34378031,  0.3486748 ,  0.27171901,  0.87413275,  0.2378107 ,\n",
      "        0.28524768,  0.39525521,  0.87413275,  0.23412678,  0.24267757,\n",
      "        0.87413275,  0.2378107 ,  0.39525521,  0.2378107 ,  0.34043393,\n",
      "        0.28144908,  0.27173233,  0.34043393,  0.30187929,  0.34043393,\n",
      "        0.69172424,  0.87920594,  0.22891951,  0.30652297,  0.30187929,\n",
      "        0.25348228,  0.30652297,  0.4729549 ,  0.34043393,  0.56472808,\n",
      "        0.4729549 ,  0.87920594,  0.4729549 ,  0.4729549 ,  0.28144908,\n",
      "        0.34043393,  0.30652297,  0.34043393,  0.22891951,  0.25348228,\n",
      "        0.4729549 ,  0.34043393,  0.4729549 ,  0.20824772,  0.22891951,\n",
      "        0.4729549 ,  0.41754416,  0.25348228,  0.30652297,  0.25348228,\n",
      "        0.30652297,  0.22891951,  0.31035349,  0.20824772,  0.22891951,\n",
      "        0.30187929,  0.6429975 ,  0.30652297,  0.30187929,  0.30652297,\n",
      "        0.20824772,  0.20824772,  0.28759301,  0.85505164,  0.63411283,\n",
      "        0.30652297,  0.31035349,  0.34043393,  0.28144908,  0.87920594,\n",
      "        0.22891951,  0.28759301,  0.34043393,  0.78960997,  0.31035349,\n",
      "        0.60995835,  0.34043393,  0.25348228,  0.22891951,  0.34043393,\n",
      "        0.28144908,  0.22891951,  0.20824772,  0.25348228,  0.22891951,\n",
      "        0.22891951,  0.27173233,  0.20824772,  0.31035349,  0.34043393,\n",
      "        0.56472808,  0.87920594,  0.26703709,  0.28144908,  0.22891951,\n",
      "        0.30652297,  0.25348228,  0.30652297,  0.87920594,  0.87920594,\n",
      "        0.30652297,  0.85505164,  0.39665261,  0.88002437,  0.85370159,\n",
      "        0.340974  ,  0.39665261,  0.23483148,  0.25630426,  0.27476174,\n",
      "        0.25630426,  0.39665261,  0.340974  ,  0.27476174,  0.27476174,\n",
      "        0.340974  ,  0.85370159,  0.21092033,  0.39665261,  0.25630426,\n",
      "        0.55095595,  0.22004855,  0.24064258,  0.39665261,  0.25630426,\n",
      "        0.27476174,  0.25630426,  0.27476174,  0.39665261,  0.39665261,\n",
      "        0.25630426,  0.39665261,  0.85370159,  0.340974  ,  0.27476174,\n",
      "        0.39665261,  0.23483148,  0.25630426,  0.340974  ,  0.88002437,\n",
      "        0.88002437,  0.25630426,  0.39665261,  0.83307195,  0.25630426,\n",
      "        0.4125365 ,  0.25630426,  0.340974  ,  0.4192788 ,  0.27476174,\n",
      "        0.25630426,  0.27476174,  0.22786665,  0.88002437,  0.340974  ,\n",
      "        0.27476174,  0.340974  ,  0.88002437,  0.25630426,  0.27476174,\n",
      "        0.25630426,  0.25630426,  0.39665261,  0.27476174,  0.27476174,\n",
      "        0.69596994,  0.21092033,  0.25630426,  0.27476174,  0.39665261,\n",
      "        0.48313314,  0.22786665,  0.27476174,  0.25630426,  0.27476174,\n",
      "        0.39665261,  0.27476174,  0.27476174,  0.27476174,  0.340974  ,\n",
      "        0.27476174,  0.24064258,  0.340974  ,  0.340974  ,  0.25630426,\n",
      "        0.340974  ,  0.27476174,  0.25630426,  0.39665261,  0.25630426]), array([ 0.18251127,  0.27171901,  0.21452314,  0.87413275,  0.47180626,\n",
      "        0.47180626,  0.21452314,  0.34378031,  0.21452314,  0.26240456,\n",
      "        0.87413275,  0.2378107 ,  0.21452314,  0.2378107 ,  0.46063739,\n",
      "        0.20849678,  0.3486748 ,  0.28650552,  0.2378107 ,  0.69131762,\n",
      "        0.27171901,  0.87413275,  0.5162921 ,  0.28524768,  0.47180626,\n",
      "        0.3486748 ,  0.28524768,  0.3486748 ,  0.18251127,  0.27171901,\n",
      "        0.39525521,  0.2378107 ,  0.58625638,  0.28524768,  0.87413275,\n",
      "        0.87413275,  0.87413275,  0.21452314,  0.27171901,  0.39525521,\n",
      "        0.2378107 ,  0.23412678,  0.22610053,  0.21452314,  0.2378107 ,\n",
      "        0.47180626,  0.21452314,  0.33312452,  0.3486748 ,  0.39525521,\n",
      "        0.24796268,  0.26240456,  0.22610053,  0.35133439,  0.27171901,\n",
      "        0.26240456,  0.87413275,  0.74364501,  0.47180626,  0.21452314,\n",
      "        0.23412678,  0.18251127,  0.22610053,  0.28524768,  0.21452314,\n",
      "        0.86212778,  0.27171901,  0.23412678,  0.28524768,  0.2378107 ,\n",
      "        0.3486748 ,  0.20849678,  0.23412678,  0.28524768,  0.2378107 ,\n",
      "        0.28524768,  0.34378031,  0.26240456,  0.87413275,  0.21452314,\n",
      "        0.26240456,  0.3486748 ,  0.87413275,  0.18251127,  0.23412678,\n",
      "        0.87413275,  0.2378107 ,  0.3486748 ,  0.18251127,  0.30652297,\n",
      "        0.28759301,  0.20824772,  0.34043393,  0.22891951,  0.30652297,\n",
      "        0.60097754,  0.87920594,  0.20824772,  0.30187929,  0.20824772,\n",
      "        0.20824772,  0.30187929,  0.34833324,  0.30652297,  0.4729549 ,\n",
      "        0.44248027,  0.87920594,  0.34833324,  0.34833324,  0.22891951,\n",
      "        0.30652297,  0.22891951,  0.34043393,  0.20824772,  0.22891951,\n",
      "        0.30652297,  0.30652297,  0.30187929,  0.20824772,  0.20824772,\n",
      "        0.30187929,  0.34043393,  0.22891951,  0.30187929,  0.20824772,\n",
      "        0.28759301,  0.20824772,  0.28759301,  0.20824772,  0.20824772,\n",
      "        0.25348228,  0.4729549 ,  0.30187929,  0.25348228,  0.30187929,\n",
      "        0.20824772,  0.20824772,  0.25348228,  0.85505164,  0.50055277,\n",
      "        0.28759301,  0.30187929,  0.30187929,  0.25348228,  0.87920594,\n",
      "        0.20824772,  0.20824772,  0.30652297,  0.4729549 ,  0.22891951,\n",
      "        0.60097754,  0.30187929,  0.22891951,  0.25348228,  0.30652297,\n",
      "        0.20824772,  0.20824772,  0.18076879,  0.18076879,  0.22891951,\n",
      "        0.22891951,  0.22891951,  0.20824772,  0.27173233,  0.30652297,\n",
      "        0.4729549 ,  0.46169966,  0.22891951,  0.20824772,  0.20824772,\n",
      "        0.30187929,  0.27173233,  0.28144908,  0.4729549 ,  0.87920594,\n",
      "        0.30187929,  0.85505164,  0.39367509,  0.88002437,  0.85370159,\n",
      "        0.27476174,  0.39665261,  0.22004855,  0.21092033,  0.26331627,\n",
      "        0.24064258,  0.39367509,  0.27476174,  0.26750448,  0.21092033,\n",
      "        0.340974  ,  0.69596994,  0.22004855,  0.39665261,  0.25630426,\n",
      "        0.39665261,  0.21092033,  0.25630426,  0.39367509,  0.25630426,\n",
      "        0.27476174,  0.21092033,  0.23483148,  0.340974  ,  0.340974  ,\n",
      "        0.21092033,  0.340974  ,  0.69596994,  0.340974  ,  0.25630426,\n",
      "        0.340974  ,  0.21092033,  0.24064258,  0.27476174,  0.88002437,\n",
      "        0.88002437,  0.25630426,  0.340974  ,  0.69596994,  0.25630426,\n",
      "        0.340974  ,  0.21092033,  0.27824485,  0.340974  ,  0.25630426,\n",
      "        0.21092033,  0.22004855,  0.22004855,  0.52578032,  0.27476174,\n",
      "        0.27476174,  0.340974  ,  0.88002437,  0.21092033,  0.27476174,\n",
      "        0.23483148,  0.21092033,  0.39665261,  0.27476174,  0.27476174,\n",
      "        0.47459248,  0.22004855,  0.22786665,  0.27476174,  0.340974  ,\n",
      "        0.39665261,  0.21092033,  0.23483148,  0.21092033,  0.25630426,\n",
      "        0.340974  ,  0.25630426,  0.25630426,  0.21092033,  0.340974  ,\n",
      "        0.272396  ,  0.21092033,  0.27476174,  0.2770521 ,  0.21092033,\n",
      "        0.2770521 ,  0.27476174,  0.21092033,  0.39665261,  0.25630426]), array([ 0.18251127,  0.27171901,  0.20849678,  0.87413275,  0.47180626,\n",
      "        0.47180626,  0.20849678,  0.30723205,  0.20849678,  0.26046538,\n",
      "        0.87413275,  0.2378107 ,  0.18251127,  0.21452314,  0.39525521,\n",
      "        0.20849678,  0.3486748 ,  0.28524768,  0.23412678,  0.69131762,\n",
      "        0.24267757,  0.81981122,  0.50564605,  0.28524768,  0.47180626,\n",
      "        0.34378031,  0.27171901,  0.33761159,  0.18251127,  0.28524768,\n",
      "        0.39525521,  0.24267757,  0.58112532,  0.27171901,  0.87413275,\n",
      "        0.87413275,  0.87413275,  0.18251127,  0.27171901,  0.36393499,\n",
      "        0.2378107 ,  0.18251127,  0.21452314,  0.22610053,  0.25844097,\n",
      "        0.39525521,  0.18251127,  0.29643101,  0.33761159,  0.39525521,\n",
      "        0.2378107 ,  0.24309579,  0.22610053,  0.36393499,  0.27171901,\n",
      "        0.2378107 ,  0.87413275,  0.47180626,  0.47180626,  0.18251127,\n",
      "        0.21452314,  0.18251127,  0.21452314,  0.28650552,  0.18251127,\n",
      "        0.86212778,  0.26240456,  0.24267757,  0.28524768,  0.2378107 ,\n",
      "        0.34378031,  0.18251127,  0.22610053,  0.28650552,  0.23412678,\n",
      "        0.28524768,  0.30023712,  0.26240456,  0.87413275,  0.21452314,\n",
      "        0.2378107 ,  0.3486748 ,  0.87413275,  0.18251127,  0.2378107 ,\n",
      "        0.87413275,  0.20336598,  0.3486748 ,  0.21452314,  0.30652297,\n",
      "        0.28759301,  0.21230111,  0.30652297,  0.23232666,  0.30187929,\n",
      "        0.55687571,  0.87920594,  0.20824772,  0.27173233,  0.21034935,\n",
      "        0.21741953,  0.30187929,  0.30652297,  0.30652297,  0.43469834,\n",
      "        0.37299889,  0.87920594,  0.35971993,  0.36780697,  0.22891951,\n",
      "        0.30187929,  0.21034935,  0.3288061 ,  0.20824772,  0.22891951,\n",
      "        0.30652297,  0.30187929,  0.26449898,  0.18076879,  0.21034935,\n",
      "        0.28759301,  0.34043393,  0.21230111,  0.28759301,  0.20824772,\n",
      "        0.28759301,  0.20824772,  0.28759301,  0.18076879,  0.20824772,\n",
      "        0.23232666,  0.4729549 ,  0.27173233,  0.29251313,  0.27173233,\n",
      "        0.20824772,  0.20824772,  0.25348228,  0.85505164,  0.48181176,\n",
      "        0.30187929,  0.28759301,  0.30187929,  0.22891951,  0.87920594,\n",
      "        0.20824772,  0.21515107,  0.30652297,  0.43106872,  0.22891951,\n",
      "        0.50055277,  0.30652297,  0.22891951,  0.22891951,  0.32720137,\n",
      "        0.20824772,  0.18076879,  0.18076879,  0.18076879,  0.22891951,\n",
      "        0.22891951,  0.20824772,  0.20824772,  0.22891951,  0.30652297,\n",
      "        0.43469834,  0.36433673,  0.22891951,  0.20824772,  0.20824772,\n",
      "        0.30652297,  0.20824772,  0.26017734,  0.46169966,  0.87920594,\n",
      "        0.30187929,  0.85505164,  0.340974  ,  0.88002437,  0.85370159,\n",
      "        0.28964049,  0.34615681,  0.21092033,  0.21092033,  0.272396  ,\n",
      "        0.22786665,  0.340974  ,  0.27476174,  0.26987022,  0.21092033,\n",
      "        0.29611298,  0.69596994,  0.21092033,  0.340974  ,  0.25630426,\n",
      "        0.39665261,  0.21092033,  0.23483148,  0.340974  ,  0.24064258,\n",
      "        0.25630426,  0.21092033,  0.24064258,  0.340974  ,  0.340974  ,\n",
      "        0.21092033,  0.340974  ,  0.69596994,  0.29296777,  0.2770521 ,\n",
      "        0.340974  ,  0.21092033,  0.24064258,  0.27476174,  0.88002437,\n",
      "        0.88002437,  0.23483148,  0.340974  ,  0.5052458 ,  0.26750448,\n",
      "        0.28964049,  0.21092033,  0.28290874,  0.340974  ,  0.25630426,\n",
      "        0.21092033,  0.21092033,  0.21092033,  0.4192788 ,  0.27824485,\n",
      "        0.27476174,  0.28964049,  0.88002437,  0.21092033,  0.25630426,\n",
      "        0.23483148,  0.21092033,  0.39665261,  0.28171599,  0.26750448,\n",
      "        0.43457991,  0.21092033,  0.21092033,  0.25630426,  0.29611298,\n",
      "        0.40116704,  0.21092033,  0.22004855,  0.21092033,  0.25630426,\n",
      "        0.340974  ,  0.25630426,  0.26331627,  0.21092033,  0.29296777,\n",
      "        0.27476174,  0.21092033,  0.28964049,  0.340974  ,  0.21092033,\n",
      "        0.28964049,  0.27476174,  0.21092033,  0.39665261,  0.25630426])]\n"
     ]
    }
   ],
   "source": [
    "gaga = t.pickle_from_file('res_xgb_next')\n",
    "print(gaga['y_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm stepwise random input length_sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm trained on final point random input length_sct.png\n",
      "path plots/XGB stepwise until final step_sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba274bf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba2682940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba25e2080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# res_lstm_finalstep = t.pickle_from_file('res_xgb_next')\n",
    "t.box_plot(Y, (5,5), [t.pickle_from_file('res_lstm_nextstep_random')],\n",
    "            ['5 steps', '10 steps', '20 steps', '30 steps'], 'lstm stepwise random input length')\n",
    "\n",
    "t.box_plot(Y, (5,5), [t.pickle_from_file('res_lstm_finalstep_random')],\n",
    "            ['5 steps', '10 steps', '20 steps', '30 steps'], 'lstm trained on final point random input length')\n",
    "\n",
    "\n",
    "t.box_plot(Y, (5,5), [t.pickle_from_file('res_xgb_next')],\n",
    "            ['5 steps', '10 steps', '20 steps', '30 steps'], 'XGB stepwise until final step')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# t.box_plot(Y, t.pickle_from_file('res_lstm_nextstep'),\n",
    "#            ['res_xgb_next'], 'XGB stepwise until final step')\n",
    "\n",
    "\n",
    "# res_lstm_finalstep = t.pickle_from_file('res_xgb_next')\n",
    "\n",
    "\n",
    "# t.box_plot(Y, [res_mlp_500, res_mlp_es, res_mlp_do, res_mlp_l1l2],\n",
    "#            ['mlp_500', 'earlystop', 'dropout', 'l1l2'], 'means of regularisation for mlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm trained stepwise \n",
      " at input length: _sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm trained on final step \n",
      " at input length: _sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba0a003c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba07a9a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.box_plot(Y, (15,5), t.pickle_from_file('res_lstm_nextstep'),\n",
    "            ['5 steps', '10 steps', '20 steps', '30 steps'], 'lstm trained stepwise \\n at input length: ', steps = [5,10,20])\n",
    "t.box_plot(Y, (15,5), t.pickle_from_file('res_lstm_finalstep'),\n",
    "            ['5 steps', '10 steps', '20 steps', '30 steps'], 'lstm trained on final step \\n at input length: ', steps = [5,10,20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm trained stepwise \n",
      " at input length: _sct.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/.local/lib/python3.5/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path plots/lstm trained on final step \n",
      " at input length: _sct.png\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+YAAAEXCAYAAAAgMV6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xt8VPWd8PHP90wmmVwgJnInYLIPakNCF1e73YfSaugaZYsW27oQadduqAosWfssgpd0t7iP0UqFrY2oa5dI7TZRd+2yWnWhNdg+yO62tZWVEK3ackmQiyRBEpJMkvk9f8zFmRAgt5kz5+T7fr3OKzNnLud7MvOdc36/87uIMQallFJKKaWUUkrZw7I7AKWUUkoppZRSaizTgrlSSimllFJKKWUjLZgrpZRSSimllFI20oK5UkoppZRSSillIy2YK6WUUkoppZRSNtKCuVJKKaWUUkopZSMtmA+RiOwXkT+1O47hEJF7ROSf4vTejvm/iMjjIvK3dsehkouTvsP9aW4HaW6rs3HS97g/ze8gzW81ECd9h/vT3A7S3P5Iit0BuJGI5AO/B7zGmN5Res9XgX82xgw7gY0x949GLKNFRK4iuE95idyuMWZFIren3ENze3A0t5UTaX4Pjua3chrN7cHR3LafXjF3CRHRShalXEhzWyn30vxWyp00t9VwaMF8BETkj0XkVyLyoYgcFZFNoYd+HvrbJiLtIvK/ReSrIvKaiPyDiLSJyO9EZF5o/SEROSYiN59lO1XAp4FHQu/3SGi9EZG/EpF3gHdC6x4Ovd+HIvK6iHw66n3Wi8g/h27nh15/s4gcFJEPRKQy6rmWiNwlIu+JyAkReVZEcqMe/4qIHAg9FnndWeL/MxHZJyKnRKRZRO4QkUzgZWBaaJ/aRWTaubYbFfOtInJYRN4XkTtCj/lEpFNEJoTuV4pIr4iMD93/vyLyndDtrSJyX+j2BBH5cegzaRGR/yciVuixaSLynIgcF5Hfi8hfn/dLoVxBc1tzW7mX5rfmt3InzW3NbcczxugyhAXYD/xp6PZ/Al8J3c4C/iR0Ox8wQErU674K9AJ/CXiA+4CDwGYgDSgFTgFZZ9nuq8DX+q0zwE+AXCA9tO7LwIUEuymsAY4AvtBj6wk2UYmO8XtAOvCHQDdQGHr8duC/gLxQfP8I1IUemw20A58JPbYptG9/epbY3wc+HbqdA/xR6PZVQFO/555ru+GY64BMYA5wPOrz+DnwxdDtHcB7wMKox24I3d4K3Be6/QDwOOANLZ8GhGCl1evA3wGpwB8AvwOusfs7qEt8Fs1tzW27v4O6xG/R/Nb8tvs7qEt8Fs1tzW27v4OjuegV85HpAWaJyARjTLsx5r/O8/zfG2OeNMb0Ac8AM4C/N8Z0G2N2AH5g1hBjeMAY02KM6QQwxvyzMeaEMabXGLORYBJdeo7X32uM6TTG7AH2EPwhAFgBVBpjmowx3QR/PL4kwaY5XwJ+bIz5eeixvwUC59hGDzBbRMYbY1qNMb8+x3PPtd3omDuMMW8CTwJlofU/A64MPffjwHdD933AJ/ioxrR/bFOBi4wxPcaY/2eCvwyfACYaY/7eGOM3xvyO4I/l0nPErtxDc1tzW7mX5rfmt3InzW3NbUfTgvnILAcuAd4SkV+KyKLzPP9o1O1wwvZflzXEGA5F3wk1R2kUkZMi0gZkAxPO8fojUbdPR23/IuDfQk1J2oBGoA+YDEyL3q4xpgM4cY5tfBH4M+CAiPxMRP73OZ57ru2GRe/zgVA8EPwBuAr4I+BNgrWWVwJ/ArxrjBkoxm8D7wI7Qs2Y7oqKY1o4jlAs9/SLQ7mX5jaa28q1NL/R/FaupLmN5raT6cAEI2CMeQcoC/V9+ALwryJyIcFmHaO+ufOtl2C/lXXAZ4EGY0xARFoJNgEZqkNAuTHmtf4PiMj7QGHU/QyCzXQGDtCYXwKfFxEvsBp4lmCt5ED7dK7t5oduzgDeCt2eCRwO3d5NsBbyBuBnxph9IjKT4I/Pz84S2ymCTYvWiEgxUC8ivwzF8XtjzMVn2y/lXprbkfua28p1NL8j9zW/latobkfua247lF4xHwER+bKITDTGBIC20OoAwf4VAYL9H0bL0UG83ziCfUqOAyki8nfA+GFu73GgSkQuAhCRiSLy+dBj/wosEpH5IpIK/D1n+S6JSKqILBORbGNMD/AhHzWvOQpcKCLZg9xu2N+KSIaIFBHsG/QMgDHmNMH+J3/FRwm/m2AznAF/AERkkYjMEhEBThKsBQwAvwBOicidIpIuIh4RKRaRT5zn/6ZcQHNbc1u5l+a35rdyJ81tzW2n04L5yFwLNIhIO/AwsDTUL+Q0UAW8JsHmFn8yCtt6mGCfjlYR+e5ZnrMd+A/gtwSbknTRr0nNELf3PMGmJKcIDvzwSQBjTAPBJKslOIBEK9B0jvf6CrBfRD4kmIzLQu/zFsEBI34X+j9NO9d2o/yMYDOXV4CHQv2Aoh/zEkzg8P1xDNyPBeBi4KcEB834T+BRY8zOUH+jRcBcgnNffgD8E8EmSMr9NLc1t5V7aX5rfit30tzW3HY0CfanVyr5SbDJzO8BrzGm195olFKjRXNbKffS/FbKnTS3R59eMVdKKaWUUkoppWykBXOllFJKKaWUUspG2pRdKaWUUkoppZSykV4xV0oppZRSSimlbKQFcwcSkQYRucruOABExIjILBu2e5WInGvESaWSnuay5rJyL81vzW+VnDQ3kz83ReSrIrLL7jgSTQvmSUZE1ovIP5/rOcaYImPMqwmIZauI3Bfv7QzGaP9whfbNLyLtUYtntN5fKc3lgcUhl/9cRHaLyGkReXWAx+eKyOuhx18XkbmjtW01dml+DywO+f2QiLwjIqdE5C0R+Yt+j2t+qxiamwOzqwJADY0WzNVYtsEYkxW19NkdkFJqyFqA7wDf6v+AiKQC/w78M5ADfB/499B6pVTy6wCuIzhX8c3AwyIyDzS/lUoWEqRlytFgjNElwQvwMHAI+BB4Hfh0aP21gB/oAdqBPWd5/X7gT0O31wPPAk8Bp4AG4Ip+z70b2Ae0Ak8CvtBjXwV29XtvA8wCbg3F4Q/F8sJZYjHArNDtNOAh4CBwFHgcSA89dhXQBKwBjgHvA38Z9T4XAi+E/ie/BO4Lxwb8PLSdjlAsS873foP4DLYC99n9XdDF2Yvmsv25HLXdrwGv9ltXCjQTGug0tO4gcK3d3x1dkn/R/E6e/I7a/vPAmtBtze8xumhu2p+bwKtAFfAa0Bna578EGkP/x98Bt0U9fzDxPx+K/xfA/43+3wLzQvt1MvR3Xr9Y7gN2h//Xoff7YdT/I9/u7+1gFq3dsMcvgblALlAL/IuI+Iwx/wHcDzxjgldw/3CQ73c98DRwAcEv9SP9Hl8GXAP8L+AS4Bvne0NjzBMEv9Dhq8rXDSKOb4Xefy7BBJ0O/F3U41MI1npPB5YDm0UkJ/TYZoI/GFMI1orfHBXLZ0I3/zAUyzPnez8RuUlE/uc88a4SkZZQ87cvDmL/lOpPczk5cvlsioD/MaEjd8j/hNYrdT6a30mU3yKSDnyCYMEJNL/HMs3N5MjNrxCsgBgHHCBY4F4EjCdYSP8HEfmjIcTfBUwFykMLoVhygReB7xIscG8CXhSRC6Pee2konukEP6f/JFiJkkuwsuCb59mXpKAFcxsYY/7ZGHPCGNNrjNlIsIbs0hG85S5jzEsm2BT7B0D/H6JHjDGHjDEtBGu3ykawrQGJiBBMzv9jjGkxxpwi+OO4NOppPcDfG2N6jDEvEazVujTUt/uLwDeNMaeNMfsINkk7nwHfD8AYU2uM+fg5Xvtd4GJgEvC3wFYR+dRQ9lkpzeWkyOVzySJYux7tJMGTCKXOSfM76fL7cWAPsD10X/N7jNLcTJrc3GqMaQh9Dj3GmBeNMe+ZoJ8BO4BPDyH+vzPGdBhj9vaL/3PAO8aYH4S2VQe8RbCbS9iToW2fBF4G3jPG/NQY0wv8C3DZIP4ftkuxO4CxSETuIFhTNI1g05LxwIQRvOWRqNunAZ+IpIS+jBBs7hN2ILTd0TYRyABeD/62ACBA9IBqJ6JiCseaFXptSr84o2+fzdne77yMMb+OuvuSiPwQ+ALBJjlKDYrmckystuTyebQT/EyijSfYzE6pc9L8jonV1vwWkW8DxUBJ1BVyze8xSnMzJlY7czNmGyKykOCV6UsIXvzNAN4cxPYGiv9A1O1p/e6HH58edf9o1O3OAe6PxjlF3OkV8wQTkU8D64A/B3KMMRcQrOENZ6E522tHYEbU7ZnA4dDtDoJJE45tSr/XDSWWDwh+8YuMMReElmxjzGAS4TjQC+SdJeZEMHz0GSh1XprLA0qGXI7WAHxcos5ygI/zUVNYpQak+T0gW/JbRO4FFgKlxpgPox7S/B6DNDcHZNexN7J/IpIGPEewj/zk0OfyEoM7tw7H3///HHYYuKjfa2YSHGPCVbRgnnjjCH75jgMpIvJ3xNb4HgXyR3l0w78SkbxQH41KINy3ZA9QFJpuxEdwAIxoR4E/GMwGjDEB4HsE+5NMAhCR6SJyzSBe2wf8CFgvIhki8jHgL/o9bdCxDIaIfElEskTEEpFS4MsE+xUpNViay2e+1o5c9oT2OQWwRMQnIt7Qw68CfcBfi0iaiKwOra8fre0r19L8PvO1duT33cBNBAfqOtHv4VfR/B6LNDfPfG3Cc3MAqQS7FBwHekNXz0sH88IB4p9NVB95ggX8S0L93lNEZAkwG/jxqO5BEtCCeeJtB/4D+C3BZhhdxDbd+JfQ3xMi8mtGRy3Bfh6/A94jOHIhxpjfAn8P/BR4B9jV73VbgNki0iYi2waxnTuBd4H/EpEPQ+872D4/qwkOCHGEYP+eOqA76vH1wPdDsfz5+d5MRJaJyLlqzW8nWNPWBnwbuMUkYE5L5SqaywNLdC5/heBVhscI9mXrJHhygzHGDywmeILSRnAwmcWh9Uqdi+b3wBKd3/cTvDL2roi0h5Z7QPN7DNPcHFiiczNGqE/8XxMc4b6VYIXaUC54rSbY3PwIwZmTnox67xMEB5VbA5wg2GJikTHmgyG8vyOIMfFo8aGShYjsB75mjPmp3bEMhYg8CEwxxtx83icrNQZoLivlXprfSiUnzU2VSHrFXCUFEfmYiHxcgv6Y4KAe/2Z3XEqpodFcVsq9NL+VSk6am+6QsFHZRSQTeBTwA68aY36YqG0rRxhHsNnNNIL9YDYC/25rRGrQNL9VFM1lF9HcVv1ofruE5rbraG66wIiasotIDcE2/8eMMcVR668FHiY4xP8/GWO+JSJfAdqMMS+IyDPGmCUjjF0pFUea30q5k+a2Uu6kua2Us420KftW4NroFRKcJH4zwaktZgNlodH18vhocIa+EW5XKRV/W9H8VsqNtqK5rZQbbUVzWynHGlFTdmPMz0Ukv9/qPwbeNcb8DkBEngY+DzQR/BF4g3NUCIjIrcCtAJmZmZd/7GMfG0mISo05r7/++gfGmIkjfZ/Rzm/NbaVGbjTy2+nH7paWFt5//326urrw+XxMnTqV3NzcuG1PqUTQ3FbKvQab3/HoYz6d2GkLmoBPAt8FHhGRzwEvnO3FxpgngCcArrjiCvOrX/0qDiEq5V4iciCObz/s/NbcVmrk4pjfjjh219XVUVlZyUsvvcT8+fPZtWsXy5cvZ82aNZSVlcVlm0olwljPbaXcbLD5nbDB34wxHcBfDua5InIdcN2sWbPiG5RSCfDxj3+cN998M3J/zpw5/M///I+NEY2+wea35rZSzpJsx+6qqiq2bNlCSUkJACUlJWzZsoWKigotmCs1BInM7UffeJTH9jwWuf/0oqcBWPrjpZF1K/9wJavmrmLBsws43nkcgMLcQp697lnW717Pc+88F3nuKze+wr4T+6ior4h5rVJOF4+CeTMwI+p+XmjdoBljXgBeuOKKK24ZzcCUSrT+hXKAN998k49//ONOLZyPKL81t5VKWo44djc2NjJ//vyYdfPnz6exsTFem1TK6WzP7VVzVw1YcH7z5tjzo7q6Oo5VHaOxsZHCwkJuqLwBgPXz1rN+3vqY507KmHTG65VyunjMY/5L4GIRKRCRVGAp8HwctqNU0utfKD/fegfQ/FbKnRyR24WFhdx7770UFxfj8XgoLi7m3nvvpbCw0O7QhqWuri5mX+rq6uwOSbmPI3I73E2lurqarq4uqqurqays1JxQY8qICuYiUgf8J3CpiDSJyHJjTC+wGtgONALPGmMahvi+14nIEydPnhxJeEolDRGJ+esE8chvzW2l7OfkY3dJSQkPPvgg5eXlnDp1ivLych588MFI03Yn0YKIGm1Ozu3obiperzfSTaWqqipu21Qq2YxoHvN400EmlNOdqyAer9wTkdeNMVfE5c1Hiea2UsMz1vO7uLiYiy++mJdffpnu7m7S0tJYuHAh77zzDnv37o3LNuOluLiY6urqmEqFnTt3UlFR4bh9USM31nPb4/HQ1dWF1+uNrOvp6cHn89HXl1yzucWzz7z2l3enweZ3wgZ/GwodIEopd9LcVsq9EpHf+/bto6Ojg5dffjkyKnt5eTkHDsRzMor40P7yyikSkduFhYXs2rUrpqJq165dSdlNZSz2ma+rq6OqqiqyL5WVlTrgZhzEo4/5iBljXjDG3JqdnW13KEqpUaS5rZR7JSK/U1NTqaioiGnuWlFRQWpqaty2GS/hgki0ZC2IqLEtEbldWVnJ8uXL2blzJz09PezcuZPly5dTWVkZt23Gk5u6qrhpX5KeMSZpl8svv9wo5WTAWZc4bvNXJgny91xLvHO7tLTUiIgBjIiY0tLSuG4v3ubMmRPz3ZkzZ47dIQ1bbW2tKSoqMpZlmaKiIlNbW2t3SI4y1vNbRMzEiRNNfn6+ERGTn59vJk6caEQkbtuMl9raWlNQUGDq6+uN3+839fX1pqCgQHNijBrruW2Mu44PRUVFpr6+PmZdfX29KSoqsimi4XPTvthlsPmdlFfMEzVA1DXXXINlWYgIlmVxzTXXxHV78VRRUYHP50NE8Pl8VFRU2B3SiOhIte6UiNy+5ppr2LFjB8HfwWDl444dOxyb3+eacs9p6urqWLZsGQ0NDQQCARoaGli2bJnmt0skIr+nT59OR0cHzc3NGGNobm6mo6OD6dOnx22b8VJWVkZVVVXk+F1RUUFVVZU2D1VJRwduHTo3dVVx074kvcGU3u1a4lkzV1paOuBVTCdeWVu9evWA+7J69Wq7QxuW2tpa4/P5YvbF5/M5suY0HL9lWTF/0Svm8dz/hLdSiCc37U/09z96sSzL7tAcY6znd25urvF4PGbjxo2mo6PDbNy40Xg8HpObmxu3bSqVCGM9t93WgsRNV5mLiopMZWVlTGuG8H01OIPN76S8Yp4IO3bsGNL6ZLZ582YALMuK+Rte7zS33HILXV1dMeu6urq45ZZbbIpo+EpLSwEIBAIxf8PrlRqsoqIiDhw4QFFRkd2hDFv4+z/Y9Ur119LSwtq1a6mpqWHcuHHU1NSwdu1aWlpa7A5NKTUCbpsurbKykiVLllBQUIDH46GgoIAlS5Y4ss+8m6apTHZJWTBPZJOZnJwcLMsiJycn7tuKl2BFzJlzZYfXO01HRwdwZkVDeL2TbN++ndLS0pjPprS0lO3bt9scmT0SmdsbN26ko6ODjRs3xn1b8ZaRkcHevXuZOXMme/fuJSMjw+6QRmTevHkcPnyYefPm2R2KGkWJyu8FCxawd+9e+vr62Lt3LwsWLIjr9pQa6xKR225uLu3U8/GwnTt3smjRIu655x4yMzO55557WLRoETt37rQ7NNdJyoK5SdDIzR6Ph/A2srOz8Xg8cd1evE2cOBHLspg4caLdoYyKSZMmYVkWkyZNsjuUEdm+fTuBQABjDIFAYMwWyiGxo7K/++679PT08O6778Z9W/F2+vRpPv/5z/PBBx/w+c9/ntOnT9sd0ojcd999TJgwgfvuu8/uUNQoSkR+5+XlceONN1JQUIBlWRQUFHDjjTeSl5cXt20qNdYlIrfdNktBVVUVzzzzDL///e8JBAL8/ve/55lnnnFkC4B9+/bxxhtv8PLLL+P3+3n55Zd544032Ldvn92huU5SFswTpa+vjxMnTgBw4sQJ+vr6bI5oZG644QZaWlq44YYb7A5lVKxdu5ZTp06xdu1au0NRDvTYY49xwQUX8Nhjj9kdyqh4/vnnmThxIs8//7zdoYzYtddeS1paGtdee63doSiHWbx4MadOnaKzsxOAzs5OTp06xeLFi22OTCk1Em6bLs1NLQBSU1P51Kc+FTNQ5ac+9SlHTlOZ7MZ0wRzg1KlTBAIBTp06ZXcoI+a2gsh9991HZmamXlVTY1pubu6Q1juB3+/HGIPf77c7FOUwO3fu5Prrr6etrQ1jDG1tbVx//fWObVKpM5AoFeS2WQrc1AKgu7ubZ555JqaP+TPPPEN3d7fdobnOmC+Yq+TV2toa81epwTjbwHpOHXDvxIkTZxTCc3NzI619nGTGjBnAmeNhhNcrdT779u1jz549MU0q9+zZ48gmlXV1dVRWVlJdXU1XVxfV1dVUVlZq4VwpF3BTC4C0tDSWLFkSM+jmkiVLSEtLszs010mxO4CBiMh1wHWzZs2yOxRlg8zMzAEHesvMzLQhGjWaEpHb+/btIz09nd7eXnp6evB6vaSkpDjyxD3MiYXwgRw8eJCZM2dy6NAhIDggzowZMzh48KDNkanRkIj8Tk1NZd68eVRUVNDY2EhhYWFkMEGniR6FGoiMQl1RUeHYq4TKnRKR2+GKqi1btjB//nx27drF8uXLARyZD+GYo3+rnNoCwO/389prr1FTUxP5bMrLy7XVWxwk5RXzRA0Q5fV6yc/PR0TIz8/H6/XGdXvx1n8Uc6fKyck5o99Kamqqo0fOV0GJyO2mpia+/vWvc8kll2BZFpdccglf//rXaWpqits21eAdPHgwZs5OpxfKw80uRSTS/HKsSkR++/1+nn766ZgmlU8//bQjTxAbGxtpamqKacre1NTkyD6oyt0Skdtumy4NgoXz6BkknFgoB5g9ezbLli2L6WawbNkyZs+ebXdoruPsEtwIpaSk0NzcjDGG5uZmUlKSsgHBoFx22WUUFhZiWRaFhYVcdtlldoc0bM3NzWdULliWRXNzs00RKad58sknY5qHPvnkk3aHpFyooqKCzZs309vbC0Bvby+bN28e04XzeEtNTWXp0qUxTSqXLl3qyEGIpk2bxrp162J+q9atW8e0adPsDk2phHPTYGluU1lZSW1tbcxvVW1trSOb5Se7MV0w7+rqIjc3FxEhNzeXrq4uu0Matj179sRcQdizZ4/dIQ2biOD3+2Pmofb7/ZH+qEqdS0pKyhlXz/x+v6Mr3lRyCg+0uWHDBjo6OtiwYUPMejX6/H4/O3bsoKOjA2MMHR0d7Nixw5FXzIEzjmt6nFNjlZsGS3Mbtw3Ml8zG7JlquB9z+GAeHiXYif2YwwWONWvWsGbNmsg6pzZpDwQCZGZmUl1dzR133MFFF11Eenr6gP3Oleqvr68Py7IoLy+P9Gm2LMvx0yGq5NPX18cDDzzA3/zN3wDwN3/zN/j9fu6++26bI3Ov6dOn09LSEhmVvbm5Ga/Xy/Tp0+0ObcgOHz7M1q1bY/qgPvjgg3z1q1+1OzSlEi48WFr/PuZObsruJmVlZVoQT4CkLLmJyHUi8sTJkyfjto3Ozk6Ki4tpbW3FGENrayvFxcWRuVGdZMGCBfT29kb6YOfk5NDb28uCBQtsjmz4/H4/+/fvxxjD/v37HXs1RMVKRG7Pnj2b2267LVLJlpmZyW233aZ9oZSKs0Tk9+nTp+nu7uZb3/oWHR0dfOtb36K7u5vTp0/HbZvxUlhYSF5eXkwf1Ly8PL1CqJJOInK7rKyMz33ucyxcuJDU1FQWLlzI5z73OS0MqjElKQvmiRhkYtq0aRw6dIj8/HwsyyI/P59Dhw45sm9Xc3MzixcvjpyYnD59msWLFzu6T3ZPTw8rV66kra2NlStX0tPTY3dIahQkIre1L5RKFMuyqKysZNOmTZw+fZpNmzZRWVnp2NZKI5WI/G5paWHt2rUxfczXrl1LS0tL3LYZL26aTkm5WyJyu66ujhdffDFmKsQXX3xRpw9MEnV1dTEDVernEh9j8+yBYOG1vb2diooKTp06RUVFBe3t7Y6sdW9sbOQLX/gCs2bNwrIsZs2axRe+8AVHD5ghIvzoRz8iJyeHH/3oR47ud6c/ZomlfaFUoqxatQpjDHfeeSeZmZnceeedGGNYtWqV3aEpB9DfKqU+4sZR2d2irq6O22+/PWZsj9tvv13PZ+MhetqaZFsuv/xyEy+Auf76601aWpoBTFpamrn++utN8F/iLHl5eWbKlCmmvr7e+P1+U19fb6ZMmWLy8vLsDm1YAHPZZZcZETGAERFz2WWXOfKzqa2tNQUFBTGfTUFBgamtrY3bNoFfmSTI33Mt8cxtpRJpzpw5Bogsc+bMiev2xnp+5+bmGsuyzMaNG01HR4fZuHGjsSzL5Obmxm2bSiXCWM9ty7LMU089ZYqKioxlWaaoqMg89dRTxrKsuG1TDU5eXp7Jzs42+fn5RkRMfn6+yc7Odmw5ww6Dze8xe8Uc4Be/+EVMk5lf/OIXdoc0bF1dXZSXl+Pz+SgvL3f0CPOWZfGb3/wmZl726PtOojXASrlXRUUFDQ0NTJkyBcuymDJlCg0NDTpdWhxlZGSQkpLCmjVryMzMZM2aNaSkpJCRkWF3aMOiLaqUCtLpA5NXU1MTPp+Pmpoauru7qampwefz0dTUZHdoruO8ks4oSUlJobu7O2Zdd3e3I6dUCo9KC8EWEABer9exfczD+5KRkYFlWZETrvB6J9F5Oe3htpNdt+2PWzz++OOkp6fj8/kA8Pl8pKen8/jjj9scmXs1NTXR29sbUxnS29vryBPEuro6KisrYwoilZWVmt9qzNLpA5PXmjVrYi4yhWeBUqNrzBbM+/r6SElJoby8nLS0NMrLy0lJSXHklEqpqamUlpaSmZmJiJCZmUlpaSmpqal2hzYs3d3dzJs3D7/fTyAQwO/3M2/evDMqUpzTMOg2AAAgAElEQVRA5+VMPLed7Lpxf9xSydDb20tPTw/79+8nEAiwf/9+enp66O3ttTs01xIRCgsLaW1tJRAI0NraSmFhoSNP4LVFlVIfOXz4MA8++GDMmAsPPvgghw8ftjs0BWzatClmoMpNmzbZHZIrjdmC+ezZs7n11ltjCrO33nqrI6dU8vv9PP3005SXl3Pq1CnKy8t5+umnHT3F2De+8Q26urowxtDV1cU3vvENu0MaFh11N/Gqqqq46aabYg7uN910k2NPdt108h49gAzgigFk+v/OOvl31wmMMTQ0NERaUmVkZNDQ0BBpLeYk2qJKqY8UFhby9ttvx6x7++239UJGEsjLyxuwy2xeXp7doblOUhbMEzFfopumVEpNTWXp0qUx08csXbrUsVfMU1JS+PKXvxxTmP3yl7/syG4GOi9nrETk9r59+wbM7X379sVtm/HU2NhIU1NTzFXmpqYmR568r1u3jpSUFGpqaujq6qKmpoaUlBTWrVtnd2gjEt2UfSxLRH5DsFtTdnY2IkJ2drYjuzlBsCBy7733xuT2vffeqwURlXQSkdslJSU8+OCDMReZHnzwQUpKSuK2zXhzSwuxDRs2nPE76/V62bBhg00RudhgRoiza4n3yM21tbUxoz/Gc6TseBKRAUf+FhG7QxuW1atXG8uyzOTJk42ImMmTJxvLsszq1avtDm3IdFT2xOd2Wlqa2bhxY8y6jRs3mrS0tLhtc7g2/2azKd5aHFn2frDX7P1gb8y6WX8xy0ydOtXM+8G8yLqPVX3M5OXlmW++9s2Y5x7tOGp2HtxpircWm82/2Wz37p0BMDt27IhZt2PHDkfOuGCMiRmNvf8Sx22O6fwGjMfjiRmV3ePxOPI7tHr1apOSkhKzLykpKY481qmRG+u5XVRUZCorK2POy8P3nciO8794ckuZyS6DzW/bk/xci06pNDhu+zEzJnjCEj2VnVNPVIqKikx9fX3Muvr6+rh+NmP94B6eyiP6YBie4sOJ3DQdolsL5jk5OTF/tWAe34L5lClTYipBwvedxo3HbjV8Yz23Lcsyfr8/Zp3f73fsdGl2nP+p5KUF8zHEbbVybmLHgWasH9zddrLrprld3VTJYIyJKZiLiBbME5Df4QpbEYn5m4wtYs7HbQURNTJjPbfdVpDV/FbRBpvfSdnHXA1NWVkZVVVVMYNdVVVVjdl+zMlER2VPPDeNHwHB71BeXh579+6lr6+PvXv3kpeX58jv0IYNG+jr64uZDaOvr8/x/dRaW1sxxtDa2mp3KK4XHlwveJ7z0V8nDrqnxwelPuK2wXI1v9VwOG80LTWgsrIyLYgnocrKSpYsWUJmZiYHDx5k5syZdHR08PDDD9sdmmuVlZWxdetWPvvZz2KMQUS4+uqrHZsflZWVLF68mM7OTnp6evB6vY6dKzv8GVRVVUVmw7j//vsd+9mkpKRgjImZZtPj8Thy6i6nMMZgWRaBQCCyrv99p4g+Phw4cICLLrpIjw9qzAofByoqKmhsbKSwsNDRF5nCFQ1btmxh/vz57Nq1i+XLlztyRhWVOHrFXKkECV/ZUfFVUVFBfX09Dz30EB0dHTz00EPU19dTUVFhd2jDsnv3btrb27nwwguxLIsLL7yQ9vZ2du/ebXdow7J7927effddAoEA7777rmP3A2DFihUYY5g8eTIAkydPxhjDihUrbI7M3QKBQMz/3ImF8v60MkepYOE8unWYUwvloLPyqGEaTHt3uxbtY66cTgd/s6cPqlNGZR8MN+2PG0ehLi0tjenvXFpaGtftjfX8JtSHf+XKlaatrc2sXLky7v3648VtfWrVyIz13DbGXSN/6/hPKtpg89v2JD/XogVz5XQ6+Js9J+4dHR0x6zo6Ohx54m6Mu/bHTZUMxuh0iGdbElEwD0+RFv7rxHzQwaFUtLGe27W1tWbixImRWVTy8/PNxIkTHVuQ1Yo3FW2w+T2mm7LX1dVRXFyMx+OhuLiYuro6u0MaNjfti5sUFhZy7733xnw29957rw7+EUdpaWln9L9+/PHHSUtLsymikXHT/nR3d5/RzHvFihV0d3fbFNHIVFVVcdNNN8UMvHnTTTdpH8JRJiKRJSzcrz+6f3//5yQ7HRxKqY+sW7eO9vZ2mpubMcbQ3NxMe3s769atszu0YWlsbGT+/Pkx6+bPn09jY6NNESlHGEzpfTQW4A+ALcC/DvY18a6Zc0sTEzftS5hb5jFfvXq1sSzLTJ482YiImTx5srEsK677gw217kPN73jmdvh/PmXKlJi/Tv4OuaX5t9uumIvIgL+9IhK3bSY6v5Pt2J2bmxv5TQUiv6m5ublx22a8RF8htCzL8VcI1ciM9WM3YCzLijnWWZblyNYwxugVcxVrsPk92MStAY4Be/utvxZ4G3gXuGuQ75UUB3c3zXXstuR3U0EkLy/PZGdnxzTNys7Ojuu8zUM9uNuR3/GudBs/frzxer0GMF6v14wfP97RJ7uJ7sccL26rNElLSzPLli2LOY4sW7YsrhUNQ8lvNx67a2trzbhx42Lye9y4cY7Mb7c13VUjM9aP3YD57Gc/G/N7+tnPftaxBXM3/VapkRvtgvlngD+KTn7AA7wXqm1LBfYAs4E5wI/7LZOiXpcUB3c7rnTEi2VZ5qmnnor5MXvqqacc20/NjpPdeAHMjh07Ytbt2LEjrgeaYRzcE57fWuk2eG46eXdbpYmIGBGJqWgIr4uXIRbMXXfsNuajAaIARw8Q5bZKdTUyY/3YTWjMiOiLMuExJJxo9erVRkRixsMQEcdWRLtpYD47jGrBPPh+5PdL/v8NbI+6fzdw9yDe55zJD9wK/Ar41cyZM+P2D3JTk8q8vDwzderUmEqGqVOnxvWqbDwBJj8/P2Z/8vPzHfnj7ISCuUlQficqt0XEZGVlxRT+srKyHFnpZkwwv6dMmRKTD1OmTEnK/N78m82meGtxZNn7wV6z94O9Meu+uPGLpqioyFz6nUsj64q+FSyIfPO1b8Y892jHUbPz4E5TvLXYbP7NZpv37kwpKSkmLS0t5ruWlpZmUlJS4rbNYZy8u+rYHc2Jx4RoOvibijbWj92ASUlJifk9TUlJcWyep6SkmNzc3Jhjd25ublyPD/Hixi6ziZaIgvmXgH+Kuv8V4JFzvP5C4PFQTd55fySMif8V84EKf048eXfSiftgiIhZuXJlzLqVK1c69rNJdKXJKB3c45rf8cxtj8czYD81j8cTt23GE2DuuuuumJrqu+66y5EnKyJiJkyYENOndsKECY7MbWOMLX0iR6Fg7uhjdzQn5kA0t7XuUSMz1o/dhGZXyMnJifnr1DwHzEsvvRSz7qWXXnLk/mjrnpEbbH4nbFR2Y8wJY8wKY8z/MsY8cK7nish1IvLEyZMn4xbP7NmzmTt3LgsXLiQ1NZWFCxcyd+5cZs+eHbdtxsvhw4e54YYbYvblhhtu4PDhw3aHNizGGL73ve+xadMmTp8+zaZNm/je974XPog4yoYNG+jt7aW8vByfz0d5eTm9vb1s2LDB7tBG1WDzOxG53dfXh2VZrFmzhszMTNasWYNlWTGjNzvNt7/9bRoaGggEAjQ0NPDtb3/b7pCGxePx0NnZGbOus7MTj8djU0Qj98lPfpJ77rmHzMxM7rnnHj75yU/aHdKoSrZjt5uUlJTwwAMP8MEHHxAIBPjggw944IEHKCkpsTs0NUYk07E7LS2NKVOm0NraCkBraytTpkxx5AwkYT/4wQ9iZuX5wQ9+YHdIw9LY2EhTU1PMvjQ1NekI83EwkoJ5MzAj6n5eaN2IGWNeMMbcmp2dPRpvN6CSkhKef/55cnJysCyLnJwcnn/+eUceEKdNm8a2bdt4+eWX8fv9vPzyy2zbto1p06bZHdqwFBUVMXfuXO644w4yMzO54447mDt3LkVFRXaHNmRlZWU8/PDDZGZmApCZmcnDDz9MWVmZzZGdV1zyOxG5DdDb2xuT2729vXHdXrz19fUxb948Dh8+zLx58xxbydDb20tHRwf79+8nEAiwf/9+Ojo6HP35/Pd//zf3338/HR0d3H///fz3f/+33SGdj6OP3W6ybds2PB4PR48eBeDo0aN4PB62bdtmc2TKwRx77O7u7ubIkSOsXLmStrY2Vq5cyZEjRxw7nWZmZiZ1dXV85jOfoaWlhc985jPU1dVFzgedZNq0adx5551UV1fT1dVFdXU1d955p2PLGclsJAXzXwIXi0iBiKQCS4HnRyOoRNTMbdu2jfHjx+Pz+TDG4PP5GD9+vGMPiP2vJjvx6nJYSUkJb7zxBg899BAdHR089NBDvPHGG46sNIFg4Xzv3r309fWxd+9eJxTKIU75nagrapZl0d7eTiAQoL29HctKWOOguBARdu/ezbRp09i9e7ej5moeSHSliZOlpKSQkZFBdXU1WVlZVFdXk5GRQUpKit2hnYujj91u0tTURG9vL5MnTwZg8uTJ9Pb20tTUZHNkysEce+wWEYqKiqipqeGCCy6gpqaGoqIixx7vcnJy8Hg8PPbYY1xwwQU89thjeDwexx73Tp8+HdP68/Tp03aH5EqDOlsVkTrgP4FLRaRJRJYbY3qB1cB2oBF41hjTMBpBJaJmrqmpiRUrVpCZmYmIkJmZyYoVKxx5QDx8+DAbNmygoqICn89HRUUFGzZscGxT9p07d7Jo0aKY5qGLFi1i586ddoc2aCIypMXmWBOW34m6ohYIBCLN39LS0ggEAnHdXrwZY5gyZQqWZTFlyhRHV7y5qdKkr6+P3t5e9u/fjzGG/fv309vbmzQtGtx47Hab1NRU0tPTsSyL9PR0UlNT7Q5JOYTbjt3GGN56662YFkhvvfWWY493TU1NGGOYPHkyIsLkyZMxxjiynNHc3Bz5bQp/HqmpqTQ3j0pjKxVlUGdExpgyY8xUY4zXGJNnjNkSWv+SMeaSUN+UqviGOvqefPLJmGYZTz75pN0hDUthYSFvv/12zLq3336bwsJCmyIamX379rFnz56Ypvl79uxh3759dod2hkffeJQ5358TWRpONNBwooHircWRZfNvNmOM4dJ/uDSy7sbnb8QYwzdf+ybFW4sjrz92+hivHnqVOd+fw6NvPJqQfXBjfmdnZ9PR0QFAR0cHTi8oeL1eamtr6erqora2Fq/Xa3dIwxYIBMjKykJEyMrKcnSlSU5ODn6/P+bEy+/3J80VETfmttv4/X46OzsxxtDZ2Ynf77c7pBGpq6uL6YdaV1dnd0iu5bb8FhGuuuoqampqGDduHDU1NVx11VW2X7wYLhGhpKSECRMmICJMmDCBkpISR+5PamoqpaWlMRczS0tLtSIxHgYzQlyiF+A64IlZs2aZeHHTNAarV682KSkpMSMDp6SkOHauRDdNZReNBI3EyTBGdk3UkojcJjSKa3iKtKysLMeP7OqW/SE0HU70nOxOnw4nJycn5jiSk5OTVNOlJXJJRH5Hc+r3JiycxwMtTqRTKo3MWM/t8PGh/7msU/MBF83LLiJGRMzkyZNj/jp1RhU7DDa/Jfjc5HTFFVeYX/3qV3F5b8uymDBhApmZmRw4cICLLrqIjo6OyOioTlJcXMzFF1/Myy+/THd3N2lpaSxcuJB33nmHvXv32h3ekFmWxUUXXURNTQ3z589n165dlJeXc+DAAcd9NtFEhETkm4i8boy5Iu4bGoF45raIYFlWzHclfD+Zf+/Oxufz4fP5iO7bl52dTVdXF11dXTZGNnThKwXhzyP6c3LiZyMi1NTUsHHjRhobGyksLGTNmjWUl5fHbX/Gen5HS9RvaryE8yErK4v29vbIX3BmPhQXF1NdXR0zHszOnTupqKhw5LlIoo313C4uLiY9PZ3XX389WEAR4fLLL6ezs9OR3x/LsliwYAFHjhyJHB+mTJlCfX29485lvV4vHo+HQCBAT08PXq83MttNT0+P3eE5wmDzOyk79yVqurRbb701plnGrbfe6sjp0pzU9PtsovtbGxPsq7lgwQJSU1NZsGBBpA9nsvTLVsOTqMGhwoU+4IxCutNceeWVnDx5kpycHESEnJwcTp48yZVXXml3aEOWkpKC1+uNfB6BQACv15vsg6WdVVpaGq2trTGDO7a2tjp6ep+R0MHfhk5EIoXx9vZ2Rx/bGhsbmT9/fsy6+fPn65RKLpCI3C4pKeHXv/41kyZNAmDSpEn8+te/duzAv8YYdu7cGTMd4s6dOx1Z6dbb24vf7+fCCy/EsiwuvPBC/H6/o2dUSVZJWTA3CRhkorKyktra2pg+5rW1tVRWVsZtm/GSmprK6tWrKSkpwev1UlJSwurVqx3V9yO6GUdtbS0FBQXU19cDUF9fT0FBAbW1tf2bVimHSURuh0UX/pxs3759WJZFa2srxhhaW1uxLMtRFW9h48ePp7e3N2Ygu97eXsaPH293aMNyyy23cOedd7Jp0yZOnz7Npk2buPPOO7nlllvsDs0WicxvtzDGxFQiOvnYVlhYyK5du2LW7dq1y7Hj3aiPJCK3t23bxrhx42IGQxw3bpxjZ0vKy8sbcDrEvLw8myMbHo/Hw5EjRwgEAhw5cgSPx2N3SO40mPbudi2XX375kNvwD0Vtba0pKioylmWZoqIix/aDCvfVjO7XFe7D6VThzwZw9GcTDe1jnpDcBoyIxPTrEhFH9usyxkT6nK5cudK0tbWZlStXOrYfqogYn88X05fW5/M5+rdq9erVJi0tzQAmLS0t7mN7jPX8jubEHIgWzoFwv9PwX6ful/YxH5mxntuAufvuu2POy++++27H5kNubu6Afcxzc3PtDm3Iwr9LOTk5MX+d+tnYYbD57cz2g6OkrKzMKXNKn9Ps2bNZvHgxFRUVkX4sy5Ytc2wtI3z02YiII/sWKXulp6dTXV3N2rVrmTlzJunp6Y6ec3Pu3Ln8/Oc/Jzc3l8LCQubOncsbb7xhd1hD5vF48Hg85Ofnc/DgQWbOnMnx48e15l2NacFzNmf2K48WPp+KPhepqqpyxXmWSozvfOc79Pb2EggE+O1vf8t3vvMdu0MatpaWFu6++25qampYu3YthYWFrFu3jgceeMDu0IYtNTUVEXFUi1ynScqm7PHqy+KkuaXPJzrOhoYGqqqqaGhoIBAIxNx3yv6osSFRfVB7e3tpbm4mEAjQ3Nzs+H5Qe/bsiemntmfPHrtDGpbe3l5Onz5NZ2cngUCAzs5OTp8+7djPp6KigkcffZScnBwsyyInJ4dHH32UiooKu0OzxUjze2rezCEfowf73Kl5M0dzV0dN9BgY0WNjOFVZWVnMmAtaKHeHRBy7RYTOzk6+9rWv0dbWxte+9jU6Ozsdff5aUlISkw9O7S8PkJmZSXp6OhC8+JGZmWlzRCOTrFM7jtlR2aM5fWRXCH7BwoXxoqIiKisrXXFAdMNnE6ajsn9kJLn96BuP8tiexyL3n170NABLf7w0su7YtmMc23aMS//hUrw5wTm/uw9289u//S3rd6/nuXeeizz3lRtfYd+JfVTUV7DyD1eyau6qYcUVL+c6KXFabogIKSkpMQXx8H2n7QsER6oVkZhRab1eL8aYuI1U6+b8nnzDZCYtnhS53/H71QBkFjwSWdd9/LP4P7iazFlVWN5TAPR1Tuf0/grSpvyI1JxfRJ7b/s49WL4mMmY8xbFtxzj6b0eHu0tx4bZZCtTIuDm3B0NEyMjIYNKkSZEWVceOHeP06dOOzIcZM2bQ19fHD3/4w8gMQ8uWLcPj8XDo0CG7wxuS8CDZEydOjMxkdfz4cTo6Ohz52dTV1VFZWcmWLVsin83y5cvj2sJnsPmtBXO08JfM3LQ/WjD/SDxz2+fz0d3dHfl/h/+mpaU5bnox+OjkPTU1NTJNid/vB5x38h7el5ycHNra2rjgggtobW0FnLcvELs/J0+eJDs7O+774+b8FhEuuvPHcYgIDjy4KOm+Y26qdFMj5+bcHgwRYdGiRfzkJz+JTP179dVX8+Mf/9iR+VBXV8ftt99OZmZmpKKho6ODhx9+2HEXzi688EJaW1uZPHkyx44dY9KkSRw9epScnBxOnDhhd3hDZsfUjo6eLm2kHn3jUeZ8f05kaTjRQMOJhph1j77xKAALnl1A8dZi5nx/Dn/+wp8DsH73+pjnHjt9jFcPvRrzOqVUcuru7qaoqCjSByo1NZWioiK6u7ttjmz4srKy8Pv9GGPw+/1kZWXZHdKIfPjhhxhj+PDDD+0OZcTS0tJ47rnn6Orq4rnnnhuzU6UppdRIeDweXnzxRe6//346Ojq4//77efHFFx07BklZWRlLlizh/fffJxAI8P7777NkyRLHFcoBHnnkEbKysjhx4gSBQIATJ06QlZXFI488cv4XJ6FkntoxKa+Yi8h1wHWzZs265Z133knE9pK6Nm4wTXfDTXA/9p2PRZruFuYW8ux1zyZV092h7MuCZxdwvPM4kJz7MlR6xTwxuS0iTJo0iaeffjrSRGnp0qUcO3YsqfP8bEQEy7L49re/zYoVK3j88cdZu3YtgUDAcfvjtiuE4f3xeDz09fVF/sLYvGI+0vweq1fMs7KyaG9vj/wFZ+aDGhk35/ZghK/KWpYV+T0NBAKOvSobfcU83PzbqVfMAa655hp+8pOfRFoiXn311Wzfvt3usIaluLiYxYsXs23btshAleH7dl8xT8qCeZg2ZR86N+0LJP/+TM2byZHm+PQVmjJ9Bu83HRzy65L54B4W7+ZwPp+Pl156KVIw/7M/+zO6urqS+rt0NiKC1+tl+vTpkeZwzc3N9PT0OG5/ogsiHR0dZGZmOrogYkcfYTfn91gsmHs8HmbMmBE5cT906BB9fX1JF6uKPzfn9mBYlhUplIeFC+fh31UnmTFjBqdOnSInJyeS362trYwbN85xfczDA51OmjQp0pT92LFjrFq1iurqarvDG7Lw/kycODGyP8ePH4/r/gw2v8f0dGlKjdSR5kNxPZFUg9P/SmxXVxcLFiw45/OcdOLb09PDyZMnCQQCnDx5Mm4DiyVKZmbmGQVzJ8vOzj6jj7lSA+n/W9XX18f+/fsBIn/7P89Jv1XhgWjDV6HcMhCtSoy+vj5WrlzJAw88wN13381jjz3m2FHZm5qa8Pl8NDc3Y4yhubkZj8dDvGelGY7ztma9AqZ9aRopv0jh4o0X483xMoEJ/MeB/wBwXGvWbdu24fP5aGlpwRhDS0sLPp+Pbdu22V7RoAVzpZTjRZ+4FhcXk56ezuuvvx5pcnX55ZfT2dkZtyZKo22gE5FwgS+64OfEk/f09PSYg2F6ejqdnZ12hzVsPp8v5rPx+XyOHGRQJUZ0nrptVPazjXQMaOFcnZcxhqysLG688UYyMjK48cYb+cEPfuDoytuuri4mT57M0aNHyc3N5ejR5JoZImzV3FUDFp7fvPlNIPhbNX78eHJzc3lnzTvMnDmTlpaW4Dgxfwfr561n/bz1Ma+dlDEp8vpk09TURHZ2NpMmTeLAgQNMnz6d1tZWmpqa7A7NnYO/KaXGrsrKSk6cOMErr7wCwCuvvMKJEyeorKy0ObLBM8ZEltraWgoKCqivrwegvr6egoICamtrY56XrPrPO93Z2Rm54t/T0xMplPd/nhOkpaVx+eWXRwZ8639fqXMpLS0FiJnHPHq901RVVbFlyxZKSkrwer2UlJSwZcsWqqqq7A5NOcRtt91GRUUFPp+PiooKbrvtNrtDGrEPPvgg5q9TBQIBampq6OrqoqamxpHdC6KFz0PC5xzJ0hJRC+ZKKVcpKyujqqqKiooKINiXKJ5zU8ab0/cnuvIgXODIycmJ+VtaWuqISob+rrzySl577bWYiobXXnuNK6+80ubIlBNs376d0tLSyImhiFBaWurYAZWSeaRj5QxPPPEE1dXVdHV1UV1dzRNPPGF3SCM2fvx4LMti/PjxdocyIu3t7Vx99dWkpqZy9dVXO7olAwQvElRUVHDq1CkqKiqSpuVeUjZljxr90e5QlFKjKFG5XVZWRllZGSLimObr5+KW/dm+fXtkZFeAtrY2RxdEwoMg9b/imYhBS5ORHruHLvzdFxHHX4EqLCxk165dMXMD79q1i8LCQhujUqMhXrndv4XUqVOnXDU+jNfrjRmDpL29PWmuzA5HeGC+6AH6nCojI4Pq6mruuOMOLrroIjIyMujo6LA7rOS8Ym6MecEYc2t2drbdoSilRpHmttq+fXtMQdaphXKAlpYWMjIyyM/Px7Is8vPzycjIoKWlxe7QbKH5PbZVVlayZMkSCgoK8Hg8FBQUsGTJEkd1I1IDi1duR7eUCreq6t+CpP9znKS3t5fOzk6MMXR2dtLb22t3SCokEAjEDMyXLBWjSXnFXLnXcKYXG2yf0+FOL6aUUkqpkevu7qatrS1y0puenm53SMpBnN6CpP/5qjEmMuBb9MBv4ec5raLBLXJzc2Mq0Ht6eujp6SE3N9fGqIK0YK4SSqcXU0q5SWdnJ52dnQQCgchtpcaidevWkZGRwbZt2yKjst90002sW7fOMWNiKDUS0QXt6Lmyjx49yuTJk+M+V3a8TZ48OTLvd7KOMD8Y3d3dQHCcm9bW1sjf8Ho7JWVTdqWUUsoJjDH4/X5EBL/fr1dA1JjV1NTEU089FTMq+1NPPZUUUxAplWjV1dWsWrWKtrY2IDimipML5RCsfGtvb2fdunV2hzIiHR0dXHzxxTGfzcUXX6x9zONlat7MmKl3zrcAg37u1LyZSb0/Q9kXu/ZHKaXcIjU1lfb2dowxtLe3k5qaandISimlkkB4hHkgMtK8U6WmpnLXXXeRmZnJXXfd5fhj3XvvvcdDDz1ER0cHDz30EO+9957dIQEubcrutubSbtsfpZRKdo++8SiP7Xkscv/pRU8DsPTHSyPrJi2exLFtx7j0Hy7Fm+MFoHN/J++tf4/pfzmd3Ks+6q/2yo2vsO/EPirqK1j5hytZNXdVgvZEqcTIy8vj5ptv5uKifTMAAB9GSURBVIc//GGkKfvNN99MXl6e3aEppUbA4/Hg9/uZMmUKx44d48ILL+TIkSN4PB67Qxs2j8dDdXU1a9euZebMmXg8nqQY1yApC+Y65YpS7qS5rZxi1dxVAxae37z5zcjtGd+YwQUXXED3w928vf9t8vPzaWtrIy8vj0NPnjnI5aSMSTGvdxvN77Ftw4YN3H777ZSXl3Pw4EFmzpxJb28vGzdutDs0NUKa22PbypUr2bx5M8ePHycQCHD8+HFEhJUrV9od2rD19PRw8OBBAoFA5G8ySMqm7DrlilLupLmt3GTDhg14vd6YdV6vlw0bNtgUkb00v8e2srIy+vr62L9/P4FAgP3799PX1+fYgd/q6uooLi7G4/FQXFxMXV2d3SHZRnN77Inu9vrII49gjImZx9wYwyOPPHJGd1qn8Hq9kSv+Ho/njGO5XZLyirlSSimV7MIFjqqqKgAyMzO5//77HVsQUaNrMN0hwt0aLv2HS5nz/TkAFOYW8ux1z7J+93qee+e5yHOTvTvEzJkzaWlpwefz0dXVhc/no6WlhZkzZ3LwoLOmMq2rq6OyspItW7ZEmuUvX74cQPNbjQlnG8hURFwxyGlPT8+At+2mBXOllFJqmMrKyigrK0NE2Lt3r93hqCQymO4QYW//n7fPONldP2896+etj1mXzN0hDh06hGVZMVfVLMvi0KEzu3Uku6qqKrZs2UJJSQkAJSUlbNmyhYqKCi2YK+USlmURCAQif5NBUjZlV0oppZRSzhIIBCJXn3p6epLmZHeoGhsbmT9/fsy6+fPn09jYaFNESqnR5PP5+OlPf4rf7+enP/0pPp/P7pAALZgrpZRSSikVUVhYyK5du2LW7dq1i8LCQpsiUkqNpqysLMrLy0lLS6O8vJysrCy7QwK0YK6UcpipeTNjBhs51wIM+rkiwtS8mUm7L0Pdn0Tvi1Iqlpt+q8aayspKlixZQkFBAR6Ph4KCApYsWUJlZaXdoSmlRkhEIgNUGmMiA1YmwwB22sdcKeUoR5oPcdGdP47Lex94cFFc3vds3LQvECyIHGkefH/SoRwEp0yfwftNzhpASo1tbsvvsaa7u5u2tjYCgQDNzc2kp6fbHZJSahTk5OTQ0tKC1+ulp6cHr9dLS0sLubm5doemBXOlRsJ8czxwU3ze/Jvj4/O+SsWJFkSUUjk5ObS1tXHBBRfQ2tpqdzjDsm7dOjIyMti2bVtkVPabbrqJdevW6eBvSjlcS0sLlmXFjIdhWRYtLS02R6YFc6VGRO79MK4FEbM+Lm+tlFJKxcXJkycxxnDy5Em7Qxm2pqYmduzYETMq+1NPPUVpaanNkTnTUFtTgbaoUvH19a9/ne3bt9PY2EhhYSHXXHMNmzZtsjusxBbMRWQx8DlgPLDFGLMjkdtXSsWH5rZS7qS5rYYqPBK7U0dkH0sSld/xbE0F2qJKDd3DDz8cmdqxoaGBt956y+aIggY9+JuI1IjIMRHZ22/9tSLytoi8KyJ3nes9jDHbjDG3ACuAJcMLWSk1mjS3lXInzW0Vb/0HsBvp85JFXl4eN998Mzt37qSnp4edO3dy8803k5eXZ3doEZrf9tGBW50vXCg/2327DOWK+VbgEeCp8AoR8QCbgauBJuCXIvI84AEe6Pf6cmPMsdDtb4Rep5Sy31Y0t5Vyo604KLd1zA7nMcZEbtfV1VFeXk5XV1dknc/no6amxnH9sjds2MDtt99OeXk5Bw8eZObMmfT29rJx40a7Q4u2FQflt5voeCoqXgZdMDfG/FxE8vut/mPgXWPM7wBE5Gng88aYB4AzvlkSrDb6FvCyMebXA21HRG4FbgWYOVNrjZSKN81tpdwpUbkdet6I81vH7HC2cOG7qqqKhoYGioqKqKysdFyhHIL7snv3br73ve8RCAR4//33ueWWW5JqX/TYrdTweb1epk+fzoEDB7joootobm6ODAZnp5HOYz4diB7NoSm07mwqgD8FviQiKwZ6gjHmCWPMFcaYKyZOnDjC8JRSw6S5rZQ7jXpug+a3CiorK2Pv3mDL6r179yZVQXYo6urqeOaZZ5g6dSqWZTF16lSeeeYZ6urq7A7tfPTYrdQgeDweampq6O7upqamBo/HY3dIQIIHfzPGfBf47vmeJyLXAdfNmjUr/kEppUZMc1spdxpsboPmt3KPdevWkZKSQk1NTWS6tGXLlrluujQ9dquxqquriy9+8Yu0traSk5MT0wXHTiO9Yt4MzIi6nxdaNyLGmBeMMbdmZ2eP9K2UUsOjua3GNBcP7hOX3AbNb+UeTU1NfOITn2DhwoWkpqaycOFCPvGJT9DU1GR3aOejx26lBjDQAJStra0xf8/2vEQa6RXzXwIXi0gBwcRfStxGblFKJZDmthrTXDy4j+a2UoPwwgsv8NBDD7FixQoef/xx7rjjDrtDGgzNbzVkQ51n3olzzPcfqPK2226jq6uLnp4evF4vPp+Pf/zHf7S9RcygC+YiUgdcBUwQkSbgm8aYLSKyGthOcMTHGmNMw0iDGmmTGR3ZVanBc1JuK6UGL5G5Hdqe5rdyjaysLC677DK8Xi+XXXYZWVlZnDp1yu6wIvTYrUaLiyuiB9R/oMpLLrkkaQaqHMqo7ANGa4x5CXhp1CIKvucLwAtXXHHFLcN5vdtGdtWKBhVPTsptpdTgJTK3Q++r+a1cw+fzUV5eHhm12efzJVXBXI/dSg1fWVkZZWVliEhkwMpkkNDB3wZLa+ZiuamiQSsZxjbNbaXcS/NbuUVaWho5OTm88847GGM4cOAAF198MR9++KHdodlCc1upxEjKgrnWzLmXmyoZ1NBpbivlXprfyi0uueQS3nzzTa6//nq2bNnC8uXLef7555kzZ47dodlCc1upxEjKgrlSSimllFJ2+O1vf8sll1zCCy+8wMSJExERLrnkEn77/9u7/2C76/rO4893wACmElQaw+a3hhpZp7DuXWxn3W6WToB2CDK0O7V0Z2W0ptrF7TrbHRntiMyshUx33O2Ks262ZDKsFdsqpYkyG2mrtXW6AtYYgimSQYGbgQY66TUoXNbw3j++3xtu7g9ybs6P7/f7uc/HzJnc8z2f8z2f973ndb55n/P9fs93vtP01CQVrN+vSxuKiNgaETsmJiaanoqkATLbUrnMt0oxOTnJ0aNHWbduHRHBunXrOHr0KJOTk01PrRFmWxqNVjbmfl+iVCazLZXLfKskzz//PDt37mRycpKdO3fy/PPPNz2lxphtaTTclV2SJEma5tixY1x22WVNT0PSItLKT8zdZUYqk9mWymW+pTKZbWk0WtmYu8uMVCazLZXLfJfrgtVriYieL0DPYy9Yvbbh6uZ39dVX8/TTT3P11Vc3PZVGmW1pNNyVXZIkSfN66vATQ/2q0zZaunQp+/fv53Wvex1r165l6dKlvPDCC01PS1LBbMwldUredC5w3XBWftO5w1mvJKlTNm3axPHjxwFYtmwZmzZtYv/+/Q3PSlLJbMwldUrc/P2hfnKTHx3KqudU2psMpdUjafGY2gV/yvQm/KGHHppzXGYOf2KSFo1WNuYRsRXYunHjxqanImmAzPbJSnqTAcqrRwtjvsu14poVvOpNN564/oPv3gDAsg23nVg2+fTP8sIzW1i28WMsecUxAI4/t4offu/9nLXyLpa++r4TY5995EMsOXucV665gxXXrBhRFS9vepN9xRVX8KUvfWnWmMsvv5y9e/eOclqtYLal0WhlY56Ze4A9Y2Nj72l6LpIGx2xL5TLf5Tpy9xHOeePOWcuPHbx11rIfHPrwrGWTT13L5FPXnrTs+LMXcezgrRy5u33HmO/du5crrriCe++9l8wkItiyZcuibMrBbKs7Lli9lqcOP7Gg+8zcW2Y+K1et4cnxx09nWj1rZWMuSZIkNWWqCY8IXnzxxYZn021DPcwJRn6ok4dttdeL/2ySN3/szSeuD3LvniN3Hxn6/G3MJUmSBsz/vEuVYR7mBKM/1MnDttqr63v32JhLkiQNmP95lyQtxJKmJzCXiNgaETsmJiaanoqkATLbUrnMt1Qmsy2NRisb88zck5nbli9f3vRUJA2Q2ZbKZb6lMpltaTTclV2SpBk8PliStBi4vWsPG3NJkmbw+GBJ0mLg9q49WrkruyRJkiRJi4WfmEt9WLlqDY9tH87XJ6xctWYo65UkSZLULjbmUh+eHH+857ERQWYOcTaSJEmSushd2SVJkiRJalArG3O/L1Eqk9mWymW+pTKZbWk0Wrkre2buAfaMjY29p+m5SBocsy2Vq998e84OqZ3cdkuj0crGXJIkLS4LOWcHeN6OUSrte44vWL2Wpw4/0fP4iOh57MpVaxb8XJYksDHvBD9FkCRJTSnte46fOvzEUOuRpNNhY94BnvlbkiRJksplY66R8tN/SZIkSTpZkY25zV97eQyhJEmSJJ2syMbc5k+SJElzWXHNCl71phtPXP/Bd28AYNmG204sm3z6Z3nhmS0s2/gxlrziGADHn1vFD7/3fs5aeRdLX33fibHPPvIhlpw9zivX3MGKa1aMqApJpSmyMZckSZLmcuTuI5zzxp2zlh87eOusZT849OFZyyafupbJp649adnxZy/i2MFbOXK3J3+TdHqWND0BSZIkSZIWs5E15hHxpoj4VER8LiLeN6rHlTR85lsqk9mWymW+pXbpaVf2iNgJXAUcycw3T1t+JfC7wBnA72Xm7H2Aapl5EHhvRCwB7gD+Rz8TlzQY5lsqk9mWymW+pdnypnOB64az8pvOHc56p+n1GPNdwG1UoQUgIs4APglsAcaB+yNiN9ULwS0z7v+uzDwSEVcD7wP+d5/zljQ4uzDfUol2YbalUu3CfEsniZu/z7oPfmEo635s+1XkR4ey6hN6aswz86sRsX7G4kuBQ5n5KEBEfBZ4e2beQvUO3lzr2Q3sjogvAp+Za0xEbAO2Aaxdu7aX6Unqw6jybbal0XLbLZXLbbdUnn7Oyr4KeGLa9XHgrfMNjojNwLXAWcA9843LzB3ADoCxsTG/w0xqxsDzbbalVnDbLZXLbbfUYSP7urTM/ArwlV7GRsRWYOvGjRuHOSVJA9Jrvs221C1uu6Vyue2W2qWfs7IfBtZMu766Xta3zNyTmduWL18+iNVJWrih5NtsS41z2y2Vy2231GH9NOb3AxdGxIaIWAq8A9g9iElFxNaI2DExMTGI1UlauKHk22xLjXPbLZXLbbfUYb1+XdqdwGbg/IgYB27KzNsj4gZgL9XZHndm5kODmFRm7gH2jI2NvWcQ65M0v1Hm22xLo+O2WypXl7bdK1et4bHtc557biBWrlpz6kFSB/R6VvZfnmf5PbzMyWAktZ/5lspktqVydSnfT44/vqDxEUGm55nT4jOyk78thCeZkMpktqVymW+pTGb7ZMPcA8BP/xe3fo4xHxpPMiGVyWxL5TLfUpnM9smeHH+czOz5AvQ8dqF7F6gsfmIuaWQGke2S3qkuqZapxyypHi2M226pTGa7bG6726OVjbknkJHKNIhsL+Td5LYfp1bacXcl/W20cG67T+Z/dlUKs102t93t0crGXJIkqcv8z2575U3nAtcNZ+U3nTuc9Uoqno25JEmSFo24+fus++AXhrLux7ZfRX50KKuWVLhWnvwtIrZGxI6JiYmmpyJpgMy2VC7zLZXJbEuj0crG3LM/SmUy21K5zLdUJrMtjYa7skuSNIMn7pIkSaNkYy5J0gylnTFfkiS1Wyt3ZfdYFqlMZlsql/mWymS2pdFo5Sfmfl+iVCazLZXLfJfLQzsWN7MtjUYrG3NJkiS1g4d2SNLwtXJXdkmSJEmSFgsbc0mSJEmSGmRjLkmSJElSg1p5jHlEbAW2bty4sempSBogsy2Vy3xLZTLb6oqun6iylZ+YZ+aezNy2fPnypqciaYDMtlQu8y2VyWyrK54cf5zM7PkC9Dx2oSfBPB2tbMwlSZIkSVosbMwlSZIkSWqQjbkkSZIkSQ2yMZckSZIkqUE25pIkSZIkNaiVjXlEbI2IHRMTE01PRdIAmW2pXOZbKpPZlkajlY25X8sglclsS+Uy31KZzLY0Gq1szCVJkiRJWixszCVJkiRJapCNuSRJkiRJDbIxlyRJkiSpQTbmkiRJkiQ1yMZckiRJkqQG2ZhLkiRJktQgG3NJkiRJkho00sY8IpZFxAMRcdUoH1fS8JlvqUxmWyqX+Zbao6fGPCJ2RsSRiDgwY/mVEfFwRByKiBt7WNUHgT88nYlKGg7zLZXJbEvlMt9Sec7scdwu4DbgjqkFEXEG8ElgCzAO3B8Ru4EzgFtm3P9dwMXAt4Gz+5uypAHbhfmWSrQLsy3NsnLVGh7bPpwPiFeuWjOU9c5hF+ZbKkpPjXlmfjUi1s9YfClwKDMfBYiIzwJvz8xbgFmvdhGxGVgGXAQ8FxH3ZOaLc4zbBmwDWLt2bc+FSDo9o8q32ZZGy223NLcnxx/veWxEkJlDnM3pcdstlafXT8znsgp4Ytr1ceCt8w3OzA8DRMT1wDNzbdjrcTuAHQBjY2PteyWUFoeB59tsS63gtlsql9tuqcP6acxPS2buOtWYiNgKbN24cePwJyRpYE6Vb7MtdZPbbqlcbruldujnrOyHgekH0qyul/UtM/dk5rbly5cPYnWSFm4o+TbbUuPcdkvlctstdVg/jfn9wIURsSEilgLvAHYPZlqSGma+pTKZbalc5lvqsF6/Lu1O4K+BN0bEeES8OzN/BNwA7AUOAn+YmQ8NYlIRsTUidkxMTAxidZJexijzbbal0XHbLZXLbbdUnmjjmSanjI2N5QMPPDD0x2nrGTdPR0m1QFn1jKqWiPhGZo4N/YH6YLZPT0n1lFQLmO/pzPfClVQLlFWP2X6J2T49JdVTUi3Qvnz3syv70PjOnFQmsy2Vy3xLZTLb0mi0sjH3JBNSmcy2VC7zLZXJbEuj0crGXJIkSZKkxaKVjbm7zEhlMttSucy3VCazLY1GKxtzd5mRymS2pXKZb6lMZlsajVY25pIkSZIkLRatbMzdZUYqk9mWymW+pTKZbWk0WtmYu8uMVCazLZXLfEtlMtvSaLSyMZckSZIkabGwMZckSZIkqUE25pIkSZIkNaiVjbknmZDKZLalcplvqUxmWxqNVjbmnmRCKpPZlsplvqUymW1pNFrZmEuSJEmStFjYmEuSJEmS1CAbc0mSJEmSGtTKxtyTTEhlMttSucy3VCazLY1GKxtzTzIhlclsS+Uy31KZzLY0Gq1szCVJkiRJWixszCVJkiRJapCNuSRJkiRJDbIxlyRJkiSpQTbmkiRJkiQ1yMZckiRJkqQGtbIx9/sSpTKZbalc5lsqk9mWRqOVjbnflyiVyWxL5TLfUpnMtjQarWzMJUmSJElaLGzMJUmSJElqkI25JEmSJEkNsjGXJEmSJKlBNuaSJEmSJDXIxlySJEmSpAbZmEuSJEmS1CAbc0mSJEmSGjSyxjwiNkfEX0bEpyJi86geV9LwmW+pTGZbKpf5ltqlp8Y8InZGxJGIODBj+ZUR8XBEHIqIG0+xmgSeBc4Gxk9vupIGzXxLZTLbUrnMt1SeM3sctwu4DbhjakFEnAF8EthCFeb7I2I3cAZwy4z7vwv4y8z8i4h4HfBx4Ff6m7qkAdmF+ZZKtAuzLZVqF+ZbKkpPjXlmfjUi1s9YfClwKDMfBYiIzwJvz8xbgKteZnVHgbPmuzEitgHb6qvPRsTDvcyxT+dHxDMjeJxRKKkWKKueUdWybiGDR5Vvsz0QJdVTUi3Qwny77e6UkmqBsuppXbbBbXfHlFRPSbVAy/Ld6yfmc1kFPDHt+jjw1vkGR8S1wBXAeVTv8M0pM3cAO/qY14JFxAOZOTbKxxyWkmqBsurpWC0Dz7fZ7l9J9ZRUC3SqHrfdLVRSLVBWPR2rxW13C5VUT0m1QPvq6acxX5DMvAu4a1SPJ2l0zLdUJrMtlct8S+3Sz1nZDwNrpl1fXS+T1H3mWyqT2ZbKZb6lDuunMb8fuDAiNkTEUuAdwO7BTGvkRrqLzpCVVAuUVU+Xaikl3136nfeipHpKqgW6U08p2Ybu/M57UVItUFY9XaqllHx36Xfei5LqKakWaFk9kZmnHhRxJ7AZOB/4O+CmzLw9In4e+G9UZ3vcmZkfG+JcJQ2B+ZbKZLalcplvqTw9NeaSJEmSJGk4+tmVXZIkSZIk9anzjXlEfC8iHoyIfRHxwALud0m9u0+jImJnRByJiAMzlr8mIu6NiEfqf1/d4/rOi4hfH85sT/nYayLiyxHx7Yh4KCJ+Y9ptnaonIs6OiPsi4lt1LTdPu21DRHw9Ig5FxB/Ux3H1ss71EXHd8GZdFrM9a31me0DMd7O6nm0oJ99mu6d1mu0F6Hq+S8l2/djm+9TrbFW+O9+Y1/5VZl6ywO+huwRo/AUA2AVcOcfyG4E/y8wLgT+rr/fiPKCRwAA/Av5jZl4E/BTw7yLiovq2rtUzCVyWmRdTPVeujIifqm/bDvzXzNwIHAXe3eM61wOtCX9HmO2XmO3BMd/N63K2oZx8m+1TW4/ZXqgu53sXZWQbzHcv1tOmfGdmpy/A94DzTzHmXwMHgG8BXwWWAo8DTwP7gF8ClgE7gfuAbwJvr+97PfAnwFeAR6hOrkE9/ov1Og8Av9RHDeuBAzOWPQxcUP98AfDwHPf7x/V89wH7gQuBzwLP1ct+px73n6jO1LkfuHnaY/4t8PvAQeBzwCvr224Fvl2P/y991PUnwJau1wO8Evgb4K1AAM8AZ9a3/TSwd477/Mt6zvvq59OrgP8LTNTLPkB1YpbfmVbLr9X33Vw/T79Y/94+RfUm2hlUG4wDwIPAB5rO3zAvmO3WZWHa/IrIdr0O8z3iCwVke9pzs6h8Y7bNdp8XCsg3BWa7Xo/5bnm+Gw9wvxfgu/Uf5hvAtnnGPAisqn8+r/73euC2aWN+G/g3U2OA71CF/HrgSeC1wDn1L34M+AXgf027//I+aljP7BeAf5j2c0y/Pm35J4BfqX9eWs/vpHUBl1N9FUDUT6AvAD9Tj0vgn9fjdgK/Wdf5MC+dGPC8Pmp6HDi3q/XUYdsHPAtsr5edDxyaNmbNzL9dvXzPtLn8GHAmVai/MG3MNuC36p/PAh4ANtTjngdeX8/hXuAXgX8K3Dvt/qf1t+nKBbPdmizMUVOns12PN98NXSgg29OyUEy+Mdtgts33S1koJtvTajLfLc93Cbuyvy0z3wL8HNUuGj8zx5ivAbsi4j1Uv8y5XA7cGBH7qN6FOxtYW992b2b+fWY+B9wFvI3qRWVLRGyPiH+RmRODK+lkWf2lc46b/hr4UER8EFhXz2+my+vLN6leKDdRveMF8ERmfq3++dNUdU1QPfFuj4hrgR8udL4R8WPA54H/kJnf72o9mXk8My8BVgOXRsSbe70v1XPu4xHx76lC+qN5avm39XPu61QvVlO13JeZj2bmceDOupZHgddHxCci4kpg1u+2MGa7JVmYUkq267ma7+YUn23oVh7M9glmu3/F57tLeQDzPU3r8935xjwzD9f/HgH+GLh0jjHvBX6L6h2Ub0TEa+dYVQC/kNUxMZdk5trMPDi1itmrzO8Ab6F6IfjPEfGRwVR0wt9FxAUA9b9HZg7IzM8AV1PtUnJPRFw2x3oCuGVaXRsz8/apVcxeZf6I6nf4OeAq4P8sZNIR8Qqq8P9+Zt7V9XrqlfwD8GWqY47+HjgvIs6sb14NHJ7jPrcCv0r17uLXImLTPLW8f1otGzLzSy9Ty1HgYqoN1HuB31toLV1ittuVhRKzXa/IfI9YwdmGDubBbJ90H7Pdp4Lz3ck8mO+T7tP6fHe6MY+IZRHxqqmfqd7lODDHuDdk5tcz8yNUx6+sAY5RHVswZS/w/oiI+j7/ZNptW6I6e+E5wDVUf8x/BPwwMz9NdSzCWwZc3m7gnfXP76Q6LmRmXa8HHs3M/17f/pPMXde76nfLiIhVEbGivm1tRPx0/fN1wF/V45Zn5j1Ux1pc3OuE69/d7cDBzPx4l+uJiB+PiPPqn88BtgB/m5lJ9ULwi6eo5Q2Z+WBmbqc6TmXTPLW8r37RJCJ+on4eQ/Uu4IaIWEJ1rNVfRcT5wJLM/DzVBm3Qz7nWMNvtyUK97mKyXa/bfDek8GxDx/Jgtmfd32z3ofB8dzEP5vvk+7c/39nHfvBNX6j28/9WfXkI+PA84+6iegftAPC7VO+GvKb+o0ydZOIc4H/W4x6iPt6A6liWu6n+4NNPMnEF1UkB9tXrGTvNGu6kOlbm/wHjwLvr5a+lOkviI8CfAq+Z47431nPdR/WO02vq5Z+pa506KcNv1HU9SLVbyht46aQMn6Y6KcPnqU6kcAHViR721+PfuYBa3kb1btLU72Uf8PNdrIfqxeeb9f0OAB+Z8by7DzgE/BFw1hz3/0R9v/313/gs4BXAn1M9Xz9A9cbYb/PSc/PLwHLmP8HExVS7CU39bn+u6QyabbPd0XrMt9k+7WyXlG/Mttk230Vm23x3M99TB95rHhFxPVW4b2h6LoMUEeupXuQWcmxGa5VUT0RsBn4zM69qei4lM9vdUGA9mzHfQ1VqtqGsPJRUC5jtUSk13wXmYT1l1bOZEeS707uyS5IkSZLUdX5iLkmSJElSg/zEXJIkSZKkBtmYS5IkSZLUIBtzSZIkSZIaZGMuSZIkSVKDbMwlSZIkSWrQ/wcdQIqMD9TKewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba2d327f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ba2bbecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps_all = t.pickle_from_file('res_lstm_nextstep')\n",
    "steps_random = [t.pickle_from_file('res_lstm_nextstep_random')]\n",
    "steps_all.extend(steps_random)\n",
    "t.box_plot(Y, (17,4), steps_all, ['5 steps', '10 steps', '20 steps', '30 steps'], \n",
    "           'lstm trained stepwise \\n at input length: ', steps = [5,10,20,'random'])\n",
    "\n",
    "steps_all = t.pickle_from_file('res_lstm_finalstep')\n",
    "steps_random = [t.pickle_from_file('res_lstm_finalstep_random')]\n",
    "steps_all.extend(steps_random)\n",
    "t.box_plot(Y, (17,4), steps_all, ['5 steps', '10 steps', '20 steps', '30 steps'], \n",
    "           'lstm trained on final step \\n at input length: ', steps = [5,10,20,'random 5-20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    running hyperparameter optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139861376198400\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.00975 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.01163 -0.00698 -0.01064]\n",
      "hyperband obj crossval results 0.00975\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0221 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.03018 -0.01989 -0.01623]\n",
      "hyperband obj crossval results 0.0221\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01826 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02461 -0.01537 -0.0148 ]\n",
      "hyperband obj crossval results 0.01826\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "traj {'losses': [-0.00975, -0.04739, -0.0611], 'budgets': [10.0, 10.0, 10.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2)], 'time_finished': [0.1408522129058838, 0.5603299140930176, 0.603823184967041]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(0, 0, 0): {'config_info': {}, 'config': {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}}, (0, 0, 2): {'config_info': {}, 'config': {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}}, (1, 0, 0): {'config_info': {}, 'config': {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}}, (0, 0, 1): {'config_info': {}, 'config': {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}}, (0, 0, 3): {'config_info': {}, 'config': {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}}, (1, 0, 1): {'config_info': {}, 'config': {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}}}\n",
      "return best config:  {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='xgb', min_budget = 10, max_budget=40, run_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140187071411968\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.008436577394848653, 'batch_size': 19}\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015971832060604426, 'batch_size': 25}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 169us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 138us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 215us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.08455] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.10933]\n",
      " [ 0.07648]\n",
      " [ 0.06783]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.08455\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 108us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02906] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04199]\n",
      " [ 0.02392]\n",
      " [ 0.02129]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02906\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 193us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 191us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 167us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03437] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04792]\n",
      " [ 0.02945]\n",
      " [ 0.02575]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03437\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.009586457136379657, 'batch_size': 27}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 176us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 238us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03834] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.05514]\n",
      " [ 0.03206]\n",
      " [ 0.02784]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03834\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.06945643081756749, 'batch_size': 42}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 153us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 217us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 186us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03485] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04871]\n",
      " [ 0.02974]\n",
      " [ 0.02609]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03485\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 168us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 166us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 193us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03232] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04512]\n",
      " [ 0.02714]\n",
      " [ 0.02468]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03232\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 141us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 299us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03407] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04797]\n",
      " [ 0.02893]\n",
      " [ 0.02531]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03407\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 175us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 463us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 98us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02954] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04222]\n",
      " [ 0.02424]\n",
      " [ 0.02215]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02954\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 164us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 116us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 156us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03475] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04788]\n",
      " [ 0.02994]\n",
      " [ 0.02643]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03475\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 116us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 115us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01829] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02596]\n",
      " [ 0.01484]\n",
      " [ 0.01408]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01829\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 132us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 152us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 175us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03005] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0427 ]\n",
      " [ 0.02536]\n",
      " [ 0.0221 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03005\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 143us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 144us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 179us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03387] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04688]\n",
      " [ 0.0292 ]\n",
      " [ 0.02552]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03387\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013227285301914478, 'batch_size': 23}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 180us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 174us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0697] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.09383]\n",
      " [ 0.06129]\n",
      " [ 0.05398]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0697\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 159us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 123us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00868] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00941]\n",
      " [ 0.00721]\n",
      " [ 0.00941]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00868\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 142us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 150us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03079]\n",
      " [ 0.01677]\n",
      " [ 0.01646]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02134\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 141us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 189us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0307] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0426 ]\n",
      " [ 0.02644]\n",
      " [ 0.02307]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0307\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 111us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00811]\n",
      " [ 0.007  ]\n",
      " [ 0.00624]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013453747549283328, 'batch_size': 54}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 89us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 140us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.09446] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.12028]\n",
      " [ 0.08502]\n",
      " [ 0.07807]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.09446\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 167us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 136us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00976] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01218]\n",
      " [ 0.00842]\n",
      " [ 0.00868]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00976\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 155us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 145us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 132us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02208] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03183]\n",
      " [ 0.01757]\n",
      " [ 0.01684]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02208\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 126us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 180us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 155us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02839] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0411 ]\n",
      " [ 0.02349]\n",
      " [ 0.02059]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02839\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 100us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 124us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 89us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03028] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04305]\n",
      " [ 0.02544]\n",
      " [ 0.02235]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03028\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 161us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 194us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 137us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01432] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02139]\n",
      " [ 0.01098]\n",
      " [ 0.0106 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01432\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015944533862384857, 'batch_size': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 160us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 130us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.05398] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.07588]\n",
      " [ 0.046  ]\n",
      " [ 0.04005]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.05398\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.002410925627224085, 'batch_size': 52}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 278us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 128us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 190us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.04653] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.06616]\n",
      " [ 0.03931]\n",
      " [ 0.03413]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.04653\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 122us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 118us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 181us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00846]\n",
      " [ 0.00592]\n",
      " [ 0.00698]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 329us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 246us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 218us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02235] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03238]\n",
      " [ 0.01768]\n",
      " [ 0.01699]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02235\n",
      "traj {'time_finished': [2.2007694244384766, 3.744567394256592, 21.17214035987854, 31.383424997329712, 38.972952127456665], 'budgets': [12.5, 12.5, 25.0, 50.0, 100.0], 'losses': [0.08455, 0.02906, 0.01829, 0.00868, 0.00712], 'config_ids': [(0, 0, 1), (0, 0, 2), (0, 0, 2), (0, 0, 2), (0, 0, 2)]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(1, 0, 3): {'config': {'lr': 0.0013453747549283328, 'batch_size': 54}, 'config_info': {}}, (0, 0, 7): {'config': {'lr': 0.021337462797969355, 'batch_size': 40}, 'config_info': {}}, (2, 0, 3): {'config': {'lr': 0.002410925627224085, 'batch_size': 52}, 'config_info': {}}, (0, 0, 2): {'config': {'lr': 0.37364476899503524, 'batch_size': 48}, 'config_info': {}}, (1, 0, 0): {'config': {'lr': 0.07769230011012973, 'batch_size': 41}, 'config_info': {}}, (0, 0, 6): {'config': {'lr': 0.07388122579837485, 'batch_size': 33}, 'config_info': {}}, (2, 0, 2): {'config': {'lr': 0.0015944533862384857, 'batch_size': 42}, 'config_info': {}}, (0, 0, 1): {'config': {'lr': 0.0015971832060604426, 'batch_size': 25}, 'config_info': {}}, (1, 0, 1): {'config': {'lr': 0.0013227285301914478, 'batch_size': 23}, 'config_info': {}}, (0, 0, 5): {'config': {'lr': 0.06945643081756749, 'batch_size': 42}, 'config_info': {}}, (0, 0, 0): {'config': {'lr': 0.008436577394848653, 'batch_size': 19}, 'config_info': {}}, (2, 0, 1): {'config': {'lr': 0.07673276670965525, 'batch_size': 62}, 'config_info': {}}, (0, 0, 4): {'config': {'lr': 0.009586457136379657, 'batch_size': 27}, 'config_info': {}}, (1, 0, 2): {'config': {'lr': 0.04580026289754379, 'batch_size': 43}, 'config_info': {}}, (2, 0, 0): {'config': {'lr': 0.164477508973482, 'batch_size': 30}, 'config_info': {}}, (0, 0, 3): {'config': {'lr': 0.016073094701798084, 'batch_size': 19}, 'config_info': {}}}\n",
      "return best config:  {'lr': 0.37364476899503524, 'batch_size': 48}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 10, max_budget=100, \n",
    "                       run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05283, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05283 to 0.04822, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04822 to 0.04465, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.04470, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04465 to 0.04354, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04354 to 0.04342, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.04631, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04342 to 0.04184, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04184 to 0.03921, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.04097, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03921 to 0.03690, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03690 to 0.03614, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03614 to 0.03320, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03320 to 0.03232, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03232 to 0.03137, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03137 to 0.02900, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.03021, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02900 to 0.02758, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02758 to 0.02378, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02378 to 0.02372, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02372 to 0.02174, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02174 to 0.02093, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02093 to 0.01898, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01898 to 0.01830, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.02128, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01830 to 0.01674, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01674 to 0.01596, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01596 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01556 to 0.01350, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01350 to 0.01340, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01340 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01316 to 0.01282, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01282 to 0.01170, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01170 to 0.01109, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.01383, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.01155, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01109 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00966 to 0.00964, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00969, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00964 to 0.00926, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00926 to 0.00921, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00921 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00887 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00944, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00854 to 0.00832, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01047, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00832 to 0.00812, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00812 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00790 to 0.00775, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00775 to 0.00770, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00770 to 0.00767, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00767 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00758 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00753 to 0.00751, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00751 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00745 to 0.00742, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00742 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00940, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00741 to 0.00740, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00740 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00733 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00733 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00732 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01269, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00728 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00727 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00724 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00719 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00715 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00709 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00935, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00709 to 0.00700, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.00700 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00699 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.00699 to 0.00696, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss is 0.00703, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.00696 to 0.00695, storing weights.\n",
      "\n",
      "Epoch 00299: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.00695 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00689 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00689 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00680 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00679 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00679 to 0.00669, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.00669 to 0.00666, storing weights.\n",
      "\n",
      "Epoch 00407: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.00666 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00413: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.00664 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00418: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.00658 to 0.00657, storing weights.\n",
      "\n",
      "Epoch 00424: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.00657 to 0.00652, storing weights.\n",
      "\n",
      "Epoch 00428: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.00652 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00431: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.00647 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00433: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.00645 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00447: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.00642 to 0.00638, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00449: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00638 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00490: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.00634 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00505: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.00624 to 0.00623, storing weights.\n",
      "\n",
      "Epoch 00515: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.00623 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00569: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.00620 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.00618 to 0.00617, storing weights.\n",
      "\n",
      "Epoch 00574: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00713, did not improve\n",
      "Epoch 00589: early stopping\n",
      "Using epoch 00573 with val_loss: 0.00617\n",
      "89/89 [==============================] - 0s 147us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03010, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03055, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03010 to 0.02792, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02792 to 0.02726, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02965, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02726 to 0.02655, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02655 to 0.02478, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02478 to 0.02292, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02292 to 0.02190, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02190 to 0.02047, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02047 to 0.02037, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.02037 to 0.01899, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01899 to 0.01775, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01775 to 0.01697, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01697 to 0.01573, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01636, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01573 to 0.01439, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01468, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01439 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01316 to 0.01254, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01254 to 0.01225, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01225 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01106 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01068 to 0.01052, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01052 to 0.01034, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01034 to 0.00997, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00997 to 0.00945, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00945 to 0.00922, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00922 to 0.00892, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00892 to 0.00824, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00824 to 0.00786, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00786 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00732 to 0.00731, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00731 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00727 to 0.00697, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00697 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00668 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00634 to 0.00626, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00626 to 0.00612, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00612 to 0.00589, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00954, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00589 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00582 to 0.00569, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00569 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00556 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00551 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00549 to 0.00547, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00547 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00539 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00539 to 0.00533, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00538, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00169: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00565, did not improve\n",
      "Epoch 00179: early stopping\n",
      "Using epoch 00164 with val_loss: 0.00533\n",
      "88/88 [==============================] - 0s 133us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02599, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02599 to 0.02511, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.02629, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.02653, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02511 to 0.02334, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02334 to 0.02258, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02258 to 0.02167, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02167 to 0.02067, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02067 to 0.01985, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01985 to 0.01933, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01933 to 0.01810, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01810 to 0.01763, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01763 to 0.01690, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.01720, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01690 to 0.01602, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01670, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01602 to 0.01461, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01461 to 0.01421, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01462, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01421 to 0.01341, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01341 to 0.01294, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01294 to 0.01205, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01205 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01202 to 0.01154, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.01522, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01154 to 0.01115, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01115 to 0.01074, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01074 to 0.01013, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01013 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00966 to 0.00914, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00914 to 0.00879, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00879 to 0.00853, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00853 to 0.00821, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00821 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00960, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00762 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00939, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00724 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00684 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00680 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00661 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00655 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.01441, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.01015, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.01199, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00642 to 0.00636, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00636 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00618 to 0.00609, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00609 to 0.00600, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.01702, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00600 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00794, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_loss improved from 0.00599 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00558 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00556 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00548 to 0.00534, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00534 to 0.00517, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00910, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00517 to 0.00492, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00492 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00658, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00298: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00496, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00986, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00955, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00578, did not improve\n",
      "Epoch 00322: early stopping\n",
      "Using epoch 00262 with val_loss: 0.00490\n",
      "88/88 [==============================] - 0s 154us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00547] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00617]\n",
      " [ 0.00533]\n",
      " [ 0.0049 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# best solution for MLP\n",
    "best_config = {'lr': 0.37364476899503524, 'batch_size': 48}\n",
    "results = m.eval_cv('mlp', configs, Y, cfg=best_cfg, epochs=1000, splits = 3, earlystop=True, \n",
    "                    dropout=False, lr_exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139639964378880\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.05958020697008728, 'l2': 0.00012441938511358324, 'l1': 0.0010959499184547534}\n",
      "create mlp using L1L2 regularisation\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0283] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04038]\n",
      " [ 0.02392]\n",
      " [ 0.0206 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02514] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02384]\n",
      " [ 0.02672]\n",
      " [ 0.02486]]\n",
      "hyperband obj crossval results 0.0283\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 19, 'lr': 0.05982392470292955, 'l2': 0.0035279467257703485, 'l1': 0.000599326734940943}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03408] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04816]\n",
      " [ 0.02858]\n",
      " [ 0.02548]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03285] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02735]\n",
      " [ 0.03488]\n",
      " [ 0.03633]]\n",
      "hyperband obj crossval results 0.03408\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03306] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0468 ]\n",
      " [ 0.02793]\n",
      " [ 0.02445]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03256] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02721]\n",
      " [ 0.0342 ]\n",
      " [ 0.03628]]\n",
      "hyperband obj crossval results 0.03306\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.05782109870299804, 'l2': 0.0023711269546917556, 'l1': 0.001003613674878896}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03322] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04637]\n",
      " [ 0.02843]\n",
      " [ 0.02487]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03257] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02728]\n",
      " [ 0.03458]\n",
      " [ 0.03585]]\n",
      "hyperband obj crossval results 0.03322\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03233] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04368]\n",
      " [ 0.02953]\n",
      " [ 0.02378]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03104] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02667]\n",
      " [ 0.03428]\n",
      " [ 0.03216]]\n",
      "hyperband obj crossval results 0.03233\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03315] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04689]\n",
      " [ 0.02798]\n",
      " [ 0.02458]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03212] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0267 ]\n",
      " [ 0.03415]\n",
      " [ 0.03552]]\n",
      "hyperband obj crossval results 0.03315\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 27, 'lr': 0.08456542078923197, 'l2': 0.00046986297806195807, 'l1': 0.0022887938499136415}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.034] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04703]\n",
      " [ 0.0295 ]\n",
      " [ 0.02548]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03349] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02753]\n",
      " [ 0.03564]\n",
      " [ 0.03729]]\n",
      "hyperband obj crossval results 0.034\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03147] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04471]\n",
      " [ 0.0264 ]\n",
      " [ 0.0233 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03007] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02607]\n",
      " [ 0.03159]\n",
      " [ 0.03254]]\n",
      "hyperband obj crossval results 0.03147\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02576] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0388 ]\n",
      " [ 0.01976]\n",
      " [ 0.01873]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02189] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02362]\n",
      " [ 0.02112]\n",
      " [ 0.02093]]\n",
      "hyperband obj crossval results 0.02576\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02796] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04167]\n",
      " [ 0.02221]\n",
      " [ 0.02001]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02512] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02429]\n",
      " [ 0.02589]\n",
      " [ 0.02517]]\n",
      "hyperband obj crossval results 0.02796\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02099] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03013]\n",
      " [ 0.01709]\n",
      " [ 0.01576]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01721] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01898]\n",
      " [ 0.01703]\n",
      " [ 0.01562]]\n",
      "hyperband obj crossval results 0.02099\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02784] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04454]\n",
      " [ 0.01868]\n",
      " [ 0.02031]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0232] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02624]\n",
      " [ 0.0218 ]\n",
      " [ 0.02154]]\n",
      "hyperband obj crossval results 0.02784\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02776] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04287]\n",
      " [ 0.02069]\n",
      " [ 0.01971]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02463] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02577]\n",
      " [ 0.02562]\n",
      " [ 0.0225 ]]\n",
      "hyperband obj crossval results 0.02776\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01785] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02822]\n",
      " [ 0.0116 ]\n",
      " [ 0.01372]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01427] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01813]\n",
      " [ 0.01192]\n",
      " [ 0.01274]]\n",
      "hyperband obj crossval results 0.01785\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01044] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01458]\n",
      " [ 0.00804]\n",
      " [ 0.00869]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00851] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00939]\n",
      " [ 0.00818]\n",
      " [ 0.00794]]\n",
      "hyperband obj crossval results 0.01044\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.026071750020129045, 'l2': 0.0001408448332681315, 'l1': 0.0003860780520028507}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03257] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04531]\n",
      " [ 0.02806]\n",
      " [ 0.02435]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03181] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02647]\n",
      " [ 0.03368]\n",
      " [ 0.03528]]\n",
      "hyperband obj crossval results 0.03257\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0093] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01171]\n",
      " [ 0.00781]\n",
      " [ 0.00837]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00723] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00752]\n",
      " [ 0.00776]\n",
      " [ 0.00641]]\n",
      "hyperband obj crossval results 0.0093\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 27, 'lr': 0.05899350466011258, 'l2': 0.0029782618629786476, 'l1': 0.001283047465048529}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03304] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04585]\n",
      " [ 0.02838]\n",
      " [ 0.02489]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03202] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02692]\n",
      " [ 0.03408]\n",
      " [ 0.03508]]\n",
      "hyperband obj crossval results 0.03304\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.02153014369878981, 'l2': 0.0010743591528754566, 'l1': 0.0011507205935786594}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03252] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04585]\n",
      " [ 0.02745]\n",
      " [ 0.02426]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03147] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02674]\n",
      " [ 0.03322]\n",
      " [ 0.03446]]\n",
      "hyperband obj crossval results 0.03252\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02784] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04247]\n",
      " [ 0.02155]\n",
      " [ 0.01952]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02452] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02496]\n",
      " [ 0.02532]\n",
      " [ 0.0233 ]]\n",
      "hyperband obj crossval results 0.02784\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0264] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0488 ]\n",
      " [ 0.01395]\n",
      " [ 0.01645]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0193] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0285 ]\n",
      " [ 0.01448]\n",
      " [ 0.01492]]\n",
      "hyperband obj crossval results 0.0264\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0312] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04851]\n",
      " [ 0.02492]\n",
      " [ 0.02016]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.02561] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02845]\n",
      " [ 0.02678]\n",
      " [ 0.02161]]\n",
      "hyperband obj crossval results 0.0312\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01633] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01849]\n",
      " [ 0.00755]\n",
      " [ 0.02295]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01757] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01465]\n",
      " [ 0.00937]\n",
      " [ 0.02869]]\n",
      "hyperband obj crossval results 0.01633\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00989] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0129 ]\n",
      " [ 0.00807]\n",
      " [ 0.00871]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.0076] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00845]\n",
      " [ 0.00759]\n",
      " [ 0.00676]]\n",
      "hyperband obj crossval results 0.00989\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 29, 'lr': 0.08513416180024498, 'l2': 0.0022384130227352055, 'l1': 0.0027488065028525646}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03834] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04963]\n",
      " [ 0.03753]\n",
      " [ 0.02785]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.03648] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02869]\n",
      " [ 0.04216]\n",
      " [ 0.03859]]\n",
      "hyperband obj crossval results 0.03834\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00746] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00891]\n",
      " [ 0.0066 ]\n",
      " [ 0.00687]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.00542] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00572]\n",
      " [ 0.00564]\n",
      " [ 0.0049 ]]\n",
      "hyperband obj crossval results 0.00746\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}\n",
      "create mlp using L1L2 regularisation\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01425] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02365]\n",
      " [ 0.00897]\n",
      " [ 0.01013]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.01207] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01722]\n",
      " [ 0.01023]\n",
      " [ 0.00875]]\n",
      "hyperband obj crossval results 0.01425\n",
      "traj {'time_finished': [1.991438388824463, 12.025814056396484, 15.350425481796265, 20.988491535186768, 23.55812168121338, 32.916274309158325, 61.148550033569336], 'losses': [0.0283, 0.02576, 0.02099, 0.01785, 0.01044, 0.0093, 0.00746], 'budgets': [12.5, 25.0, 25.0, 50.0, 50.0, 100.0, 100.0], 'config_ids': [(0, 0, 1), (0, 0, 5), (0, 0, 1), (0, 0, 5), (0, 0, 1), (0, 0, 1), (2, 0, 2)]}\n",
      "best_cfg_id (2, 0, 2)\n",
      "all_configs {(1, 0, 3): {'config_info': {}, 'config': {'batch_size': 27, 'lr': 0.05899350466011258, 'l2': 0.0029782618629786476, 'l1': 0.001283047465048529}}, (0, 0, 7): {'config_info': {}, 'config': {'batch_size': 27, 'lr': 0.08456542078923197, 'l2': 0.00046986297806195807, 'l1': 0.0022887938499136415}}, (2, 0, 3): {'config_info': {}, 'config': {'batch_size': 29, 'lr': 0.08513416180024498, 'l2': 0.0022384130227352055, 'l1': 0.0027488065028525646}}, (0, 0, 2): {'config_info': {}, 'config': {'batch_size': 19, 'lr': 0.05982392470292955, 'l2': 0.0035279467257703485, 'l1': 0.000599326734940943}}, (1, 0, 0): {'config_info': {}, 'config': {'batch_size': 30, 'lr': 0.0810961569129174, 'l2': 0.00018096665106352795, 'l1': 0.0017700232139460103}}, (0, 0, 6): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.08263898892757952, 'l2': 0.00016048093286362305, 'l1': 0.001139678041440414}}, (2, 0, 2): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}}, (0, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.12169705572542237, 'l2': 0.002212816444365411, 'l1': 0.0003982270934210315}}, (1, 0, 1): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.14261112072023766, 'l2': 0.0011797921871477292, 'l1': 0.0017107551612273232}}, (0, 0, 5): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.1327865384217142, 'l2': 0.00015323570193598286, 'l1': 0.0009965813801526037}}, (0, 0, 0): {'config_info': {}, 'config': {'batch_size': 30, 'lr': 0.05958020697008728, 'l2': 0.00012441938511358324, 'l1': 0.0010959499184547534}}, (2, 0, 1): {'config_info': {}, 'config': {'batch_size': 32, 'lr': 0.17860301697331735, 'l2': 0.0023637164687333153, 'l1': 0.00171003574458352}}, (0, 0, 4): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.05782109870299804, 'l2': 0.0023711269546917556, 'l1': 0.001003613674878896}}, (1, 0, 2): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.026071750020129045, 'l2': 0.0001408448332681315, 'l1': 0.0003860780520028507}}, (2, 0, 0): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.02153014369878981, 'l2': 0.0010743591528754566, 'l1': 0.0011507205935786594}}, (0, 0, 3): {'config_info': {}, 'config': {'batch_size': 16, 'lr': 0.15506329149467196, 'l2': 0.00012810555119458335, 'l1': 0.0021306501327552265}}}\n",
      "return best config:  {'batch_size': 16, 'lr': 0.0987192471380652, 'l2': 0.0008048349801333865, 'l1': 0.00016294967259595808}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 10, max_budget=100, \n",
    "                       run_name='', earlystop=False, L1L2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139639964378880\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.1874018718283377}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 57305.17739 / 49320.43900\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.35682 / 1.32998\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.63600 / 1.60417\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16441.12438] ***\n",
      "Results validation data of all Folds: \n",
      "[[  4.93204390e+04]\n",
      " [  1.32998000e+00]\n",
      " [  1.60417000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 19102.7234] ***\n",
      "Results training data of all Folds: \n",
      "[[  5.73051774e+04]\n",
      " [  1.35682000e+00]\n",
      " [  1.63600000e+00]]\n",
      "hyperband obj crossval results 16441.12438\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.58379 / 6.78356\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.17556 / 1.32706\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.66769 / 5.89443\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.66835] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 6.78356]\n",
      " [ 1.32706]\n",
      " [ 5.89443]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.47568] ***\n",
      "Results training data of all Folds: \n",
      "[[ 6.58379]\n",
      " [ 1.17556]\n",
      " [ 5.66769]]\n",
      "hyperband obj crossval results 4.66835\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.06374 / 0.08929\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.94452 / 2.82466\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.88062 / 3.28236\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.06544] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.08929]\n",
      " [ 2.82466]\n",
      " [ 3.28236]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96296] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.06374]\n",
      " [ 2.94452]\n",
      " [ 2.88062]]\n",
      "hyperband obj crossval results 2.06544\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 39.15438 / 36.29156\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.85508 / 2.74689\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.94695 / 0.96315\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.33386] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 36.29156]\n",
      " [  2.74689]\n",
      " [  0.96315]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 14.3188] ***\n",
      "Results training data of all Folds: \n",
      "[[ 39.15438]\n",
      " [  2.85508]\n",
      " [  0.94695]]\n",
      "hyperband obj crossval results 13.33386\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 25, 'lr': 0.3835397084896682}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 173541.34126 / 202132.37460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1651.36476 / 1902.48543\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.59737 / 6.48495\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 68013.78166] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.02132375e+05]\n",
      " [  1.90248543e+03]\n",
      " [  6.48495000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 58399.7678] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.73541341e+05]\n",
      " [  1.65136476e+03]\n",
      " [  6.59737000e+00]]\n",
      "hyperband obj crossval results 68013.78166\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.17632548175046464}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1985.91012 / 1775.64284\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 34.40116 / 34.60897\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.14322 / 0.18056\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 603.47746] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.77564284e+03]\n",
      " [  3.46089700e+01]\n",
      " [  1.80560000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 673.48483] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98591012e+03]\n",
      " [  3.44011600e+01]\n",
      " [  1.43220000e-01]]\n",
      "hyperband obj crossval results 603.47746\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.45587 / 2.55588\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.94175 / 6.95478\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.55024 / 3.54978\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.35348] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.55588]\n",
      " [ 6.95478]\n",
      " [ 3.54978]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.31595] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.45587]\n",
      " [ 6.94175]\n",
      " [ 3.55024]]\n",
      "hyperband obj crossval results 4.35348\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 19, 'lr': 0.003469030372105559}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.83829 / 16.07803\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.03017 / 5.83519\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 25.34513 / 25.21779\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 15.71033] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 16.07803]\n",
      " [  5.83519]\n",
      " [ 25.21779]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.73786] ***\n",
      "Results training data of all Folds: \n",
      "[[ 13.83829]\n",
      " [  8.03017]\n",
      " [ 25.34513]]\n",
      "hyperband obj crossval results 15.71033\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.28430749054335513}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3472.04396 / 3523.06327\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 0.63654 / 0.69261\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.49294 / 0.53654\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1174.76414] ***\n",
      "Results validation data of all Folds: \n",
      "[[  3.52306327e+03]\n",
      " [  6.92610000e-01]\n",
      " [  5.36540000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1157.72448] ***\n",
      "Results training data of all Folds: \n",
      "[[  3.47204396e+03]\n",
      " [  6.36540000e-01]\n",
      " [  4.92940000e-01]]\n",
      "hyperband obj crossval results 1174.76414\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.07994 / 5.19139\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98066 / 1.25787\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.47991 / 2.00694\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.81873] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19139]\n",
      " [ 1.25787]\n",
      " [ 2.00694]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.51351] ***\n",
      "Results training data of all Folds: \n",
      "[[ 3.07994]\n",
      " [ 1.98066]\n",
      " [ 2.47991]]\n",
      "hyperband obj crossval results 2.81873\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 53.63178 / 49.70385\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 27.54559 / 24.83507\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.28858 / 1.20521\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 25.24804] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 49.70385]\n",
      " [ 24.83507]\n",
      " [  1.20521]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 27.48865] ***\n",
      "Results training data of all Folds: \n",
      "[[ 53.63178]\n",
      " [ 27.54559]\n",
      " [  1.28858]]\n",
      "hyperband obj crossval results 25.24804\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.78026 / 2.92213\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05201 / 0.04299\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.33999 / 1.72137\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.56216] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.92213]\n",
      " [ 0.04299]\n",
      " [ 1.72137]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.39075] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.78026]\n",
      " [ 0.05201]\n",
      " [ 2.33999]]\n",
      "hyperband obj crossval results 1.56216\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 37.93051 / 35.21599\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 14.08750 / 11.14776\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 10.54999 / 8.86799\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 18.41058] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 35.21599]\n",
      " [ 11.14776]\n",
      " [  8.86799]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 20.856] ***\n",
      "Results training data of all Folds: \n",
      "[[ 37.93051]\n",
      " [ 14.0875 ]\n",
      " [ 10.54999]]\n",
      "hyperband obj crossval results 18.41058\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 44, 'lr': 0.4913996332060787}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1575585.33856 / 1698166.62906\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1294.57183 / 1544.12482\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.51372 / 2.53258\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 566571.09549] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.69816663e+06]\n",
      " [  1.54412482e+03]\n",
      " [  2.53258000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 525627.4747] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.57558534e+06]\n",
      " [  1.29457183e+03]\n",
      " [  2.51372000e+00]]\n",
      "hyperband obj crossval results 566571.09549\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.95017 / 1.32579\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04080 / 0.03309\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04376 / 0.02759\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.46215] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.32579]\n",
      " [ 0.03309]\n",
      " [ 0.02759]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34491] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.95017]\n",
      " [ 0.0408 ]\n",
      " [ 0.04376]]\n",
      "hyperband obj crossval results 0.46215\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.62112 / 4.18508\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04926 / 0.04060\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04391 / 0.02767\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.41778] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 4.18508]\n",
      " [ 0.0406 ]\n",
      " [ 0.02767]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.90476] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.62112]\n",
      " [ 0.04926]\n",
      " [ 0.04391]]\n",
      "hyperband obj crossval results 1.41778\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.30579 / 17.29121\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.52870 / 9.34835\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.08090 / 13.25739\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.29898] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 17.29121]\n",
      " [  9.34835]\n",
      " [ 13.25739]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 13.63846] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.30579]\n",
      " [ 11.5287 ]\n",
      " [ 13.0809 ]]\n",
      "hyperband obj crossval results 13.29898\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.37079 / 0.41493\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04794 / 0.03834\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.68127 / 4.39918\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.61748] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.41493]\n",
      " [ 0.03834]\n",
      " [ 4.39918]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.03333] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.37079]\n",
      " [ 0.04794]\n",
      " [ 5.68127]]\n",
      "hyperband obj crossval results 1.61748\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 853.34644 / 905.47774\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.08386 / 0.08086\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05171 / 0.05630\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 301.87164] ***\n",
      "Results validation data of all Folds: \n",
      "[[  9.05477740e+02]\n",
      " [  8.08600000e-02]\n",
      " [  5.63000000e-02]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 284.494] ***\n",
      "Results training data of all Folds: \n",
      "[[  8.53346440e+02]\n",
      " [  8.38600000e-02]\n",
      " [  5.17100000e-02]]\n",
      "hyperband obj crossval results 301.87164\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.45195 / 0.61088\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04009 / 0.03219\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.13532 / 0.25532\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.29946] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.61088]\n",
      " [ 0.03219]\n",
      " [ 0.25532]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.20912] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.45195]\n",
      " [ 0.04009]\n",
      " [ 0.13532]]\n",
      "hyperband obj crossval results 0.29946\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.79849 / 5.19062\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04337 / 0.04290\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04351 / 0.04174\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.75842] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19062]\n",
      " [ 0.0429 ]\n",
      " [ 0.04174]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96179] ***\n",
      "Results training data of all Folds: \n",
      "[[ 5.79849]\n",
      " [ 0.04337]\n",
      " [ 0.04351]]\n",
      "hyperband obj crossval results 1.75842\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.55753 / 20.22860\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.72184 / 6.69817\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.85380 / 4.63014\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 10.51897] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 20.2286 ]\n",
      " [  6.69817]\n",
      " [  4.63014]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 10.71106] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.55753]\n",
      " [  8.72184]\n",
      " [  6.8538 ]]\n",
      "hyperband obj crossval results 10.51897\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.0012756729206255175}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 88.28766 / 88.52939\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 9.23875 / 6.21818\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 15.96105 / 11.43095\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 35.39284] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 88.52939]\n",
      " [  6.21818]\n",
      " [ 11.43095]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 37.82915] ***\n",
      "Results training data of all Folds: \n",
      "[[ 88.28766]\n",
      " [  9.23875]\n",
      " [ 15.96105]]\n",
      "hyperband obj crossval results 35.39284\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 38.78400 / 41.18006\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.46152 / 2.47339\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 4.26055 / 4.35291\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16.00212] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 41.18006]\n",
      " [  2.47339]\n",
      " [  4.35291]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.16869] ***\n",
      "Results training data of all Folds: \n",
      "[[ 38.784  ]\n",
      " [  2.46152]\n",
      " [  4.26055]]\n",
      "hyperband obj crossval results 16.00212\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.001051445164047007}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 50.41666 / 48.16330\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.99796 / 9.56694\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.71480 / 8.87737\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 22.20254] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 48.1633 ]\n",
      " [  9.56694]\n",
      " [  8.87737]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 25.37647] ***\n",
      "Results training data of all Folds: \n",
      "[[ 50.41666]\n",
      " [ 13.99796]\n",
      " [ 11.7148 ]]\n",
      "hyperband obj crossval results 22.20254\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.84544 / 1.95491\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.26675 / 0.27446\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 1.42394 / 1.42303\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.21747] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.95491]\n",
      " [ 0.27446]\n",
      " [ 1.42303]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.17871] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.84544]\n",
      " [ 0.26675]\n",
      " [ 1.42394]]\n",
      "hyperband obj crossval results 1.21747\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98941 / 1.91761\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.48118 / 6.42872\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 18.31786 / 18.51387\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 8.9534] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.91761]\n",
      " [  6.42872]\n",
      " [ 18.51387]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 8.92948] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98941]\n",
      " [  6.48118]\n",
      " [ 18.31786]]\n",
      "hyperband obj crossval results 8.9534\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.80275 / 1.05999\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05447 / 0.04670\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.19026 / 0.20350\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.43673] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.05999]\n",
      " [ 0.0467 ]\n",
      " [ 0.2035 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34916] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.80275]\n",
      " [ 0.05447]\n",
      " [ 0.19026]]\n",
      "hyperband obj crossval results 0.43673\n",
      "traj {'time_finished': [22.488330125808716, 60.9959557056427, 91.6520767211914, 313.71562457084656, 390.30302476882935, 627.3354201316833], 'losses': [16441.12438, 4.66835, 2.06544, 1.56216, 0.46215, 0.29946], 'budgets': [5.0, 5.0, 5.0, 10.0, 20.0, 20.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 1), (0, 0, 2), (2, 0, 0)]}\n",
      "best_cfg_id (2, 0, 0)\n",
      "all_configs {(1, 0, 3): {'config_info': {}, 'config': {'batch_size': 26, 'lr': 0.1579507660887153}}, (0, 0, 7): {'config_info': {}, 'config': {'batch_size': 19, 'lr': 0.003469030372105559}}, (2, 0, 3): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.06641032495399549}}, (0, 0, 2): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.0076099528036695455}}, (1, 0, 0): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.28430749054335513}}, (0, 0, 6): {'config_info': {}, 'config': {'batch_size': 22, 'lr': 0.0010424757895254506}}, (2, 0, 2): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.001051445164047007}}, (0, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.007725817805663781}}, (1, 0, 1): {'config_info': {}, 'config': {'batch_size': 44, 'lr': 0.4913996332060787}}, (0, 0, 5): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.17632548175046464}}, (0, 0, 0): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.1874018718283377}}, (2, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.0012756729206255175}}, (0, 0, 4): {'config_info': {}, 'config': {'batch_size': 25, 'lr': 0.3835397084896682}}, (1, 0, 2): {'config_info': {}, 'config': {'batch_size': 40, 'lr': 0.002370731368911436}}, (2, 0, 0): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.023123758972112808}}, (0, 0, 3): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.001921554114714757}}}\n",
      "return best config:  {'batch_size': 21, 'lr': 0.023123758972112808}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize([configs,lcs], Y, model_type='multi_lstm', \n",
    "                       min_budget = 4, max_budget=40, run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15458, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15458 to 0.09678, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09678 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.03820, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02604 to 0.01416, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01416 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01231 to 0.00266, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00400, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00266 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00153 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00336, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00143 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00132 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00126 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00122 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00116 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00108 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00105 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00102 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00095 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00087 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00079 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00069 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00065 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00052 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00048 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00066, did not improve\n",
      "Epoch 00144: early stopping\n",
      "Using epoch 00086 with val_loss: 0.00045\n",
      "validate on 30 steps, mse on train / validation data: 0.17828 / 0.28994\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.90476, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 1361.45895, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 510.44960, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 255.13004, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 54.13024, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 21.52839, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.90476 to 4.37317, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.37317 to 1.53281, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.53281 to 0.45816, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45816 to 0.10284, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10284 to 0.10064, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10064 to 0.03229, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03229 to 0.01450, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01450 to 0.00979, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00979 to 0.00822, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00822 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00662 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00488 to 0.00424, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00424 to 0.00388, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00388 to 0.00365, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00365 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00335 to 0.00313, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00313 to 0.00293, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00293 to 0.00274, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00274 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00257 to 0.00243, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00243 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00231 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00220 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00210 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00201 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00193 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00186 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00179 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00172 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00166 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00161 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00157 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00154 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00151 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00148 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00145 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00143 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00140 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00138 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00136 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00134 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00132 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00129 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00126 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00121 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00115 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00105 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00080 to 0.00079, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00136, did not improve\n",
      "Epoch 00184: early stopping\n",
      "Using epoch 00134 with val_loss: 0.00069\n",
      "validate on 30 steps, mse on train / validation data: 1.26030 / 1.59993\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.09429, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.03942, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03170 to 0.02370, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02402, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02370 to 0.02337, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02337 to 0.01970, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01970 to 0.01745, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01745 to 0.01647, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01647 to 0.01517, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01517 to 0.01348, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01348 to 0.01201, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01201 to 0.01066, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01066 to 0.00930, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00930 to 0.00798, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00798 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00680 to 0.00574, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00574 to 0.00484, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00484 to 0.00409, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00409 to 0.00350, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00350 to 0.00307, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00307 to 0.00276, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00276 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00257 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00245 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00239 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00235 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00234 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00234 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00233 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00232 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00232 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00231 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00230 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00230 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00228 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00226 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00226 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00225 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00225 to 0.00224, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_loss improved from 0.00224 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00223 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00222 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00220 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00220 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00219 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00219 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00218 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00217 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00216 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00215 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00215 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00214 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00214 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00213 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00213 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00212 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00211 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00210 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00210 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00209 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00209 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00208 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00208 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00206 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00206 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00205 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00205 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00204 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00201 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00200 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00197 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00196 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00193 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00192 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00176 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00174 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00171 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00169 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00168 to 0.00168, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00159: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00166 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00116 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00139, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.00099 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00383: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00070, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00405: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00077, did not improve\n",
      "Epoch 00432: early stopping\n",
      "Using epoch 00382 with val_loss: 0.00068\n",
      "validate on 30 steps, mse on train / validation data: 0.13800 / 0.08938\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.73713  0.79544  0.92657  0.65975] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.70987  0.61491  0.50921  0.28994]\n",
      " [ 1.45444  1.73461  2.25349  1.59993]\n",
      " [ 0.04709  0.03681  0.01701  0.08938]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.54098  0.66638  0.72073  0.52552] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.52994  0.51729  0.3413   0.17828]\n",
      " [ 1.02344  1.42766  1.79191  1.2603 ]\n",
      " [ 0.06957  0.0542   0.02898  0.138  ]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11671, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12661, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11671 to 0.05024, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05024 to 0.02909, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.03532, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02909 to 0.00954, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00954 to 0.00902, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00902 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00634 to 0.00631, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00631 to 0.00496, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00496 to 0.00286, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00286 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00172 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00073 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00052 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00047 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00506, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00044 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00040, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00416, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00640, did not improve\n",
      "Epoch 00133: early stopping\n",
      "Using epoch 00094 with val_loss: 0.00038\n",
      "validate on 30 steps, mse on train / validation data: 0.01650 / 0.01693\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 378.56074, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 378.56074 to 30.09993, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 34.44132, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 30.09993 to 3.21959, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 4.52868, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.21959 to 0.70476, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 1.93981, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70476 to 0.47590, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47590 to 0.15103, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15103 to 0.06758, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06758 to 0.05877, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05877 to 0.03435, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03435 to 0.03089, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03089 to 0.03066, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03066 to 0.02833, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.02988, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02833 to 0.02818, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.02889, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02818 to 0.02816, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02824, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02830, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00019 with val_loss: 0.02816\n",
      "validate on 30 steps, mse on train / validation data: 0.03174 / 0.02943\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04151 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01769 to 0.00459, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00459 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00235 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00191 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00095 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00046 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00039 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00033 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00029, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00029, did not improve\n",
      "Epoch 00085: early stopping\n",
      "Using epoch 00056 with val_loss: 0.00029\n",
      "validate on 30 steps, mse on train / validation data: 0.01208 / 0.00941\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.08369  0.04828  0.03323  0.01859] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.1925   0.08568  0.04499  0.01693]\n",
      " [ 0.02943  0.02943  0.02943  0.02943]\n",
      " [ 0.02916  0.02972  0.02526  0.00941]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.07582  0.05066  0.03704  0.02011] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.14894  0.07332  0.04103  0.0165 ]\n",
      " [ 0.03174  0.03174  0.03174  0.03174]\n",
      " [ 0.04678  0.04693  0.03834  0.01208]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32145, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32145 to 0.12631, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12631 to 0.10577, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10577 to 0.06052, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06052 to 0.01204, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.01487, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01204 to 0.00730, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00730 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00380, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00228 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00097 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00063 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00054 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00077, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00222, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00017 with val_loss: 0.00048\n",
      "validate on 30 steps, mse on train / validation data: 0.00203 / 0.00196\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05475, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12115, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.08707, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05475 to 0.05174, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05174 to 0.04611, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04611 to 0.02661, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02661 to 0.01495, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01495 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00808 to 0.00483, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00483 to 0.00260, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00260 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00112 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00054 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00045 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00025, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00089 with val_loss: 0.00025\n",
      "validate on 30 steps, mse on train / validation data: 0.00131 / 0.00129\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51855, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51855 to 0.09694, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss is 14.75946, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 275.30684, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 13.29729, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 8.29154, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.53958, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 10.31797, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 1.31900, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.25212, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.24620, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09694 to 0.05898, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05898 to 0.04390, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04390 to 0.03132, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.03260, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03132 to 0.02776, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.02879, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02776 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02769, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02754 to 0.02724, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.02757, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02750, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02734, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02735, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02732, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02730, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02727, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02725, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02724 to 0.02722, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02722 to 0.02720, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02720 to 0.02718, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02718 to 0.02716, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02716 to 0.02714, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.02714 to 0.02711, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02711 to 0.02709, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.02709 to 0.02707, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.02707 to 0.02705, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.02705 to 0.02703, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.02703 to 0.02701, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.02701 to 0.02699, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.02699 to 0.02697, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02697 to 0.02696, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.02696 to 0.02694, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.02694 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.02692 to 0.02690, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.02690 to 0.02689, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.02689 to 0.02687, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.02687 to 0.02686, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02686 to 0.02684, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.02684 to 0.02683, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02683 to 0.02681, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02681 to 0.02680, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02680 to 0.02679, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02679 to 0.02678, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02678 to 0.02677, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02677 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02676 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02676 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02677, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02678, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02680, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02681, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02683, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02686, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02688, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02691, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02695, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02698, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02703, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02707, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02712, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02718, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02724, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02731, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02756, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02765, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02775, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02787, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02799, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02812, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.02857, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.02874, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.02892, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.02912, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.02933, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.02955, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.02979, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.03003, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.03030, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.03058, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.03087, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.03118, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.03151, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.03185, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.03220, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.03257, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.03296, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.03336, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.03377, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.03420, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.03463, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.03508, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.03554, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.03601, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.03648, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.03696, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.03744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.03792, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.03840, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.03888, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.03936, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.03983, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.04029, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.04074, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.04118, did not improve\n",
      "Epoch 00129: early stopping\n",
      "Using epoch 00064 with val_loss: 0.02675\n",
      "validate on 30 steps, mse on train / validation data: 0.02955 / 0.02495\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02429  0.01109  0.0104   0.0094 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04105  0.00511  0.00295  0.00196]\n",
      " [ 0.00689  0.00322  0.00331  0.00129]\n",
      " [ 0.02495  0.02495  0.02495  0.02495]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0211   0.01247  0.01185  0.01096] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02463  0.00444  0.00319  0.00203]\n",
      " [ 0.00913  0.00341  0.00281  0.00131]\n",
      " [ 0.02955  0.02955  0.02955  0.02955]]\n",
      "results validation data \n",
      " [[ 0.73713  0.79544  0.92657  0.65975]\n",
      " [ 0.08369  0.04828  0.03323  0.01859]\n",
      " [ 0.02429  0.01109  0.0104   0.0094 ]]\n",
      "results training data\n",
      " [[ 0.54098  0.66638  0.72073  0.52552]\n",
      " [ 0.07582  0.05066  0.03704  0.02011]\n",
      " [ 0.0211   0.01247  0.01185  0.01096]]\n"
     ]
    }
   ],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=best_cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)\n",
    "# results were far better with lr = 0.002 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                     mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "build lstm with input_dim: 6\n",
      "Train on 200 samples, validate on 65 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0743 - mean_squared_error: 0.0743 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "65/65 [==============================] - 0s 843us/step\n",
      "mse:  0.00873059442697\n"
     ]
    }
   ],
   "source": [
    "# experiment with concatenating the config to each data point of learning curve\n",
    "timesteps = 5\n",
    "configs,lcs,Y = t.load_lstm_data_concat_cfg(timesteps=timesteps)\n",
    "model_type = 'lstm'\n",
    "model = m.lstm(lcs[0][0].shape[0])\n",
    "m.train_lstm(model, lcs, Y, split=200, batch_size=20, epochs=5)\n",
    "mse = m.eval_lstm(model, lcs, Y, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_curves' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-b1ef2fff93a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_curves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subset of learning curves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_curves' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_subset=20\n",
    "n_epochs = 40\n",
    "t_idx = np.arange(1, n_epochs+1)\n",
    "\n",
    "[plt.plot(t_idx, lc) for lc in learning_curves[:n_subset]]\n",
    "plt.title(\"Subset of learning curves\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over the final error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf648b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effdf6cbbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted = np.sort(lcs[:, -1])   # sorted list of final val error\n",
    "print(len(sorted))\n",
    "h = plt.hist(sorted, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(len(sorted))/float(len(sorted))   # from 0 to 1 in 265 even steps\n",
    "plt.plot(sorted, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over all error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4935a3ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493581f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_values = np.sort(learning_curves.flatten())\n",
    "\n",
    "h = plt.hist(all_values, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(all_values.shape[0])/all_values.shape[0]\n",
    "plt.plot(all_values, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
