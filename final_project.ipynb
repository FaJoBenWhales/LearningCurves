{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Learning Curves of Convolutional Neural Network on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tools as t\n",
    "import models as m\n",
    "import hyperband as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n"
     ]
    }
   ],
   "source": [
    "configs,lcs,Y = t.load_data(scale_configs = True)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Testing models (mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177 samples, validate on 88 samples\n",
      "Epoch 1/200\n",
      "177/177 [==============================] - 0s 1ms/step - loss: 0.0570 - val_loss: 0.0263\n",
      "Epoch 2/200\n",
      "177/177 [==============================] - 0s 183us/step - loss: 0.0388 - val_loss: 0.0259\n",
      "Epoch 3/200\n",
      "177/177 [==============================] - 0s 296us/step - loss: 0.0384 - val_loss: 0.0260\n",
      "Epoch 4/200\n",
      "177/177 [==============================] - 0s 294us/step - loss: 0.0393 - val_loss: 0.0290\n",
      "Epoch 5/200\n",
      "177/177 [==============================] - 0s 289us/step - loss: 0.0373 - val_loss: 0.0248\n",
      "Epoch 6/200\n",
      "177/177 [==============================] - 0s 308us/step - loss: 0.0357 - val_loss: 0.0245\n",
      "Epoch 7/200\n",
      "177/177 [==============================] - 0s 407us/step - loss: 0.0359 - val_loss: 0.0267\n",
      "Epoch 8/200\n",
      "177/177 [==============================] - 0s 341us/step - loss: 0.0342 - val_loss: 0.0237\n",
      "Epoch 9/200\n",
      "177/177 [==============================] - 0s 389us/step - loss: 0.0344 - val_loss: 0.0235\n",
      "Epoch 10/200\n",
      "177/177 [==============================] - 0s 492us/step - loss: 0.0335 - val_loss: 0.0260\n",
      "Epoch 11/200\n",
      "177/177 [==============================] - 0s 315us/step - loss: 0.0329 - val_loss: 0.0226\n",
      "Epoch 12/200\n",
      "177/177 [==============================] - 0s 271us/step - loss: 0.0309 - val_loss: 0.0238\n",
      "Epoch 13/200\n",
      "177/177 [==============================] - 0s 387us/step - loss: 0.0314 - val_loss: 0.0220\n",
      "Epoch 14/200\n",
      "177/177 [==============================] - 0s 323us/step - loss: 0.0300 - val_loss: 0.0219\n",
      "Epoch 15/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0294 - val_loss: 0.0216\n",
      "Epoch 16/200\n",
      "177/177 [==============================] - 0s 314us/step - loss: 0.0286 - val_loss: 0.0211\n",
      "Epoch 17/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0277 - val_loss: 0.0212\n",
      "Epoch 18/200\n",
      "177/177 [==============================] - 0s 497us/step - loss: 0.0273 - val_loss: 0.0206\n",
      "Epoch 19/200\n",
      "177/177 [==============================] - 0s 405us/step - loss: 0.0266 - val_loss: 0.0199\n",
      "Epoch 20/200\n",
      "177/177 [==============================] - 0s 337us/step - loss: 0.0250 - val_loss: 0.0194\n",
      "Epoch 21/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0247 - val_loss: 0.0196\n",
      "Epoch 22/200\n",
      "177/177 [==============================] - 0s 269us/step - loss: 0.0240 - val_loss: 0.0186\n",
      "Epoch 23/200\n",
      "177/177 [==============================] - 0s 394us/step - loss: 0.0232 - val_loss: 0.0185\n",
      "Epoch 24/200\n",
      "177/177 [==============================] - 0s 282us/step - loss: 0.0226 - val_loss: 0.0184\n",
      "Epoch 25/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0222 - val_loss: 0.0176\n",
      "Epoch 26/200\n",
      "177/177 [==============================] - 0s 300us/step - loss: 0.0219 - val_loss: 0.0180\n",
      "Epoch 27/200\n",
      "177/177 [==============================] - 0s 326us/step - loss: 0.0197 - val_loss: 0.0166\n",
      "Epoch 28/200\n",
      "177/177 [==============================] - 0s 583us/step - loss: 0.0190 - val_loss: 0.0166\n",
      "Epoch 29/200\n",
      "177/177 [==============================] - 0s 254us/step - loss: 0.0187 - val_loss: 0.0173\n",
      "Epoch 30/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0194 - val_loss: 0.0167\n",
      "Epoch 31/200\n",
      "177/177 [==============================] - 0s 322us/step - loss: 0.0181 - val_loss: 0.0182\n",
      "Epoch 32/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0174 - val_loss: 0.0154\n",
      "Epoch 33/200\n",
      "177/177 [==============================] - 0s 376us/step - loss: 0.0172 - val_loss: 0.0154\n",
      "Epoch 34/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0164 - val_loss: 0.0154\n",
      "Epoch 35/200\n",
      "177/177 [==============================] - 0s 317us/step - loss: 0.0165 - val_loss: 0.0150\n",
      "Epoch 36/200\n",
      "177/177 [==============================] - 0s 265us/step - loss: 0.0165 - val_loss: 0.0159\n",
      "Epoch 37/200\n",
      "177/177 [==============================] - 0s 422us/step - loss: 0.0166 - val_loss: 0.0147\n",
      "Epoch 38/200\n",
      "177/177 [==============================] - 0s 380us/step - loss: 0.0145 - val_loss: 0.0153\n",
      "Epoch 39/200\n",
      "177/177 [==============================] - 0s 311us/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 40/200\n",
      "177/177 [==============================] - 0s 363us/step - loss: 0.0130 - val_loss: 0.0145\n",
      "Epoch 41/200\n",
      "177/177 [==============================] - 0s 313us/step - loss: 0.0130 - val_loss: 0.0141\n",
      "Epoch 42/200\n",
      "177/177 [==============================] - 0s 237us/step - loss: 0.0118 - val_loss: 0.0145\n",
      "Epoch 43/200\n",
      "177/177 [==============================] - 0s 406us/step - loss: 0.0140 - val_loss: 0.0133\n",
      "Epoch 44/200\n",
      "177/177 [==============================] - 0s 251us/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 45/200\n",
      "177/177 [==============================] - 0s 220us/step - loss: 0.0118 - val_loss: 0.0131\n",
      "Epoch 46/200\n",
      "177/177 [==============================] - 0s 427us/step - loss: 0.0105 - val_loss: 0.0153\n",
      "Epoch 47/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0109 - val_loss: 0.0145\n",
      "Epoch 48/200\n",
      "177/177 [==============================] - 0s 413us/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 49/200\n",
      "177/177 [==============================] - 0s 325us/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 50/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 51/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 52/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0098 - val_loss: 0.0117\n",
      "Epoch 53/200\n",
      "177/177 [==============================] - 0s 442us/step - loss: 0.0130 - val_loss: 0.0115\n",
      "Epoch 54/200\n",
      "177/177 [==============================] - 0s 309us/step - loss: 0.0090 - val_loss: 0.0122\n",
      "Epoch 55/200\n",
      "177/177 [==============================] - 0s 298us/step - loss: 0.0101 - val_loss: 0.0117\n",
      "Epoch 56/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0106 - val_loss: 0.0121\n",
      "Epoch 57/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0102 - val_loss: 0.0119\n",
      "Epoch 58/200\n",
      "177/177 [==============================] - 0s 395us/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 59/200\n",
      "177/177 [==============================] - 0s 319us/step - loss: 0.0091 - val_loss: 0.0134\n",
      "Epoch 60/200\n",
      "177/177 [==============================] - 0s 443us/step - loss: 0.0097 - val_loss: 0.0122\n",
      "Epoch 61/200\n",
      "177/177 [==============================] - 0s 314us/step - loss: 0.0091 - val_loss: 0.0114\n",
      "Epoch 62/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0080 - val_loss: 0.0108\n",
      "Epoch 63/200\n",
      "177/177 [==============================] - 0s 190us/step - loss: 0.0084 - val_loss: 0.0108\n",
      "Epoch 64/200\n",
      "177/177 [==============================] - 0s 366us/step - loss: 0.0086 - val_loss: 0.0104\n",
      "Epoch 65/200\n",
      "177/177 [==============================] - 0s 330us/step - loss: 0.0086 - val_loss: 0.0128\n",
      "Epoch 66/200\n",
      "177/177 [==============================] - 0s 260us/step - loss: 0.0074 - val_loss: 0.0112\n",
      "Epoch 67/200\n",
      "177/177 [==============================] - 0s 358us/step - loss: 0.0085 - val_loss: 0.0111\n",
      "Epoch 68/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0086 - val_loss: 0.0104\n",
      "Epoch 69/200\n",
      "177/177 [==============================] - 0s 232us/step - loss: 0.0080 - val_loss: 0.0118\n",
      "Epoch 70/200\n",
      "177/177 [==============================] - 0s 363us/step - loss: 0.0075 - val_loss: 0.0099\n",
      "Epoch 71/200\n",
      "177/177 [==============================] - 0s 274us/step - loss: 0.0085 - val_loss: 0.0142\n",
      "Epoch 72/200\n",
      "177/177 [==============================] - 0s 277us/step - loss: 0.0082 - val_loss: 0.0099\n",
      "Epoch 73/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0073 - val_loss: 0.0101\n",
      "Epoch 74/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.006 - 0s 324us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 75/200\n",
      "177/177 [==============================] - 0s 390us/step - loss: 0.0070 - val_loss: 0.0104\n",
      "Epoch 76/200\n",
      "177/177 [==============================] - 0s 308us/step - loss: 0.0070 - val_loss: 0.0115\n",
      "Epoch 77/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0077 - val_loss: 0.0094\n",
      "Epoch 78/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 79/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 80/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0070 - val_loss: 0.0094\n",
      "Epoch 81/200\n",
      "177/177 [==============================] - 0s 282us/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 82/200\n",
      "177/177 [==============================] - 0s 433us/step - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 83/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0059 - val_loss: 0.0099\n",
      "Epoch 84/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0061 - val_loss: 0.0096\n",
      "Epoch 85/200\n",
      "177/177 [==============================] - 0s 286us/step - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 86/200\n",
      "177/177 [==============================] - 0s 248us/step - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 87/200\n",
      "177/177 [==============================] - 0s 432us/step - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 88/200\n",
      "177/177 [==============================] - 0s 252us/step - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 89/200\n",
      "177/177 [==============================] - 0s 353us/step - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 90/200\n",
      "177/177 [==============================] - 0s 252us/step - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 91/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0052 - val_loss: 0.0101\n",
      "Epoch 92/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 93/200\n",
      "177/177 [==============================] - 0s 289us/step - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 94/200\n",
      "177/177 [==============================] - 0s 329us/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 95/200\n",
      "177/177 [==============================] - 0s 303us/step - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 96/200\n",
      "177/177 [==============================] - 0s 357us/step - loss: 0.0054 - val_loss: 0.0087\n",
      "Epoch 97/200\n",
      "177/177 [==============================] - 0s 312us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 98/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 99/200\n",
      "177/177 [==============================] - 0s 307us/step - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 100/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0048 - val_loss: 0.0086\n",
      "Epoch 101/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.003 - 0s 411us/step - loss: 0.0050 - val_loss: 0.0091\n",
      "Epoch 102/200\n",
      "177/177 [==============================] - 0s 297us/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 103/200\n",
      "177/177 [==============================] - 0s 530us/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 104/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0047 - val_loss: 0.0092\n",
      "Epoch 105/200\n",
      "177/177 [==============================] - 0s 388us/step - loss: 0.0057 - val_loss: 0.0108\n",
      "Epoch 106/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 107/200\n",
      "177/177 [==============================] - 0s 325us/step - loss: 0.0048 - val_loss: 0.0102\n",
      "Epoch 108/200\n",
      "177/177 [==============================] - 0s 344us/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 109/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0048 - val_loss: 0.0084\n",
      "Epoch 110/200\n",
      "177/177 [==============================] - 0s 403us/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Epoch 111/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0044 - val_loss: 0.0086\n",
      "Epoch 112/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 113/200\n",
      "177/177 [==============================] - 0s 292us/step - loss: 0.0042 - val_loss: 0.0084\n",
      "Epoch 114/200\n",
      "177/177 [==============================] - 0s 324us/step - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 115/200\n",
      "177/177 [==============================] - 0s 399us/step - loss: 0.0042 - val_loss: 0.0083\n",
      "Epoch 116/200\n",
      "177/177 [==============================] - 0s 272us/step - loss: 0.0052 - val_loss: 0.0094\n",
      "Epoch 117/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0046 - val_loss: 0.0074\n",
      "Epoch 118/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0044 - val_loss: 0.0076\n",
      "Epoch 119/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0045 - val_loss: 0.0080\n",
      "Epoch 120/200\n",
      "177/177 [==============================] - 0s 360us/step - loss: 0.0042 - val_loss: 0.0092\n",
      "Epoch 121/200\n",
      "177/177 [==============================] - 0s 313us/step - loss: 0.0043 - val_loss: 0.0079\n",
      "Epoch 122/200\n",
      "177/177 [==============================] - 0s 240us/step - loss: 0.0045 - val_loss: 0.0082\n",
      "Epoch 123/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0039 - val_loss: 0.0089\n",
      "Epoch 124/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0044 - val_loss: 0.0078\n",
      "Epoch 125/200\n",
      "177/177 [==============================] - 0s 342us/step - loss: 0.0039 - val_loss: 0.0084\n",
      "Epoch 126/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0039 - val_loss: 0.0080\n",
      "Epoch 127/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.005 - 0s 394us/step - loss: 0.0042 - val_loss: 0.0104\n",
      "Epoch 128/200\n",
      "177/177 [==============================] - 0s 259us/step - loss: 0.0041 - val_loss: 0.0082\n",
      "Epoch 129/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0041 - val_loss: 0.0081\n",
      "Epoch 130/200\n",
      "177/177 [==============================] - 0s 225us/step - loss: 0.0041 - val_loss: 0.0077\n",
      "Epoch 131/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0041 - val_loss: 0.0080\n",
      "Epoch 132/200\n",
      "177/177 [==============================] - 0s 523us/step - loss: 0.0042 - val_loss: 0.0077\n",
      "Epoch 133/200\n",
      "177/177 [==============================] - 0s 231us/step - loss: 0.0042 - val_loss: 0.0087\n",
      "Epoch 134/200\n",
      "177/177 [==============================] - 0s 382us/step - loss: 0.0039 - val_loss: 0.0071\n",
      "Epoch 135/200\n",
      "177/177 [==============================] - 0s 263us/step - loss: 0.0040 - val_loss: 0.0078\n",
      "Epoch 136/200\n",
      "177/177 [==============================] - 0s 328us/step - loss: 0.0040 - val_loss: 0.0075\n",
      "Epoch 137/200\n",
      "177/177 [==============================] - 0s 321us/step - loss: 0.0039 - val_loss: 0.0069\n",
      "Epoch 138/200\n",
      "177/177 [==============================] - 0s 323us/step - loss: 0.0036 - val_loss: 0.0072\n",
      "Epoch 139/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0036 - val_loss: 0.0089\n",
      "Epoch 140/200\n",
      "177/177 [==============================] - 0s 278us/step - loss: 0.0048 - val_loss: 0.0075\n",
      "Epoch 141/200\n",
      "177/177 [==============================] - 0s 366us/step - loss: 0.0035 - val_loss: 0.0072\n",
      "Epoch 142/200\n",
      "177/177 [==============================] - 0s 205us/step - loss: 0.0037 - val_loss: 0.0079\n",
      "Epoch 143/200\n",
      "177/177 [==============================] - 0s 467us/step - loss: 0.0037 - val_loss: 0.0083\n",
      "Epoch 144/200\n",
      "177/177 [==============================] - 0s 332us/step - loss: 0.0038 - val_loss: 0.0086\n",
      "Epoch 145/200\n",
      "177/177 [==============================] - 0s 326us/step - loss: 0.0035 - val_loss: 0.0076\n",
      "Epoch 146/200\n",
      "177/177 [==============================] - 0s 196us/step - loss: 0.0036 - val_loss: 0.0078\n",
      "Epoch 147/200\n",
      "177/177 [==============================] - 0s 195us/step - loss: 0.0046 - val_loss: 0.0076\n",
      "Epoch 148/200\n",
      "177/177 [==============================] - 0s 353us/step - loss: 0.0035 - val_loss: 0.0076\n",
      "Epoch 149/200\n",
      "177/177 [==============================] - 0s 266us/step - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 150/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0032 - val_loss: 0.0072\n",
      "Epoch 151/200\n",
      "177/177 [==============================] - 0s 304us/step - loss: 0.0034 - val_loss: 0.0068\n",
      "Epoch 152/200\n",
      "177/177 [==============================] - 0s 352us/step - loss: 0.0034 - val_loss: 0.0073\n",
      "Epoch 153/200\n",
      "177/177 [==============================] - 0s 295us/step - loss: 0.0042 - val_loss: 0.0069\n",
      "Epoch 154/200\n",
      "177/177 [==============================] - 0s 287us/step - loss: 0.0031 - val_loss: 0.0071\n",
      "Epoch 155/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0035 - val_loss: 0.0082\n",
      "Epoch 156/200\n",
      "177/177 [==============================] - 0s 250us/step - loss: 0.0034 - val_loss: 0.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "177/177 [==============================] - 0s 419us/step - loss: 0.0038 - val_loss: 0.0068\n",
      "Epoch 158/200\n",
      "177/177 [==============================] - 0s 269us/step - loss: 0.0032 - val_loss: 0.0070\n",
      "Epoch 159/200\n",
      "177/177 [==============================] - 0s 428us/step - loss: 0.0033 - val_loss: 0.0092\n",
      "Epoch 160/200\n",
      "177/177 [==============================] - 0s 288us/step - loss: 0.0042 - val_loss: 0.0078\n",
      "Epoch 161/200\n",
      "177/177 [==============================] - 0s 348us/step - loss: 0.0036 - val_loss: 0.0068\n",
      "Epoch 162/200\n",
      "177/177 [==============================] - 0s 241us/step - loss: 0.0038 - val_loss: 0.0071\n",
      "Epoch 163/200\n",
      "177/177 [==============================] - 0s 394us/step - loss: 0.0032 - val_loss: 0.0078\n",
      "Epoch 164/200\n",
      "177/177 [==============================] - 0s 249us/step - loss: 0.0031 - val_loss: 0.0066\n",
      "Epoch 165/200\n",
      "177/177 [==============================] - 0s 356us/step - loss: 0.0030 - val_loss: 0.0065\n",
      "Epoch 166/200\n",
      "177/177 [==============================] - 0s 280us/step - loss: 0.0032 - val_loss: 0.0081\n",
      "Epoch 167/200\n",
      "177/177 [==============================] - 0s 290us/step - loss: 0.0029 - val_loss: 0.0076\n",
      "Epoch 168/200\n",
      "177/177 [==============================] - 0s 428us/step - loss: 0.0030 - val_loss: 0.0066\n",
      "Epoch 169/200\n",
      "177/177 [==============================] - 0s 218us/step - loss: 0.0032 - val_loss: 0.0067\n",
      "Epoch 170/200\n",
      "177/177 [==============================] - 0s 410us/step - loss: 0.0029 - val_loss: 0.0080\n",
      "Epoch 171/200\n",
      "177/177 [==============================] - 0s 277us/step - loss: 0.0031 - val_loss: 0.0064\n",
      "Epoch 172/200\n",
      "177/177 [==============================] - 0s 364us/step - loss: 0.0029 - val_loss: 0.0068\n",
      "Epoch 173/200\n",
      "177/177 [==============================] - 0s 296us/step - loss: 0.0028 - val_loss: 0.0067\n",
      "Epoch 174/200\n",
      "177/177 [==============================] - 0s 223us/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 175/200\n",
      "177/177 [==============================] - 0s 378us/step - loss: 0.0037 - val_loss: 0.0071\n",
      "Epoch 176/200\n",
      "177/177 [==============================] - 0s 334us/step - loss: 0.0030 - val_loss: 0.0071\n",
      "Epoch 177/200\n",
      "177/177 [==============================] - 0s 391us/step - loss: 0.0030 - val_loss: 0.0075\n",
      "Epoch 178/200\n",
      "177/177 [==============================] - 0s 375us/step - loss: 0.0029 - val_loss: 0.0065\n",
      "Epoch 179/200\n",
      "177/177 [==============================] - 0s 463us/step - loss: 0.0027 - val_loss: 0.0071\n",
      "Epoch 180/200\n",
      "177/177 [==============================] - 0s 273us/step - loss: 0.0028 - val_loss: 0.0064\n",
      "Epoch 181/200\n",
      "177/177 [==============================] - 0s 390us/step - loss: 0.0027 - val_loss: 0.0078\n",
      "Epoch 182/200\n",
      "177/177 [==============================] - 0s 307us/step - loss: 0.0028 - val_loss: 0.0065\n",
      "Epoch 183/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 327us/step - loss: 0.0028 - val_loss: 0.0065\n",
      "Epoch 184/200\n",
      "177/177 [==============================] - 0s 340us/step - loss: 0.0025 - val_loss: 0.0077\n",
      "Epoch 185/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 270us/step - loss: 0.0030 - val_loss: 0.0084\n",
      "Epoch 186/200\n",
      "177/177 [==============================] - 0s 389us/step - loss: 0.0030 - val_loss: 0.0064\n",
      "Epoch 187/200\n",
      "177/177 [==============================] - 0s 262us/step - loss: 0.0028 - val_loss: 0.0063\n",
      "Epoch 188/200\n",
      "177/177 [==============================] - 0s 374us/step - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 189/200\n",
      "177/177 [==============================] - 0s 254us/step - loss: 0.0026 - val_loss: 0.0069\n",
      "Epoch 190/200\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.002 - 0s 303us/step - loss: 0.0027 - val_loss: 0.0068\n",
      "Epoch 191/200\n",
      "177/177 [==============================] - 0s 257us/step - loss: 0.0026 - val_loss: 0.0118\n",
      "Epoch 192/200\n",
      "177/177 [==============================] - 0s 317us/step - loss: 0.0046 - val_loss: 0.0071\n",
      "Epoch 193/200\n",
      "177/177 [==============================] - 0s 406us/step - loss: 0.0029 - val_loss: 0.0071\n",
      "Epoch 194/200\n",
      "177/177 [==============================] - 0s 284us/step - loss: 0.0027 - val_loss: 0.0065\n",
      "Epoch 195/200\n",
      "177/177 [==============================] - 0s 346us/step - loss: 0.0025 - val_loss: 0.0067\n",
      "Epoch 196/200\n",
      "177/177 [==============================] - 0s 322us/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 197/200\n",
      "177/177 [==============================] - 0s 276us/step - loss: 0.0031 - val_loss: 0.0070\n",
      "Epoch 198/200\n",
      "177/177 [==============================] - 0s 385us/step - loss: 0.0024 - val_loss: 0.0064\n",
      "Epoch 199/200\n",
      "177/177 [==============================] - 0s 309us/step - loss: 0.0026 - val_loss: 0.0070\n",
      "Epoch 200/200\n",
      "177/177 [==============================] - 0s 369us/step - loss: 0.0031 - val_loss: 0.0066\n",
      "65/65 [==============================] - 0s 123us/step\n",
      "mse:  0.00701043214649\n"
     ]
    }
   ],
   "source": [
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928}\n",
    "model = m.mlp(cfg['lr'])\n",
    "m.train_mlp(model, configs, Y, cfg, split=177, epochs=500)\n",
    "mse = m.eval_mlp(model, configs, Y, split=177, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build lstm with input_dim: 1\n",
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 3s 304ms/step - loss: 0.0598 - mean_squared_error: 0.0598 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 2s 170ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "evaluate lstm without consideration of configs\n",
      "65/65 [==============================] - 0s 779us/step\n",
      "mse:  0.0173839222855\n"
     ]
    }
   ],
   "source": [
    "model = m.lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, lcs, steps=(5,5), split=200, batch_size=20, epochs=20, mode = 'nextstep')\n",
    "m.train_lstm(model, lcs, steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, lcs, steps=5, split=200, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 4s 457ms/step - loss: 0.3766 - mean_squared_error: 0.3766 - val_loss: 0.4930 - val_mean_squared_error: 0.4930\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 0.9218 - mean_squared_error: 0.9218 - val_loss: 0.2245 - val_mean_squared_error: 0.2245\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 0.2141 - mean_squared_error: 0.2141 - val_loss: 0.1223 - val_mean_squared_error: 0.1223\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 1s 144ms/step - loss: 0.1177 - mean_squared_error: 0.1177 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 1s 173ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 1s 104ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 8.8576e-04 - val_mean_squared_error: 8.8576e-04\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 9.1999e-04 - mean_squared_error: 9.1999e-04 - val_loss: 6.1876e-04 - val_mean_squared_error: 6.1876e-04\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 4.5607e-04 - mean_squared_error: 4.5607e-04 - val_loss: 4.6909e-04 - val_mean_squared_error: 4.6909e-04\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 3.4069e-04 - mean_squared_error: 3.4069e-04 - val_loss: 4.7472e-04 - val_mean_squared_error: 4.7472e-04\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 1s 88ms/step - loss: 3.0764e-04 - mean_squared_error: 3.0764e-04 - val_loss: 4.0949e-04 - val_mean_squared_error: 4.0949e-04\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 1s 115ms/step - loss: 2.6481e-04 - mean_squared_error: 2.6481e-04 - val_loss: 3.9920e-04 - val_mean_squared_error: 3.9920e-04\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.6800e-04 - mean_squared_error: 2.6800e-04 - val_loss: 4.0419e-04 - val_mean_squared_error: 4.0419e-04\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.6282e-04 - mean_squared_error: 2.6282e-04 - val_loss: 3.9006e-04 - val_mean_squared_error: 3.9006e-04\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.5470e-04 - mean_squared_error: 2.5470e-04 - val_loss: 3.8480e-04 - val_mean_squared_error: 3.8480e-04\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.5354e-04 - mean_squared_error: 2.5354e-04 - val_loss: 3.8584e-04 - val_mean_squared_error: 3.8584e-04\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.5453e-04 - mean_squared_error: 2.5453e-04 - val_loss: 3.8151e-04 - val_mean_squared_error: 3.8151e-04\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.5168e-04 - mean_squared_error: 2.5168e-04 - val_loss: 3.7989e-04 - val_mean_squared_error: 3.7989e-04\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.4947e-04 - mean_squared_error: 2.4947e-04 - val_loss: 3.7850e-04 - val_mean_squared_error: 3.7850e-04\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.4922e-04 - mean_squared_error: 2.4922e-04 - val_loss: 3.7601e-04 - val_mean_squared_error: 3.7601e-04\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4832e-04 - mean_squared_error: 2.4832e-04 - val_loss: 3.7310e-04 - val_mean_squared_error: 3.7310e-04\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.4703e-04 - mean_squared_error: 2.4703e-04 - val_loss: 3.7014e-04 - val_mean_squared_error: 3.7014e-04\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 2.4595e-04 - mean_squared_error: 2.4595e-04 - val_loss: 3.6862e-04 - val_mean_squared_error: 3.6862e-04\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.4498e-04 - mean_squared_error: 2.4498e-04 - val_loss: 3.6712e-04 - val_mean_squared_error: 3.6712e-04\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4411e-04 - mean_squared_error: 2.4411e-04 - val_loss: 3.6492e-04 - val_mean_squared_error: 3.6492e-04\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4329e-04 - mean_squared_error: 2.4329e-04 - val_loss: 3.6287e-04 - val_mean_squared_error: 3.6287e-04\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4250e-04 - mean_squared_error: 2.4250e-04 - val_loss: 3.6116e-04 - val_mean_squared_error: 3.6116e-04\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.4170e-04 - mean_squared_error: 2.4170e-04 - val_loss: 3.5959e-04 - val_mean_squared_error: 3.5959e-04\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.4095e-04 - mean_squared_error: 2.4095e-04 - val_loss: 3.5789e-04 - val_mean_squared_error: 3.5789e-04\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 2.4029e-04 - mean_squared_error: 2.4029e-04 - val_loss: 3.5625e-04 - val_mean_squared_error: 3.5625e-04\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 2.3967e-04 - mean_squared_error: 2.3967e-04 - val_loss: 3.5478e-04 - val_mean_squared_error: 3.5478e-04\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3909e-04 - mean_squared_error: 2.3909e-04 - val_loss: 3.5337e-04 - val_mean_squared_error: 3.5337e-04\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.3854e-04 - mean_squared_error: 2.3854e-04 - val_loss: 3.5199e-04 - val_mean_squared_error: 3.5199e-04\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3802e-04 - mean_squared_error: 2.3802e-04 - val_loss: 3.5063e-04 - val_mean_squared_error: 3.5063e-04\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3755e-04 - mean_squared_error: 2.3755e-04 - val_loss: 3.4936e-04 - val_mean_squared_error: 3.4936e-04\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3713e-04 - mean_squared_error: 2.3713e-04 - val_loss: 3.4816e-04 - val_mean_squared_error: 3.4816e-04\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3674e-04 - mean_squared_error: 2.3674e-04 - val_loss: 3.4701e-04 - val_mean_squared_error: 3.4701e-04\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3640e-04 - mean_squared_error: 2.3640e-04 - val_loss: 3.4588e-04 - val_mean_squared_error: 3.4588e-04\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.3608e-04 - mean_squared_error: 2.3608e-04 - val_loss: 3.4477e-04 - val_mean_squared_error: 3.4477e-04\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3580e-04 - mean_squared_error: 2.3580e-04 - val_loss: 3.4369e-04 - val_mean_squared_error: 3.4369e-04\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3555e-04 - mean_squared_error: 2.3555e-04 - val_loss: 3.4261e-04 - val_mean_squared_error: 3.4261e-04\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3532e-04 - mean_squared_error: 2.3532e-04 - val_loss: 3.4152e-04 - val_mean_squared_error: 3.4152e-04\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 1s 83ms/step - loss: 2.3512e-04 - mean_squared_error: 2.3512e-04 - val_loss: 3.4039e-04 - val_mean_squared_error: 3.4039e-04\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 1s 101ms/step - loss: 2.3492e-04 - mean_squared_error: 2.3492e-04 - val_loss: 3.3919e-04 - val_mean_squared_error: 3.3919e-04\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 1s 105ms/step - loss: 2.3473e-04 - mean_squared_error: 2.3473e-04 - val_loss: 3.3790e-04 - val_mean_squared_error: 3.3790e-04\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3454e-04 - mean_squared_error: 2.3454e-04 - val_loss: 3.3647e-04 - val_mean_squared_error: 3.3647e-04\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3432e-04 - mean_squared_error: 2.3432e-04 - val_loss: 3.3488e-04 - val_mean_squared_error: 3.3488e-04\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 2.3406e-04 - mean_squared_error: 2.3406e-04 - val_loss: 3.3309e-04 - val_mean_squared_error: 3.3309e-04\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 2.3374e-04 - mean_squared_error: 2.3374e-04 - val_loss: 3.3107e-04 - val_mean_squared_error: 3.3107e-04\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3336e-04 - mean_squared_error: 2.3336e-04 - val_loss: 3.2881e-04 - val_mean_squared_error: 3.2881e-04\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3292e-04 - mean_squared_error: 2.3292e-04 - val_loss: 3.2636e-04 - val_mean_squared_error: 3.2636e-04\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.3245e-04 - mean_squared_error: 2.3245e-04 - val_loss: 3.2379e-04 - val_mean_squared_error: 3.2379e-04\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3205e-04 - mean_squared_error: 2.3205e-04 - val_loss: 3.2129e-04 - val_mean_squared_error: 3.2129e-04\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 2.3192e-04 - mean_squared_error: 2.3192e-04 - val_loss: 3.1920e-04 - val_mean_squared_error: 3.1920e-04\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 2.3248e-04 - mean_squared_error: 2.3248e-04 - val_loss: 3.1808e-04 - val_mean_squared_error: 3.1808e-04\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 1s 109ms/step - loss: 2.3454e-04 - mean_squared_error: 2.3454e-04 - val_loss: 3.1893e-04 - val_mean_squared_error: 3.1893e-04\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 1s 98ms/step - loss: 2.3954e-04 - mean_squared_error: 2.3954e-04 - val_loss: 3.2341e-04 - val_mean_squared_error: 3.2341e-04\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 1s 97ms/step - loss: 2.5000e-04 - mean_squared_error: 2.5000e-04 - val_loss: 3.3438e-04 - val_mean_squared_error: 3.3438e-04\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 2.7009e-04 - mean_squared_error: 2.7009e-04 - val_loss: 3.5663e-04 - val_mean_squared_error: 3.5663e-04\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 1s 91ms/step - loss: 3.0595e-04 - mean_squared_error: 3.0595e-04 - val_loss: 3.9742e-04 - val_mean_squared_error: 3.9742e-04\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 3.6446e-04 - mean_squared_error: 3.6446e-04 - val_loss: 4.6388e-04 - val_mean_squared_error: 4.6388e-04\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.4581e-04 - mean_squared_error: 4.4581e-04 - val_loss: 5.4863e-04 - val_mean_squared_error: 5.4863e-04\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 5.2531e-04 - mean_squared_error: 5.2531e-04 - val_loss: 5.9937e-04 - val_mean_squared_error: 5.9937e-04\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 5.4342e-04 - mean_squared_error: 5.4342e-04 - val_loss: 5.3956e-04 - val_mean_squared_error: 5.3956e-04\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.6155e-04 - mean_squared_error: 4.6155e-04 - val_loss: 4.0685e-04 - val_mean_squared_error: 4.0685e-04\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.3895e-04 - mean_squared_error: 3.3895e-04 - val_loss: 3.2785e-04 - val_mean_squared_error: 3.2785e-04\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.6445e-04 - mean_squared_error: 2.6445e-04 - val_loss: 3.1702e-04 - val_mean_squared_error: 3.1702e-04\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4456e-04 - mean_squared_error: 2.4456e-04 - val_loss: 3.2263e-04 - val_mean_squared_error: 3.2263e-04\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4326e-04 - mean_squared_error: 2.4326e-04 - val_loss: 3.2345e-04 - val_mean_squared_error: 3.2345e-04\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4217e-04 - mean_squared_error: 2.4217e-04 - val_loss: 3.2020e-04 - val_mean_squared_error: 3.2020e-04\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3920e-04 - mean_squared_error: 2.3920e-04 - val_loss: 3.1639e-04 - val_mean_squared_error: 3.1639e-04\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.3606e-04 - mean_squared_error: 2.3606e-04 - val_loss: 3.1358e-04 - val_mean_squared_error: 3.1358e-04\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.3396e-04 - mean_squared_error: 2.3396e-04 - val_loss: 3.1189e-04 - val_mean_squared_error: 3.1189e-04\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.3313e-04 - mean_squared_error: 2.3313e-04 - val_loss: 3.1114e-04 - val_mean_squared_error: 3.1114e-04\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3335e-04 - mean_squared_error: 2.3335e-04 - val_loss: 3.1119e-04 - val_mean_squared_error: 3.1119e-04\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.3430e-04 - mean_squared_error: 2.3430e-04 - val_loss: 3.1198e-04 - val_mean_squared_error: 3.1198e-04\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3582e-04 - mean_squared_error: 2.3582e-04 - val_loss: 3.1344e-04 - val_mean_squared_error: 3.1344e-04\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.3796e-04 - mean_squared_error: 2.3796e-04 - val_loss: 3.1562e-04 - val_mean_squared_error: 3.1562e-04\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.4096e-04 - mean_squared_error: 2.4096e-04 - val_loss: 3.1871e-04 - val_mean_squared_error: 3.1871e-04\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.4529e-04 - mean_squared_error: 2.4529e-04 - val_loss: 3.2321e-04 - val_mean_squared_error: 3.2321e-04\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.5162e-04 - mean_squared_error: 2.5162e-04 - val_loss: 3.2983e-04 - val_mean_squared_error: 3.2983e-04\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.6090e-04 - mean_squared_error: 2.6090e-04 - val_loss: 3.3953e-04 - val_mean_squared_error: 3.3953e-04\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 2.7430e-04 - mean_squared_error: 2.7430e-04 - val_loss: 3.5348e-04 - val_mean_squared_error: 3.5348e-04\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9319e-04 - mean_squared_error: 2.9319e-04 - val_loss: 3.7291e-04 - val_mean_squared_error: 3.7291e-04\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.1870e-04 - mean_squared_error: 3.1870e-04 - val_loss: 3.9866e-04 - val_mean_squared_error: 3.9866e-04\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.5081e-04 - mean_squared_error: 3.5081e-04 - val_loss: 4.2978e-04 - val_mean_squared_error: 4.2978e-04\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 3.8647e-04 - mean_squared_error: 3.8647e-04 - val_loss: 4.6125e-04 - val_mean_squared_error: 4.6125e-04\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 4.1769e-04 - mean_squared_error: 4.1769e-04 - val_loss: 4.8225e-04 - val_mean_squared_error: 4.8225e-04\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.3208e-04 - mean_squared_error: 4.3208e-04 - val_loss: 4.7978e-04 - val_mean_squared_error: 4.7978e-04\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.1960e-04 - mean_squared_error: 4.1960e-04 - val_loss: 4.4979e-04 - val_mean_squared_error: 4.4979e-04\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 69ms/step - loss: 3.8228e-04 - mean_squared_error: 3.8228e-04 - val_loss: 4.0450e-04 - val_mean_squared_error: 4.0450e-04\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.3511e-04 - mean_squared_error: 3.3511e-04 - val_loss: 3.6290e-04 - val_mean_squared_error: 3.6290e-04\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.9448e-04 - mean_squared_error: 2.9448e-04 - val_loss: 3.3539e-04 - val_mean_squared_error: 3.3539e-04\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 2.6762e-04 - mean_squared_error: 2.6762e-04 - val_loss: 3.2115e-04 - val_mean_squared_error: 3.2115e-04\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.5296e-04 - mean_squared_error: 2.5296e-04 - val_loss: 3.1496e-04 - val_mean_squared_error: 3.1496e-04\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 2.4593e-04 - mean_squared_error: 2.4593e-04 - val_loss: 3.1252e-04 - val_mean_squared_error: 3.1252e-04\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4285e-04 - mean_squared_error: 2.4285e-04 - val_loss: 3.1156e-04 - val_mean_squared_error: 3.1156e-04\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.4166e-04 - mean_squared_error: 2.4166e-04 - val_loss: 3.1123e-04 - val_mean_squared_error: 3.1123e-04\n",
      "evaluate lstm with consideration of configs\n",
      "115/115 [==============================] - 0s 632us/step\n",
      "mse:  0.000311232009984\n"
     ]
    }
   ],
   "source": [
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# m.train_lstm(model, [configs,lcs], steps=(0,5), split=200, batch_size=20, epochs=20, mode = 'finalstep')\n",
    "m.train_lstm(model, [configs,lcs], steps=(20,20), split=150, batch_size=20, epochs=100, mode='nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=20, split=150, batch_size=20, mode='nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse train: 0.00141, mse validation 0.00111\n"
     ]
    }
   ],
   "source": [
    "m.pred_finalpoints(model, [configs,lcs], steps=20, split=150, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm without consideration of configs\n",
      "88/88 [==============================] - 0s 482us/step\n",
      "mse:  0.00275807289555\n"
     ]
    }
   ],
   "source": [
    "mse = m.eval_lstm(model, lcs, Y, steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 7s 742ms/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.0959 - val_mean_squared_error: 0.0959\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 2s 171ms/step - loss: 0.2231 - mean_squared_error: 0.2231 - val_loss: 0.1004 - val_mean_squared_error: 0.1004\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.1406 - mean_squared_error: 0.1406 - val_loss: 0.1044 - val_mean_squared_error: 0.1044\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.2207 - mean_squared_error: 0.2207 - val_loss: 0.0289 - val_mean_squared_error: 0.0289\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 10/50\n",
      "1/9 [==>...........................] - ETA: 1s - loss: 0.0033 - mean_squared_error: 0.0033"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3026717f4502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lcs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"configs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m177\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m177\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL Theory/project/models.py\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, X, Y, steps, split, batch_size, epochs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_train_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DL Theory/project/models.py\u001b[0m in \u001b[0;36m_train_lstm\u001b[0;34m(model, X, Y, steps, idx, batch_size, epochs, callbacks)\u001b[0m\n\u001b[1;32m    172\u001b[0m                                \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                                \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                                verbose = 1)\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = m.multi_lstm(lr=0.002)\n",
    "model = m.multi_lstm()\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,5), split=177, batch_size=20, epochs=50, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs: (265, 40, 1) configs: (265, 5) Y (265, 1)\n",
      "train with random nr. of epochs, evaluate with 5 epochs\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 304ms/step - loss: 0.1160 - mean_squared_error: 0.1160 - val_loss: 0.0853 - val_mean_squared_error: 0.0853\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.0558 - mean_squared_error: 0.0558 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 179ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0137 - val_mean_squared_error: 0.0137\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 250ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 0.0145 - mean_squared_error: 0.0145 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0146 - val_mean_squared_error: 0.0146\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 250ms/step - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0253 - val_mean_squared_error: 0.0253\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 228ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 3s 278ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 2s 229ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 2s 168ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 2s 169ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 2s 168ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 2s 240ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0072 - val_mean_squared_error: 0.0072\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "9/9 [==============================] - 2s 231ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 3s 313ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0229 - val_mean_squared_error: 0.0229\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 2s 261ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 2s 230ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 2s 253ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 3s 300ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 3s 333ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 2s 251ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 2s 265ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 2s 249ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 3s 320ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 2s 248ms/step - loss: 9.7024e-04 - mean_squared_error: 9.7024e-04 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 2s 253ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 8.8618e-04 - mean_squared_error: 8.8618e-04 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 2s 259ms/step - loss: 9.8973e-04 - mean_squared_error: 9.8973e-04 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 8.1752e-04 - mean_squared_error: 8.1752e-04 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 2s 272ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 808us/step\n",
      "mse:  0.00515400678639\n"
     ]
    }
   ],
   "source": [
    "model = m.multi_lstm(lr=0.002)\n",
    "print(\"lcs:\", lcs.shape, \"configs:\", configs.shape, \"Y\", Y.shape)\n",
    "# now using random length for timesteps considered steps = (0,x)\n",
    "m.train_lstm(model, [configs,lcs], steps=(0,5), split=177, batch_size=20, epochs=100, mode = 'nextstep')\n",
    "mse = m.eval_lstm(model, [configs,lcs], steps=5, split=177, batch_size=20, mode = 'nextstep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate lstm with consideration of configs\n",
      "65/65 [==============================] - 0s 2ms/step\n",
      "mse:  0.000678403697048\n"
     ]
    }
   ],
   "source": [
    "mse = m.eval_lstm(model, [configs,lcs], Y, steps=20, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    evaluating models with cross validation (ridge, XGB, mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'alpha': 1.0}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.02977 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.03703 -0.02671 -0.02556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "cfg={'alpha':1.0}\n",
    "results = m.eval_cv('ridge', configs, Y, cfg=cfg, splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'maxdepth': 10, 'subsample': 0.7946631901813815, 'cols_bt': 0.9376450587145334, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'n_estimators': 1000}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.00691 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.00898 -0.00403 -0.00771]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.6s finished\n"
     ]
    }
   ],
   "source": [
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "#cfg = {'maxdepth': 4, 'lr': 0.07120217610550672, 'gamma': 0.03393596760993278, 'cols_bt': 0.823494199726015, 'n_estimators': 107, 'subsample': 0.7288741544938715}\n",
    "results = m.eval_cv('xgb', configs, Y, cfg=cfg, splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'k_exp': 0.005043479631870928, 'lr': 0.2213474827989724, 'batch_size': 20}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04638, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04638 to 0.04408, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04464, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04408 to 0.04137, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.04187, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04137 to 0.04044, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04044 to 0.03913, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.03922, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03913 to 0.03324, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03324 to 0.03174, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03174 to 0.03173, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03173 to 0.02817, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02817 to 0.02606, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.02733, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02606 to 0.02496, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02496 to 0.02285, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02285 to 0.02082, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.02113, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.02244, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02082 to 0.01710, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01710 to 0.01590, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01590 to 0.01505, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.01521, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01505 to 0.01398, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.01417, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01398 to 0.01371, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01371 to 0.01258, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01258 to 0.01161, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.01427, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01161 to 0.01069, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.01261, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01069 to 0.01062, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01062 to 0.01045, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01045 to 0.00948, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00948 to 0.00933, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00933 to 0.00924, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00972, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00977, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00924 to 0.00906, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00922, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00906 to 0.00904, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00904 to 0.00849, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00861, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00900, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.01081, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00849 to 0.00835, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00896, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00835 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00808 to 0.00793, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00850, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00793 to 0.00779, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00779 to 0.00779, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00833, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00779 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00897, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00762 to 0.00751, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00751 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00745 to 0.00737, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00737 to 0.00725, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00725 to 0.00725, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00725 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00719 to 0.00710, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00710 to 0.00703, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00765, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00703 to 0.00697, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.00802, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00916, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00697 to 0.00690, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00690 to 0.00686, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00137: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00872, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00686 to 0.00678, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00678 to 0.00671, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00671 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00661 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00658 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00651 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00651 to 0.00651, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00651 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00645 to 0.00643, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00643 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00633 to 0.00633, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00818, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00906, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00633 to 0.00625, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00824, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00665, did not improve\n",
      "Epoch 00273: early stopping\n",
      "Using epoch 00229 with val_loss: 0.00625\n",
      "89/89 [==============================] - 0s 354us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02920, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02920 to 0.02775, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02775 to 0.02655, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02655 to 0.02575, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02575 to 0.02407, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02407 to 0.02372, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02372 to 0.02122, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02122 to 0.02028, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02028 to 0.01910, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01910 to 0.01859, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01859 to 0.01754, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.01754 to 0.01673, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01673 to 0.01606, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01606 to 0.01523, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01523 to 0.01476, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01476 to 0.01369, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01369 to 0.01310, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01347, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.01387, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01310 to 0.01281, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01281 to 0.01159, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01159 to 0.01001, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01001 to 0.00969, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00969 to 0.00958, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00958 to 0.00864, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00864 to 0.00820, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00820 to 0.00781, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00781 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00859, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00745 to 0.00707, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00707 to 0.00704, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00704 to 0.00690, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00904, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00690 to 0.00648, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00648 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00645 to 0.00630, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00630 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00624 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00618 to 0.00598, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00598 to 0.00588, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00777, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00588 to 0.00575, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00575 to 0.00571, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00622, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00571 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00556 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00548 to 0.00538, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00538 to 0.00535, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00161: val_loss improved from 0.00535 to 0.00530, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00530 to 0.00526, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00526 to 0.00524, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00537, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00947, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00526, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00843, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00524 to 0.00514, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00604, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00867, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00854, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.01082, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00577, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00555, did not improve\n",
      "Epoch 00321: early stopping\n",
      "Using epoch 00246 with val_loss: 0.00514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 214us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02545, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02545 to 0.02507, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02507 to 0.02317, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02317 to 0.02283, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02283 to 0.02086, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02090, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02086 to 0.02001, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02001 to 0.02000, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02000 to 0.01804, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01804 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.01907, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01769 to 0.01669, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01669 to 0.01458, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01458 to 0.01401, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01401 to 0.01400, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01400 to 0.01270, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01270 to 0.01204, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01204 to 0.01154, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01154 to 0.01120, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01120 to 0.01049, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01049 to 0.00991, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00991 to 0.00965, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00965 to 0.00909, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.01044, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00914, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00909 to 0.00873, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00873 to 0.00839, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00839 to 0.00816, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00997, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00816 to 0.00774, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00774 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00745 to 0.00738, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00915, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00738 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00699 to 0.00687, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00814, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00687 to 0.00674, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00674 to 0.00649, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00969, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00902, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00649 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.01186, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00965, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00642 to 0.00640, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00847, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00640 to 0.00614, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00927, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00786, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00614 to 0.00594, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00832, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00594 to 0.00574, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00574 to 0.00566, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00627, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00144: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00566 to 0.00540, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00540 to 0.00529, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss is 0.00875, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00827, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00958, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00529 to 0.00529, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00593, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00529 to 0.00526, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00526 to 0.00525, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00525 to 0.00506, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00628, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00506 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00490 to 0.00484, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00517, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00515, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00549, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00532, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00783, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00534, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00602, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00567, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.00484 to 0.00477, storing weights.\n",
      "\n",
      "Epoch 00288: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00536, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00577, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00302: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00518, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00586, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00498, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00498, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00538, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00581, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00331: val_loss is 0.00601, did not improve\n",
      "\n",
      "Epoch 00332: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00524, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00337: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00338: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00494, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00483, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00507, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00569, did not improve\n",
      "Epoch 00362: early stopping\n",
      "Using epoch 00287 with val_loss: 0.00477\n",
      "88/88 [==============================] - 0s 233us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00539] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00625]\n",
      " [ 0.00514]\n",
      " [ 0.00477]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# evaluate via cross validation\n",
    "cfg = {'lr': 0.2213474827989724, 'batch_size': 20, 'k_exp': 0.005043479631870928} \n",
    "results = m.eval_cv('mlp', configs, Y, cfg=cfg, epochs=1000, splits = 3, earlystop=True, \n",
    "                    dropout=False, lr_exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15833, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15833 to 0.04731, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04731 to 0.03801, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03801 to 0.02834, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02834 to 0.01149, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01149 to 0.00788, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00928, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00788 to 0.00367, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00367 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00310, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00257 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00195 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00300, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00140 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00132 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00113 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00101 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00090 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00085 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00079 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00076 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00073 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00067 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00060 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00053 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00048 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00046 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00039, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00131: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "Epoch 00140: early stopping\n",
      "Using epoch 00103 with val_loss: 0.00039\n",
      "validate on 5 steps, mse on train / validation data: 1.39873 / 2.94549\n",
      "validate on 10 steps, mse on train / validation data: 1.29646 / 2.50751\n",
      "validate on 20 steps, mse on train / validation data: 0.91139 / 1.62842\n",
      "validate on 30 steps, mse on train / validation data: 0.42367 / 0.79404\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36051, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36051 to 0.71615, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71615 to 0.06664, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06664 to 0.05364, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05364 to 0.03628, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03628 to 0.03322, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03322 to 0.03315, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03315 to 0.02767, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.03008, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.03070, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02767 to 0.02729, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02845, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.02813, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.02743, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.02762, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02729 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02692 to 0.02647, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02647 to 0.02543, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02543 to 0.02297, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02297 to 0.01643, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01643 to 0.00435, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00435 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00200 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00160 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00293, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00084 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00064 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00056 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00052 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00050 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00044 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00046, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00047, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00082 with val_loss: 0.00040\n",
      "validate on 5 steps, mse on train / validation data: 0.33923 / 0.30867\n",
      "validate on 10 steps, mse on train / validation data: 0.29828 / 0.27770\n",
      "validate on 20 steps, mse on train / validation data: 0.24790 / 0.25507\n",
      "validate on 30 steps, mse on train / validation data: 0.15224 / 0.16362\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.43205, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.43205 to 0.08444, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08444 to 0.03833, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03833 to 0.01168, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02661, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01168 to 0.00909, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00909 to 0.00400, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00400 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00141 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00110 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00089 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00083 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00067 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00066, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00110: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00067, did not improve\n",
      "Epoch 00138: early stopping\n",
      "Using epoch 00097 with val_loss: 0.00066\n",
      "validate on 5 steps, mse on train / validation data: 0.05671 / 0.05211\n",
      "validate on 10 steps, mse on train / validation data: 0.05669 / 0.05225\n",
      "validate on 20 steps, mse on train / validation data: 0.08992 / 0.08317\n",
      "validate on 30 steps, mse on train / validation data: 0.07963 / 0.07053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 1.10209  0.94582  0.65555  0.34273] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.94549  2.50751  1.62842  0.79404]\n",
      " [ 0.30867  0.2777   0.25507  0.16362]\n",
      " [ 0.05211  0.05225  0.08317  0.07053]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.59822  0.55048  0.4164   0.21851] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.39873  1.29646  0.91139  0.42367]\n",
      " [ 0.33923  0.29828  0.2479   0.15224]\n",
      " [ 0.05671  0.05669  0.08992  0.07963]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28097, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28097 to 0.04951, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04951 to 0.03098, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03098 to 0.01735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01735 to 0.01043, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01043 to 0.00562, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00562 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00338, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00195 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00070 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00051 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00044 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00048, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00057, did not improve\n",
      "Epoch 00159: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00039\n",
      "validate on 5 steps, mse on train / validation data: 0.09498 / 0.14268\n",
      "validate on 10 steps, mse on train / validation data: 0.01112 / 0.00763\n",
      "validate on 20 steps, mse on train / validation data: 0.00481 / 0.00426\n",
      "validate on 30 steps, mse on train / validation data: 0.00179 / 0.00161\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53724, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 6.09075, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 1.76684, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53724 to 0.43818, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43818 to 0.16106, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16106 to 0.10837, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10837 to 0.07243, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07243 to 0.03493, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03493 to 0.03073, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03073 to 0.02940, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02940 to 0.02885, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02885 to 0.02763, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02763 to 0.02669, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.02681, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02669 to 0.02585, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02585 to 0.02527, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02527 to 0.02456, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02456 to 0.02411, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02411 to 0.02343, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02343 to 0.02284, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02284 to 0.02214, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02214 to 0.02141, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02141 to 0.02062, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02062 to 0.01983, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01983 to 0.01897, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01897 to 0.01805, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01805 to 0.01706, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01706 to 0.01600, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01600 to 0.01489, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01489 to 0.01372, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01372 to 0.01251, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01251 to 0.01126, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01126 to 0.01000, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01000 to 0.00873, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00873 to 0.00747, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00747 to 0.00627, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00627 to 0.00515, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00515 to 0.00415, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00415 to 0.00331, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00331 to 0.00263, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00263 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00212 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00175 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00150 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00134 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00123 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00116 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00110 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00102 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00098 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00094 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00091 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00089 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00086 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00083 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00081 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00079 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00076 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00074 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00072 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00070 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00068 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00063 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00060 to 0.00058, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00057 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00053 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00058, did not improve\n",
      "Epoch 00157: early stopping\n",
      "Using epoch 00102 with val_loss: 0.00033\n",
      "validate on 5 steps, mse on train / validation data: 0.15210 / 0.15761\n",
      "validate on 10 steps, mse on train / validation data: 0.24021 / 0.22045\n",
      "validate on 20 steps, mse on train / validation data: 0.18364 / 0.17788\n",
      "validate on 30 steps, mse on train / validation data: 0.09263 / 0.09459\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08628, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08628 to 0.05569, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05569 to 0.04042, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04042 to 0.01699, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01699 to 0.01185, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01185 to 0.00413, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00413 to 0.00336, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00336 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00162 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00114 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00102 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00100 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00066 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00063 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00060 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00059 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00057 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00054 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00052 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00050 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00050 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00049 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00043 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00042 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00031 to 0.00031, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00037, did not improve\n",
      "Epoch 00222: early stopping\n",
      "Using epoch 00168 with val_loss: 0.00026\n",
      "validate on 5 steps, mse on train / validation data: 0.08700 / 0.06098\n",
      "validate on 10 steps, mse on train / validation data: 0.08540 / 0.05817\n",
      "validate on 20 steps, mse on train / validation data: 0.12541 / 0.08756\n",
      "validate on 30 steps, mse on train / validation data: 0.10266 / 0.07260\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.12042  0.09541  0.0899   0.05627] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.14268  0.00763  0.00426  0.00161]\n",
      " [ 0.15761  0.22045  0.17788  0.09459]\n",
      " [ 0.06098  0.05817  0.08756  0.0726 ]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.11136  0.11224  0.10462  0.06569] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.09498  0.01112  0.00481  0.00179]\n",
      " [ 0.1521   0.24021  0.18364  0.09263]\n",
      " [ 0.087    0.0854   0.12541  0.10266]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06147, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.06147 to 0.02912, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04456, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02912 to 0.00938, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02167, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00938 to 0.00417, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00417 to 0.00318, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00318 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00202 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00183 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00085 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00071 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00068 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00070, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00065 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00057 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00048 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00042 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00039 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00056, did not improve\n",
      "Epoch 00148: early stopping\n",
      "Using epoch 00099 with val_loss: 0.00037\n",
      "validate on 5 steps, mse on train / validation data: 0.00966 / 0.00962\n",
      "validate on 10 steps, mse on train / validation data: 0.00565 / 0.00540\n",
      "validate on 20 steps, mse on train / validation data: 0.00491 / 0.00422\n",
      "validate on 30 steps, mse on train / validation data: 0.00185 / 0.00175\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.38183, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 3.87232, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.38183 to 3.33221, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.33221 to 0.81293, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81293 to 0.47807, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.47807 to 0.22761, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22761 to 0.09574, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09574 to 0.05372, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.05372 to 0.03384, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03384 to 0.03228, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03228 to 0.03124, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03124 to 0.02922, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02922 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.02795, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.02846, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02845, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.02814, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.02813, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02816, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02818, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02819, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02820, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02822, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02821, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02820, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02820, did not improve\n",
      "Epoch 00088: early stopping\n",
      "Using epoch 00013 with val_loss: 0.02754\n",
      "validate on 5 steps, mse on train / validation data: 0.02964 / 0.02665\n",
      "validate on 10 steps, mse on train / validation data: 0.02964 / 0.02665\n",
      "validate on 20 steps, mse on train / validation data: 0.02965 / 0.02666\n",
      "validate on 30 steps, mse on train / validation data: 0.02965 / 0.02666\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03272, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03272 to 0.03242, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03242 to 0.02405, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02405 to 0.01395, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01395 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00753 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00655 to 0.00349, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00349 to 0.00271, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00271 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00151 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00126 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00083 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00077 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00055 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00046 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00036 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00035 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00035 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00034 to 0.00034, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00034 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00033 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00033, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00033, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00076 with val_loss: 0.00032\n",
      "validate on 5 steps, mse on train / validation data: 0.00925 / 0.00581\n",
      "validate on 10 steps, mse on train / validation data: 0.00231 / 0.00226\n",
      "validate on 20 steps, mse on train / validation data: 0.00168 / 0.00145\n",
      "validate on 30 steps, mse on train / validation data: 0.00082 / 0.00098\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01403  0.01144  0.01078  0.0098 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00962  0.0054   0.00422  0.00175]\n",
      " [ 0.02665  0.02665  0.02666  0.02666]\n",
      " [ 0.00581  0.00226  0.00145  0.00098]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01618  0.01253  0.01208  0.01077] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00966  0.00565  0.00491  0.00185]\n",
      " [ 0.02964  0.02964  0.02965  0.02965]\n",
      " [ 0.00925  0.00231  0.00168  0.00082]]\n",
      "results training data\n",
      " [[ 0.59822095  0.55047764  0.41640321  0.21851303]\n",
      " [ 0.11135717  0.11224402  0.10462055  0.0656942 ]\n",
      " [ 0.01618353  0.01253378  0.01207872  0.01077083]]\n",
      "results validation data \n",
      " [[ 1.10209298  0.94582258  0.65555483  0.3427303 ]\n",
      " [ 0.12042354  0.09541441  0.08990091  0.05626914]\n",
      " [ 0.01402978  0.01143511  0.01077778  0.00979835]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.2\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08584, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08584 to 0.07439, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07439 to 0.02369, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02369 to 0.01386, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01386 to 0.01289, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01289 to 0.00480, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00480 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00353, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00335 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00121 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00099 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00339, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00065 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00063 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00046 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00059, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00036, did not improve\n",
      "Epoch 00160: early stopping\n",
      "Using epoch 00085 with val_loss: 0.00027\n",
      "validate on 5 steps, mse on train / validation data: 0.01471 / 0.02526\n",
      "validate on 10 steps, mse on train / validation data: 0.01031 / 0.01362\n",
      "validate on 20 steps, mse on train / validation data: 0.00436 / 0.00365\n",
      "validate on 30 steps, mse on train / validation data: 0.00166 / 0.00143\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60643, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60643 to 0.04722, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04722 to 0.04653, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.06111, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04653 to 0.03205, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03300, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03205 to 0.02693, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02693 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01068 to 0.00309, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00501, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00309 to 0.00285, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00285 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00163 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00095 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00060 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00055 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00041 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00035 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00026 to 0.00020, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00057, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00134: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "Epoch 00135: early stopping\n",
      "Using epoch 00081 with val_loss: 0.00020\n",
      "validate on 5 steps, mse on train / validation data: 0.16159 / 0.08641\n",
      "validate on 10 steps, mse on train / validation data: 0.12393 / 0.07440\n",
      "validate on 20 steps, mse on train / validation data: 0.03269 / 0.02646\n",
      "validate on 30 steps, mse on train / validation data: 0.00476 / 0.00441\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11001, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11001 to 0.08327, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08327 to 0.03608, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03608 to 0.01065, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01065 to 0.00554, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00554 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00247 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00202 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00197 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00120 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00058 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00043 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00036 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00031 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00030, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00026, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00028, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00027, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00031, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00039, did not improve\n",
      "Epoch 00119: early stopping\n",
      "Using epoch 00055 with val_loss: 0.00025\n",
      "validate on 5 steps, mse on train / validation data: 0.02207 / 0.03434\n",
      "validate on 10 steps, mse on train / validation data: 0.01448 / 0.02045\n",
      "validate on 20 steps, mse on train / validation data: 0.00771 / 0.00859\n",
      "validate on 30 steps, mse on train / validation data: 0.00305 / 0.00316\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04867  0.03615  0.0129   0.003  ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02526  0.01362  0.00365  0.00143]\n",
      " [ 0.08641  0.0744   0.02646  0.00441]\n",
      " [ 0.03434  0.02045  0.00859  0.00316]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.06613  0.04957  0.01492  0.00315] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.01471  0.01031  0.00436  0.00166]\n",
      " [ 0.16159  0.12393  0.03269  0.00476]\n",
      " [ 0.02207  0.01448  0.00771  0.00305]]\n",
      "results training data\n",
      " [ 0.06612556  0.04957401  0.01491995  0.00315308]\n",
      "results validation data \n",
      " [ 0.04866848  0.03615473  0.01290235  0.00300086]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train with random lenghts\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                     mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11554, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11554 to 0.04302, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04302 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01769 to 0.01097, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01097 to 0.00882, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00882 to 0.00847, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00847 to 0.00606, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00606 to 0.00438, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00438 to 0.00315, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.01117, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00329, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01114, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.01599, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.01499, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.01350, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.01058, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.01165, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.01304, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00866, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00371, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00410, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00315 to 0.00251, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00349, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00280, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00251 to 0.00250, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00250 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00220 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00181 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00160 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00137 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00513, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00362, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00285, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00220, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00227, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00223, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00184, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00428, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00150, did not improve\n",
      "Epoch 00123: early stopping\n",
      "Using epoch 00049 with val_loss: 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 459us/step\n",
      "mse:  0.00131853502845\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 870us/step\n",
      "mse:  0.00133469417448\n",
      "validate on 5 steps, mse on train / validation data: 0.00133 / 0.00132\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.001162388689\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00171085036444\n",
      "validate on 10 steps, mse on train / validation data: 0.00171 / 0.00116\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00321647753217\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00388603562235\n",
      "validate on 20 steps, mse on train / validation data: 0.00389 / 0.00322\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00459700725512\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.00526945944875\n",
      "validate on 30 steps, mse on train / validation data: 0.00527 / 0.00460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34632, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34632 to 0.06494, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06494 to 0.05340, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05340 to 0.02735, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02735 to 0.01483, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.02208, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.02008, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01483 to 0.01388, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01388 to 0.01224, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01224 to 0.00833, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00833 to 0.00687, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00687 to 0.00294, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.01338, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.01179, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.01202, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00840, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00460, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00359, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00848, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00749, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00294 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00530, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00671, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00487, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00441, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00421, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00466, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00401, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00347, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00317, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00262, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00271, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00259, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00250, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00244, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00238, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00226 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00220 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00216 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00214 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00211 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00211 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00209 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00206 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00201 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00197 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00193 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00176 to 0.00175, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00168 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00158 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00155 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00152 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00150 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00148 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00146 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00144 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00142 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00141 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00138 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00136 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00135 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00134 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00133 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00131 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00129 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00127 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00126, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss improved from 0.00126 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00258: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00259: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00261: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00264: val_loss improved from 0.00123 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00266: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.00122 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00268: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00274: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00275: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00276: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.00120 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00282: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00288: val_loss improved from 0.00119 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00289: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00291: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.00118 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00301: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00303: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00117 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00116 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00116, did not improve\n",
      "Epoch 00328: early stopping\n",
      "Using epoch 00323 with val_loss: 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00115162648779\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 903us/step\n",
      "mse:  0.0012707624862\n",
      "validate on 5 steps, mse on train / validation data: 0.00127 / 0.00115\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00158448308833\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0011413448456\n",
      "validate on 10 steps, mse on train / validation data: 0.00114 / 0.00158\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0039176084207\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.00348516953004\n",
      "validate on 20 steps, mse on train / validation data: 0.00349 / 0.00392\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0050399990075\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.00497433785639\n",
      "validate on 30 steps, mse on train / validation data: 0.00497 / 0.00504\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.99042, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.99042 to 1.47941, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 2.96802, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47941 to 0.63913, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63913 to 0.51863, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.51863 to 0.08843, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08843 to 0.05703, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.05703 to 0.04830, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04830 to 0.03768, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03768 to 0.03411, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03411 to 0.02496, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.02528, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02496 to 0.02449, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02449 to 0.02441, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.02461, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.02456, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02441 to 0.02439, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02445, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02444, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02445, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02446, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02447, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02448, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02449, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02450, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02451, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02451, did not improve\n",
      "Epoch 00093: early stopping\n",
      "Using epoch 00018 with val_loss: 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 765us/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 776us/step\n",
      "mse:  0.0299257414868\n",
      "validate on 5 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 10 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 20 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.0243879513053\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0299257414868\n",
      "validate on 30 steps, mse on train / validation data: 0.02993 / 0.02439\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00895  0.00904  0.01051  0.01134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00132  0.00116  0.00322  0.0046 ]\n",
      " [ 0.00115  0.00158  0.00392  0.00504]\n",
      " [ 0.02439  0.02439  0.02439  0.02439]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.01084  0.01093  0.01243  0.01339] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00133  0.00171  0.00389  0.00527]\n",
      " [ 0.00127  0.00114  0.00349  0.00497]\n",
      " [ 0.02993  0.02993  0.02993  0.02993]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12057, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12057 to 0.06355, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.06355 to 0.03759, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03759 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00790 to 0.00766, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00766 to 0.00323, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00454, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.01132, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00389, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00323 to 0.00304, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00340, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00318, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00304 to 0.00301, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00301 to 0.00289, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00289 to 0.00259, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00259 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00227 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00199 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00178 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00152 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00180, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00122 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00153, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00120 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00159, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00104 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00149, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00089 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00084, did not improve\n",
      "Epoch 00120: early stopping\n",
      "Using epoch 00088 with val_loss: 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 481us/step\n",
      "mse:  0.0608066631418\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 855us/step\n",
      "mse:  0.0477681301365\n",
      "validate on 5 steps, mse on train / validation data: 0.04777 / 0.06081\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000824270867403\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000763803920894\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00082\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.00128009282\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.00143760890404\n",
      "validate on 20 steps, mse on train / validation data: 0.00144 / 0.00128\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00213678000401\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.0023780453167\n",
      "validate on 30 steps, mse on train / validation data: 0.00238 / 0.00214\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26239, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26239 to 0.07691, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.12018, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07691 to 0.05873, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05873 to 0.02621, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.03345, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02621 to 0.02494, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02494 to 0.01605, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01605 to 0.01107, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01107 to 0.00395, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00395 to 0.00253, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00253 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00261, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00169 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00090 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00079 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00072 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00066 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00064 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00065, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00068, did not improve\n",
      "Epoch 00106: early stopping\n",
      "Using epoch 00083 with val_loss: 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 590us/step\n",
      "mse:  0.0690140077336\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 744us/step\n",
      "mse:  0.0738325941319\n",
      "validate on 5 steps, mse on train / validation data: 0.07383 / 0.06901\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000604276981903\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000690422686459\n",
      "validate on 10 steps, mse on train / validation data: 0.00069 / 0.00060\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.00140974406068\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.0012300674403\n",
      "validate on 20 steps, mse on train / validation data: 0.00123 / 0.00141\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00224876565732\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00229339740215\n",
      "validate on 30 steps, mse on train / validation data: 0.00229 / 0.00225\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12354, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12354 to 0.04591, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.04597, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04591 to 0.02748, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02748 to 0.01963, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01963 to 0.01252, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01252 to 0.00803, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00803 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00658 to 0.00560, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00560 to 0.00375, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00375 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00290, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00194 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00174 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00172 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00193, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00153 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00150 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00140 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00110 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00108 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00104 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00165, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00095 to 0.00094, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00093 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "Epoch 00104: early stopping\n",
      "Using epoch 00104 with val_loss: 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 526us/step\n",
      "mse:  0.789464980364\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 710us/step\n",
      "mse:  0.750682481601\n",
      "validate on 5 steps, mse on train / validation data: 0.75068 / 0.78946\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000914607464272\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000559727040101\n",
      "validate on 10 steps, mse on train / validation data: 0.00056 / 0.00091\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.980713968927\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.812348923104\n",
      "validate on 20 steps, mse on train / validation data: 0.81235 / 0.98071\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.750668579882\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "mse:  0.836846585664\n",
      "validate on 30 steps, mse on train / validation data: 0.83685 / 0.75067\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.30643  0.00078  0.3278   0.25168] ***\n",
      "Results validation data of all Folds: \n",
      "[[  6.08100000e-02   8.20000000e-04   1.28000000e-03   2.14000000e-03]\n",
      " [  6.90100000e-02   6.00000000e-04   1.41000000e-03   2.25000000e-03]\n",
      " [  7.89460000e-01   9.10000000e-04   9.80710000e-01   7.50670000e-01]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.29076  0.00067  0.27167  0.28051] ***\n",
      "Results training data of all Folds: \n",
      "[[  4.77700000e-02   7.60000000e-04   1.44000000e-03   2.38000000e-03]\n",
      " [  7.38300000e-02   6.90000000e-04   1.23000000e-03   2.29000000e-03]\n",
      " [  7.50680000e-01   5.60000000e-04   8.12350000e-01   8.36850000e-01]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14798, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14798 to 0.04405, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04405 to 0.00993, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00993 to 0.00817, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00817 to 0.00592, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00592 to 0.00324, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00324 to 0.00244, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00244 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00093 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00059 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00055 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00051 to 0.00050, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00050 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00149, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00252, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00079: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "Epoch 00116: early stopping\n",
      "Using epoch 00041 with val_loss: 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 641us/step\n",
      "mse:  0.0258297447869\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 860us/step\n",
      "mse:  0.0263428334147\n",
      "validate on 5 steps, mse on train / validation data: 0.02634 / 0.02583\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.0107730397683\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.00910780364013\n",
      "validate on 10 steps, mse on train / validation data: 0.00911 / 0.01077\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000469433154199\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000500161555961\n",
      "validate on 20 steps, mse on train / validation data: 0.00050 / 0.00047\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.000675560233544\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 3ms/step\n",
      "mse:  0.000807838148665\n",
      "validate on 30 steps, mse on train / validation data: 0.00081 / 0.00068\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.17151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.17151 to 1.22318, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22318 to 0.66304, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.96452, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66304 to 0.11085, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11085 to 0.07196, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07196 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03687 to 0.03687, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03687 to 0.00837, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00837 to 0.00530, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00530 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00227 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00175 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00161 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00113 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00107 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00103 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00100 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00096 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00093 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00090 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00087 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00085 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00070 to 0.00069, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00068 to 0.00067, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00067 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00065 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00065 to 0.00064, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00064 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00063 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00061 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00061 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00060 to 0.00060, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00060 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00058 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00057 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00056 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00055 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00052 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00051 to 0.00051, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00051 to 0.00049, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00049 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00048 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00047 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00047 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00046 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00046 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00045 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00045 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00044 to 0.00043, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00043 to 0.00042, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00042 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00041 to 0.00041, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00041 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00038 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00037 to 0.00037, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00037 to 0.00036, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00036 to 0.00035, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00044, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00051, did not improve\n",
      "Epoch 00223: early stopping\n",
      "Using epoch 00156 with val_loss: 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 665us/step\n",
      "mse:  0.0250337426974\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 799us/step\n",
      "mse:  0.0298722941786\n",
      "validate on 5 steps, mse on train / validation data: 0.02987 / 0.02503\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  4.91890081492\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  5.00050854009\n",
      "validate on 10 steps, mse on train / validation data: 5.00051 / 4.91890\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000351062770113\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000462018283239\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00035\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  3.78144966472\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  3.87956652129\n",
      "validate on 30 steps, mse on train / validation data: 3.87957 / 3.78145\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11390, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11390 to 0.05036, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05036 to 0.02813, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02813 to 0.01418, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01418 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00294, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00257 to 0.00224, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00224 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00218 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00158 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00129 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00126 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00113 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00106 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00100 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00098 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00097 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00096 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00092 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00091 to 0.00090, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00090 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00089 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00085 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00083 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00081 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss is 0.00073, did not improve\n",
      "Epoch 00213: early stopping\n",
      "Using epoch 00212 with val_loss: 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 614us/step\n",
      "mse:  0.00897776255045\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 853us/step\n",
      "mse:  0.0123261121959\n",
      "validate on 5 steps, mse on train / validation data: 0.01233 / 0.00898\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00932959319008\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.0085558105197\n",
      "validate on 10 steps, mse on train / validation data: 0.00856 / 0.00933\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000731091864344\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000461046307382\n",
      "validate on 20 steps, mse on train / validation data: 0.00046 / 0.00073\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000526615978742\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.000265021034481\n",
      "validate on 30 steps, mse on train / validation data: 0.00027 / 0.00053\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [  1.99500000e-02   1.64633000e+00   5.20000000e-04   1.26088000e+00] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.58300000e-02   1.07700000e-02   4.70000000e-04   6.80000000e-04]\n",
      " [  2.50300000e-02   4.91890000e+00   3.50000000e-04   3.78145000e+00]\n",
      " [  8.98000000e-03   9.33000000e-03   7.30000000e-04   5.30000000e-04]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [  2.28500000e-02   1.67272000e+00   4.70000000e-04   1.29355000e+00] ***\n",
      "Results training data of all Folds: \n",
      "[[  2.63400000e-02   9.11000000e-03   5.00000000e-04   8.10000000e-04]\n",
      " [  2.98700000e-02   5.00051000e+00   4.60000000e-04   3.87957000e+00]\n",
      " [  1.23300000e-02   8.56000000e-03   4.60000000e-04   2.70000000e-04]]\n",
      "[[  1.08437327e-02   1.09259789e-02   1.24323155e-02   1.33898463e-02]\n",
      " [  2.90761069e-01   6.71317882e-04   2.71672200e-01   2.80506009e-01]\n",
      " [  2.28470799e-02   1.67272405e+00   4.74408716e-04   1.29354646e+00]]\n",
      "[[  8.95270427e-03   9.04494103e-03   1.05073458e-02   1.13416525e-02]\n",
      " [  3.06428550e-01   7.81051771e-04   3.27801269e-01   2.51684709e-01]\n",
      " [  1.99470833e-02   1.64633448e+00   5.17195930e-04   1.26088395e+00]]\n"
     ]
    }
   ],
   "source": [
    "# task 3.3 base line training with fixed lenghts (on final epoch)\n",
    "cfg = {'batch_size': 20, 'lr': 0.002}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='finalstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 20, 'lr': 0.002}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04549, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.05167, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04549 to 0.01942, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.02287, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01942 to 0.01751, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01751 to 0.01720, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01720 to 0.00615, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00975, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00615 to 0.00555, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00555 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00528, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00488 to 0.00413, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01056, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00447, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00413 to 0.00326, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss is 0.00520, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00326 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00456, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00670, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00436, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00419, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00444, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00388, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00448, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00257 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00385, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00239 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00321, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00418, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00341, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00328, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00157 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00274, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00178, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00249, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00222, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00387, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00424, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00260, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00883, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00147 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00130 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00104 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00305, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00316, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00278, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00233, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00226, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00324, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00373, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss is 0.00263, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00264, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00394, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00082 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00070 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00289, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00346, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00168, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00156, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00238, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00063 to 0.00057, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00337, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00142, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00319, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00374, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00254, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00057 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00235, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00154, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00098, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00199, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00152, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00085, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00176, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00090, did not improve\n",
      "Epoch 00248: early stopping\n",
      "Using epoch 00192 with val_loss: 0.00053\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 381us/step\n",
      "mse:  0.00152149519836\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 807us/step\n",
      "mse:  0.00137692030099\n",
      "validate on 5 steps, mse on train / validation data: 0.00138 / 0.00152\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 1ms/step\n",
      "mse:  0.000526376222613\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 1ms/step\n",
      "mse:  0.000751574570049\n",
      "validate on 10 steps, mse on train / validation data: 0.00075 / 0.00053\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 2ms/step\n",
      "mse:  0.000635943812412\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 0s 2ms/step\n",
      "mse:  0.000685406969834\n",
      "validate on 20 steps, mse on train / validation data: 0.00069 / 0.00064\n",
      "evaluate lstm with consideration of configs\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "mse:  0.00172576026238\n",
      "evaluate lstm with consideration of configs\n",
      "176/176 [==============================] - 1s 3ms/step\n",
      "mse:  0.00197781999437\n",
      "validate on 30 steps, mse on train / validation data: 0.00198 / 0.00173\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28546, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 12.84484, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 2.77282, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 2.25465, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 0.57671, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 1.12980, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.35501, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.28546 to 0.12109, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12109 to 0.03442, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.05426, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03442 to 0.02533, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02533 to 0.02353, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.02599, did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02353 to 0.02270, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02270 to 0.02023, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02023 to 0.01940, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01940 to 0.01835, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01835 to 0.01375, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01375 to 0.00735, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00735 to 0.00401, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00403, did not improve\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00401 to 0.00325, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00325 to 0.00247, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.00391, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00490, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00291, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00403, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00247 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00412, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00174 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00431, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00123 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00187, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00236, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00230, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00191, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00277, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00384, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00322, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00266, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00086 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00363, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00407, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00177, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00212, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00258, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00265, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00161, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00160, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00167, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00151, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00079, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00343, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00213, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00239, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00315, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00256, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00120, did not improve\n",
      "Epoch 00145: early stopping\n",
      "Using epoch 00070 with val_loss: 0.00068\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 667us/step\n",
      "mse:  0.00348511340351\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 869us/step\n",
      "mse:  0.00441753392839\n",
      "validate on 5 steps, mse on train / validation data: 0.00442 / 0.00349\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.000761275335787\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.00105889618961\n",
      "validate on 10 steps, mse on train / validation data: 0.00106 / 0.00076\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000584286765926\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000738992010629\n",
      "validate on 20 steps, mse on train / validation data: 0.00074 / 0.00058\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.000752295505001\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 1s 3ms/step\n",
      "mse:  0.0010405342926\n",
      "validate on 30 steps, mse on train / validation data: 0.00104 / 0.00075\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train with random nr. of epochs, evaluate with 0 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08260, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08260 to 0.03279, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03279 to 0.01579, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01579 to 0.00783, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00783 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00634 to 0.00532, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00532 to 0.00347, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 0.00347 to 0.00268, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.00367, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00422, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00427, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00396, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00361, did not improve\n",
      "\n",
      "Epoch 00016: val_loss is 0.00342, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00372, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00343, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00333, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00273, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00268 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00404, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00446, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00435, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00449, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00493, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00451, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00436, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00443, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00237, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00379, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00350, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00413, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00432, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00348, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00470, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00218 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00255, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00335, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00311, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00377, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00188, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00365, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00221, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00535, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00241, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00175, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00207, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00334, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00185, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00296, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00234, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00275, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00284, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00382, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00299, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00381, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00344, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00398, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00357, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00276, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00197, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00313, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00218, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00399, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00345, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00297, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00211, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00169 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00306, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00379, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00257, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00169, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00202, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00196, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00181, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00163, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00179, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00325, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00267, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00198, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00247, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00203, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00272, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00224, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00326, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00245, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00125 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00190, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00225, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00205, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00186, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00174, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00216, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00195, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00251, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00217, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00231, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00150, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00323, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00215, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00312, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00200, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00268, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00295, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00286, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00201, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00270, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00378, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00402, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00229, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00242, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00354, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00243, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00158, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00228, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00248, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00219, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00182, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00209, did not improve\n",
      "Epoch 00166: early stopping\n",
      "Using epoch 00118 with val_loss: 0.00122\n",
      "evaluate lstm with consideration of configs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 915us/step\n",
      "mse:  0.00519909668417\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 875us/step\n",
      "mse:  0.00466729941289\n",
      "validate on 5 steps, mse on train / validation data: 0.00467 / 0.00520\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 1ms/step\n",
      "mse:  0.00113231386852\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 1ms/step\n",
      "mse:  0.000758179737384\n",
      "validate on 10 steps, mse on train / validation data: 0.00076 / 0.00113\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 2ms/step\n",
      "mse:  0.000796099263508\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 2ms/step\n",
      "mse:  0.000717087945767\n",
      "validate on 20 steps, mse on train / validation data: 0.00072 / 0.00080\n",
      "evaluate lstm with consideration of configs\n",
      "88/88 [==============================] - 0s 3ms/step\n",
      "mse:  0.00134987773014\n",
      "evaluate lstm with consideration of configs\n",
      "177/177 [==============================] - 0s 3ms/step\n",
      "mse:  0.00113224787874\n",
      "validate on 30 steps, mse on train / validation data: 0.00113 / 0.00135\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0034   0.00081  0.00067  0.00128] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00152  0.00053  0.00064  0.00173]\n",
      " [ 0.00349  0.00076  0.00058  0.00075]\n",
      " [ 0.0052   0.00113  0.0008   0.00135]]\n",
      "MSE on training data on [5, 10, 20, 30] steps: means over folds: *** [ 0.00349  0.00086  0.00071  0.00138] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.00138  0.00075  0.00069  0.00198]\n",
      " [ 0.00442  0.00106  0.00074  0.00104]\n",
      " [ 0.00467  0.00076  0.00072  0.00113]]\n",
      "[ 0.00348725  0.00085622  0.00071383  0.00138353] [ 0.0034019   0.00080666  0.00067211  0.00127598]\n"
     ]
    }
   ],
   "source": [
    "# 3.3 train train using final points with random lenghts\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, mode='finalstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 0 epochs, train on 0 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'lr': 0.08119864140758115, 'subsample': 0.7946631901813815, 'n_estimators': 1000, 'gamma': 0.007833441242813044, 'maxdepth': 10, 'cols_bt': 0.9376450587145334}\n",
      "train fold 1 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.584156378552\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.578395060919\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.527057613488\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.500720170913\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.494855966833\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.454835391707\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.458230453509\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.465432094203\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.472633742624\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.44094650061\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.398662551686\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.417901235598\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.432510285466\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.37952675422\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.399382723702\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.371913578775\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.374074074957\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.378189303257\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.351543205756\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.330349789725\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.318209884343\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.36121398652\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.320370373902\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.34917695434\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.318004115864\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.313580245883\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.321296292323\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.32448559558\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.34002057049\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.311934159862\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.339094645447\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.330349800763\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.306172834502\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.32294238276\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.302572014155\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.326982 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.318048 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.325919 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308534 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.317099 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.306027 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.311122 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.301471 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.301088 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.296533 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.296533 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.296533 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.296533 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.296533 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.296533 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.296533 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.296533 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.296533 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.296533 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.296533 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.296533 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.296533 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.296533 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.296533 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.296533 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.296533 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.296533 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.296533 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.296533 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.296533 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.296533 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.296533 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.827147 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.792634 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.71391 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.742944 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.870816 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.838877 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.817926 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.842618 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.874557 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.874557 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.874557 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.874557 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.874557 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.874557 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.874557 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.874557 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.874557 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.874557 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.874557 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.874557 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.874557 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.874557 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.874557 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.874557 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.874557 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.874557 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.874557 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.874557 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.874557 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.874557 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.874557 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.874557 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.874557 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.874557 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.874557 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.281761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293365 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.273654 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.287996 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27613 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.285244 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.273654 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.283982 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.273654 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281323 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.273654 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27613 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.273654 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.273654 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.273654 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.273654 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.273654 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.273654 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.273654 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.273654 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.273654 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.273654 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.273654 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.273654 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.273654 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.273654 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.273654 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.273654 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.273654 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.273654 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.542816 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.474905 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.501156 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.474768 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.559699 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.470011 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.501156 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.474768 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.584386 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470011 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.501156 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.474768 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.692753 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470011 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.501156 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.50006 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.771477 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.479863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.501156 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.504955 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.792428 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.479863 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.501156 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.504955 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.792428 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.479863 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.501156 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.504955 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.792428 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.479863 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.240653 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.251472 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.244716 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.239961 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.244298 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.244716 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.244298 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.244298 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.244298 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.244298 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.244298 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.244298 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.244298 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.244298 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.244298 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.244298 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.244298 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.244298 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.244298 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.244298 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.387555 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.383345 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.377112 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.367352 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.374395 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.365674 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.374395 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.35863 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.369658 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.354651 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.365404 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.354651 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.354376 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.354651 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.350397 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.350397 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.348069 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.348069 / 0.32294238276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 38 prediction / true value for lc number 13 0.348069 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.348069 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221099 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.221099 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.214998 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.216762 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.216762 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.214998 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.214998 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.214998 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.214998 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.214998 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.316017 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.320824 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.311122 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311122 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.306027 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.306027 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.301471 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.296533 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.296533 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.296533 / 0.330041154667\n",
      "validate on 30 steps, mse on train / validation data: 0.00049 / 0.00105\n",
      "train fold 2 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n",
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 5 prediction / true value for lc number 13 0.806425 / 0.578395060919\n",
      "step nr. 6 prediction / true value for lc number 13 0.756053 / 0.527057613488\n",
      "step nr. 7 prediction / true value for lc number 13 0.741435 / 0.500720170913\n",
      "step nr. 8 prediction / true value for lc number 13 0.735141 / 0.494855966833\n",
      "step nr. 9 prediction / true value for lc number 13 0.860325 / 0.454835391707\n",
      "step nr. 10 prediction / true value for lc number 13 0.814813 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.810539 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.829144 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.878929 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.878929 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.878929 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.878929 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.878929 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.878929 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.878929 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.878929 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.878929 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.878929 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.878929 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.878929 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.878929 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.878929 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.878929 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.878929 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.878929 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.878929 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.878929 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.878929 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.878929 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.878929 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.878929 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.878929 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.878929 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.878929 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.878929 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.33173 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.308723 / 0.275050303766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 7 prediction / true value for lc number 13 0.323158 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.308723 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.311356 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.308723 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.307247 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.307247 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.307247 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.307247 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.307247 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.307247 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.307247 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.307247 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.307247 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.307247 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.307247 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.307247 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.307247 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.307247 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.307247 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.307247 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.307247 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.307247 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.307247 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.307247 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.307247 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.307247 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.307247 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.307247 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 10 prediction / true value for lc number 13 0.518932 / 0.458230453509\n",
      "step nr. 11 prediction / true value for lc number 13 0.496154 / 0.465432094203\n",
      "step nr. 12 prediction / true value for lc number 13 0.47981 / 0.472633742624\n",
      "step nr. 13 prediction / true value for lc number 13 0.470411 / 0.44094650061\n",
      "step nr. 14 prediction / true value for lc number 13 0.518932 / 0.398662551686\n",
      "step nr. 15 prediction / true value for lc number 13 0.478753 / 0.417901235598\n",
      "step nr. 16 prediction / true value for lc number 13 0.478264 / 0.432510285466\n",
      "step nr. 17 prediction / true value for lc number 13 0.470411 / 0.37952675422\n",
      "step nr. 18 prediction / true value for lc number 13 0.518932 / 0.399382723702\n",
      "step nr. 19 prediction / true value for lc number 13 0.470411 / 0.371913578775\n",
      "step nr. 20 prediction / true value for lc number 13 0.478264 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.470411 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.518932 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.470411 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.478264 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.470411 / 0.36121398652\n",
      "step nr. 26 prediction / true value for lc number 13 0.518932 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.470411 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.478264 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.470411 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.518932 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.470411 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.478264 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.470411 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.518932 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.470411 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.478264 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.470411 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.518932 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.470411 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.284761 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.293934 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.292961 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295669 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.291778 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.301144 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.295669 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.301144 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.301144 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.301144 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.301144 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.301144 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.301144 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.301144 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.301144 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.301144 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.301144 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.301144 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.301144 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.301144 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.301144 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.301144 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.301144 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.301144 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.301144 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.301144 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.301144 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.301144 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.301144 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.301144 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 20 prediction / true value for lc number 13 0.402062 / 0.374074074957\n",
      "step nr. 21 prediction / true value for lc number 13 0.379441 / 0.378189303257\n",
      "step nr. 22 prediction / true value for lc number 13 0.384904 / 0.351543205756\n",
      "step nr. 23 prediction / true value for lc number 13 0.374249 / 0.330349789725\n",
      "step nr. 24 prediction / true value for lc number 13 0.384904 / 0.318209884343\n",
      "step nr. 25 prediction / true value for lc number 13 0.371541 / 0.36121398652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 26 prediction / true value for lc number 13 0.376876 / 0.320370373902\n",
      "step nr. 27 prediction / true value for lc number 13 0.366349 / 0.34917695434\n",
      "step nr. 28 prediction / true value for lc number 13 0.376876 / 0.318004115864\n",
      "step nr. 29 prediction / true value for lc number 13 0.363279 / 0.313580245883\n",
      "step nr. 30 prediction / true value for lc number 13 0.366349 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.359094 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.360761 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.359094 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.356575 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.351391 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.356575 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.351391 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.349468 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.344566 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.238221 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.243812 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.231052 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.238221 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.231052 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.231052 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.231052 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.231052 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.231052 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.231052 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.231052 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.231052 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.231052 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.231052 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.231052 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.231052 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.231052 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.231052 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.231052 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.231052 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.79588478  0.74742798  0.65154321  0.61779835  0.58415638  0.57839506\n",
      "  0.52705761  0.50072017  0.49485597  0.45483539  0.45823045  0.46543209\n",
      "  0.47263374  0.4409465   0.39866255  0.41790124  0.43251029  0.37952675\n",
      "  0.39938272  0.37191358  0.37407407  0.3781893   0.35154321  0.33034979\n",
      "  0.31820988  0.36121399  0.32037037  0.34917695  0.31800412  0.31358025\n",
      "  0.32129629  0.3244856   0.34002057  0.31193416  0.33909465  0.3303498\n",
      "  0.30617283  0.32294238  0.30257201  0.33004115]\n",
      "step nr. 30 prediction / true value for lc number 13 0.31241 / 0.321296292323\n",
      "step nr. 31 prediction / true value for lc number 13 0.329096 / 0.32448559558\n",
      "step nr. 32 prediction / true value for lc number 13 0.308723 / 0.34002057049\n",
      "step nr. 33 prediction / true value for lc number 13 0.311356 / 0.311934159862\n",
      "step nr. 34 prediction / true value for lc number 13 0.308723 / 0.339094645447\n",
      "step nr. 35 prediction / true value for lc number 13 0.307247 / 0.330349800763\n",
      "step nr. 36 prediction / true value for lc number 13 0.307247 / 0.306172834502\n",
      "step nr. 37 prediction / true value for lc number 13 0.307247 / 0.32294238276\n",
      "step nr. 38 prediction / true value for lc number 13 0.307247 / 0.302572014155\n",
      "step nr. 39 prediction / true value for lc number 13 0.307247 / 0.330041154667\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.221494 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.216509 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.212515 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.214149 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.209929 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.209929 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.209929 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.207869 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.207869 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.207869 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00050 / 0.00036\n",
      "train fold 3 on 0 steps, validation on 0 steps\n",
      "train on new epoch 5 true value for curve no. 13 (example) 0.297987927284\n",
      "train on new epoch 6 true value for curve no. 13 (example) 0.276659959129\n",
      "train on new epoch 7 true value for curve no. 13 (example) 0.275050303766\n",
      "train on new epoch 8 true value for curve no. 13 (example) 0.288933598569\n",
      "train on new epoch 9 true value for curve no. 13 (example) 0.275251509888\n",
      "train on new epoch 10 true value for curve no. 13 (example) 0.311066399728\n",
      "train on new epoch 11 true value for curve no. 13 (example) 0.257645875216\n",
      "train on new epoch 12 true value for curve no. 13 (example) 0.26348088256\n",
      "train on new epoch 13 true value for curve no. 13 (example) 0.282394366605\n",
      "train on new epoch 14 true value for curve no. 13 (example) 0.245372237904\n",
      "train on new epoch 15 true value for curve no. 13 (example) 0.252716300743\n",
      "train on new epoch 16 true value for curve no. 13 (example) 0.246177060263\n",
      "train on new epoch 17 true value for curve no. 13 (example) 0.235714284437\n",
      "train on new epoch 18 true value for curve no. 13 (example) 0.264486921685\n",
      "train on new epoch 19 true value for curve no. 13 (example) 0.253319919109\n",
      "train on new epoch 20 true value for curve no. 13 (example) 0.230482899717\n",
      "train on new epoch 21 true value for curve no. 13 (example) 0.227062367967\n",
      "train on new epoch 22 true value for curve no. 13 (example) 0.232796779701\n",
      "train on new epoch 23 true value for curve no. 13 (example) 0.227364180343\n",
      "train on new epoch 24 true value for curve no. 13 (example) 0.229476860591\n",
      "train on new epoch 25 true value for curve no. 13 (example) 0.229577464717\n",
      "train on new epoch 26 true value for curve no. 13 (example) 0.248390346766\n",
      "train on new epoch 27 true value for curve no. 13 (example) 0.242354122656\n",
      "train on new epoch 28 true value for curve no. 13 (example) 0.243561365775\n",
      "train on new epoch 29 true value for curve no. 13 (example) 0.22173038125\n",
      "train on new epoch 30 true value for curve no. 13 (example) 0.215492953147\n",
      "train on new epoch 31 true value for curve no. 13 (example) 0.219617709517\n",
      "train on new epoch 32 true value for curve no. 13 (example) 0.235211265939\n",
      "train on new epoch 33 true value for curve no. 13 (example) 0.230382293463\n",
      "train on new epoch 34 true value for curve no. 13 (example) 0.227565382208\n",
      "train on new epoch 35 true value for curve no. 13 (example) 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on new epoch 36 true value for curve no. 13 (example) 0.331790747387\n",
      "train on new epoch 37 true value for curve no. 13 (example) 0.246981897524\n",
      "train on new epoch 38 true value for curve no. 13 (example) 0.214084506035\n",
      "train on new epoch 39 true value for curve no. 13 (example) 0.214084501777\n",
      "train on new epoch 40 true value for curve no. 13 (example) 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 5 prediction / true value for lc number 13 0.485691 / 0.421210902518\n",
      "step nr. 6 prediction / true value for lc number 13 0.437519 / 0.407979146417\n",
      "step nr. 7 prediction / true value for lc number 13 0.424689 / 0.396952684712\n",
      "step nr. 8 prediction / true value for lc number 13 0.4422 / 0.393143543139\n",
      "step nr. 9 prediction / true value for lc number 13 0.451538 / 0.383019244234\n",
      "step nr. 10 prediction / true value for lc number 13 0.421922 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.419173 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.43516 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.424689 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.415883 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.415883 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.419563 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.409141 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.40588 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.406599 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.397296 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.397296 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.397296 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.394435 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.394435 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.394435 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.394435 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.389773 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.389773 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.386784 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.376037 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.373892 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.373892 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.373892 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.373892 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.363841 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.361281 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.361281 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.35565 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.35565 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 5\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 5 prediction / true value for lc number 13 0.311133 / 0.276659959129\n",
      "step nr. 6 prediction / true value for lc number 13 0.311415 / 0.275050303766\n",
      "step nr. 7 prediction / true value for lc number 13 0.308342 / 0.288933598569\n",
      "step nr. 8 prediction / true value for lc number 13 0.29057 / 0.275251509888\n",
      "step nr. 9 prediction / true value for lc number 13 0.305116 / 0.311066399728\n",
      "step nr. 10 prediction / true value for lc number 13 0.305116 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.300425 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.289175 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.295496 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.289611 / 0.252716300743\n",
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.28329 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.281266 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 10 prediction / true value for lc number 13 0.394435 / 0.378809140802\n",
      "step nr. 11 prediction / true value for lc number 13 0.391446 / 0.375400962886\n",
      "step nr. 12 prediction / true value for lc number 13 0.379026 / 0.373596630477\n",
      "step nr. 13 prediction / true value for lc number 13 0.384639 / 0.3625701689\n",
      "step nr. 14 prediction / true value for lc number 13 0.383023 / 0.358059343721\n",
      "step nr. 15 prediction / true value for lc number 13 0.376037 / 0.360866077244\n",
      "step nr. 16 prediction / true value for lc number 13 0.373892 / 0.352947072736\n",
      "step nr. 17 prediction / true value for lc number 13 0.373892 / 0.35244586961\n",
      "step nr. 18 prediction / true value for lc number 13 0.373892 / 0.345328789204\n",
      "step nr. 19 prediction / true value for lc number 13 0.373892 / 0.34653167388\n",
      "step nr. 20 prediction / true value for lc number 13 0.363841 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.361281 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.361281 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.35565 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.35565 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 10\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 10 prediction / true value for lc number 13 0.280891 / 0.257645875216\n",
      "step nr. 11 prediction / true value for lc number 13 0.289175 / 0.26348088256\n",
      "step nr. 12 prediction / true value for lc number 13 0.277666 / 0.282394366605\n",
      "step nr. 13 prediction / true value for lc number 13 0.297836 / 0.245372237904\n",
      "step nr. 14 prediction / true value for lc number 13 0.27627 / 0.252716300743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 15 prediction / true value for lc number 13 0.289175 / 0.246177060263\n",
      "step nr. 16 prediction / true value for lc number 13 0.27627 / 0.235714284437\n",
      "step nr. 17 prediction / true value for lc number 13 0.28329 / 0.264486921685\n",
      "step nr. 18 prediction / true value for lc number 13 0.27627 / 0.253319919109\n",
      "step nr. 19 prediction / true value for lc number 13 0.281266 / 0.230482899717\n",
      "step nr. 20 prediction / true value for lc number 13 0.27627 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.27627 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.27627 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.27627 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.27627 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.27627 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.27627 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.27627 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.27627 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.27627 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.27627 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.27627 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.27627 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.27627 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.27627 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.27627 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.27627 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.27627 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.27627 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.27627 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 20 prediction / true value for lc number 13 0.342239 / 0.341920611545\n",
      "step nr. 21 prediction / true value for lc number 13 0.342239 / 0.341018444762\n",
      "step nr. 22 prediction / true value for lc number 13 0.342239 / 0.336908581313\n",
      "step nr. 23 prediction / true value for lc number 13 0.342239 / 0.341619886843\n",
      "step nr. 24 prediction / true value for lc number 13 0.342239 / 0.335805934693\n",
      "step nr. 25 prediction / true value for lc number 13 0.342239 / 0.333099439098\n",
      "step nr. 26 prediction / true value for lc number 13 0.342239 / 0.333400159689\n",
      "step nr. 27 prediction / true value for lc number 13 0.342239 / 0.331996794148\n",
      "step nr. 28 prediction / true value for lc number 13 0.342239 / 0.325581397219\n",
      "step nr. 29 prediction / true value for lc number 13 0.342239 / 0.326684044995\n",
      "step nr. 30 prediction / true value for lc number 13 0.342239 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.342239 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.342239 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.342239 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.342239 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.342239 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.342239 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.342239 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.342239 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.342239 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 20\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 20 prediction / true value for lc number 13 0.228495 / 0.227062367967\n",
      "step nr. 21 prediction / true value for lc number 13 0.228228 / 0.232796779701\n",
      "step nr. 22 prediction / true value for lc number 13 0.227044 / 0.227364180343\n",
      "step nr. 23 prediction / true value for lc number 13 0.227044 / 0.229476860591\n",
      "step nr. 24 prediction / true value for lc number 13 0.227044 / 0.229577464717\n",
      "step nr. 25 prediction / true value for lc number 13 0.227044 / 0.248390346766\n",
      "step nr. 26 prediction / true value for lc number 13 0.227044 / 0.242354122656\n",
      "step nr. 27 prediction / true value for lc number 13 0.227044 / 0.243561365775\n",
      "step nr. 28 prediction / true value for lc number 13 0.227044 / 0.22173038125\n",
      "step nr. 29 prediction / true value for lc number 13 0.227044 / 0.215492953147\n",
      "step nr. 30 prediction / true value for lc number 13 0.227044 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.227044 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.227044 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.227044 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.227044 / 0.224044261234\n",
      "step nr. 35 prediction / true value for lc number 13 0.227044 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.227044 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.227044 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.227044 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.227044 / 0.205030181578\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.56285084  0.50601443  0.46792301  0.44506816  0.42983159  0.4212109\n",
      "  0.40797915  0.39695268  0.39314354  0.38301924  0.37880914  0.37540096\n",
      "  0.37359663  0.36257017  0.35805934  0.36086608  0.35294707  0.35244587\n",
      "  0.34532879  0.34653167  0.34192061  0.34101844  0.33690858  0.34161989\n",
      "  0.33580593  0.33309944  0.33340016  0.33199679  0.3255814   0.32668404\n",
      "  0.32407779  0.32457899  0.3223737   0.32117081  0.32056937  0.32317563\n",
      "  0.31776263  0.32016841  0.31345229  0.31134724]\n",
      "step nr. 30 prediction / true value for lc number 13 0.324573 / 0.324077786171\n",
      "step nr. 31 prediction / true value for lc number 13 0.324573 / 0.324578990196\n",
      "step nr. 32 prediction / true value for lc number 13 0.324573 / 0.322373699011\n",
      "step nr. 33 prediction / true value for lc number 13 0.324573 / 0.321170811509\n",
      "step nr. 34 prediction / true value for lc number 13 0.324573 / 0.320569367116\n",
      "step nr. 35 prediction / true value for lc number 13 0.324573 / 0.323175625169\n",
      "step nr. 36 prediction / true value for lc number 13 0.324573 / 0.317762631281\n",
      "step nr. 37 prediction / true value for lc number 13 0.324573 / 0.320168406414\n",
      "step nr. 38 prediction / true value for lc number 13 0.324573 / 0.313452286068\n",
      "step nr. 39 prediction / true value for lc number 13 0.324573 / 0.311347236536\n",
      "\n",
      "eval_xgb starting at step 30\n",
      "lcs [ 0.4387324   0.34678068  0.33108651  0.33480885  0.29798793  0.27665996\n",
      "  0.2750503   0.2889336   0.27525151  0.3110664   0.25764588  0.26348088\n",
      "  0.28239437  0.24537224  0.2527163   0.24617706  0.23571428  0.26448692\n",
      "  0.25331992  0.2304829   0.22706237  0.23279678  0.22736418  0.22947686\n",
      "  0.22957746  0.24839035  0.24235412  0.24356137  0.22173038  0.21549295\n",
      "  0.21961771  0.23521127  0.23038229  0.22756538  0.22404426  0.33179075\n",
      "  0.2469819   0.21408451  0.2140845   0.20503018]\n",
      "step nr. 30 prediction / true value for lc number 13 0.211502 / 0.219617709517\n",
      "step nr. 31 prediction / true value for lc number 13 0.211502 / 0.235211265939\n",
      "step nr. 32 prediction / true value for lc number 13 0.211502 / 0.230382293463\n",
      "step nr. 33 prediction / true value for lc number 13 0.211502 / 0.227565382208\n",
      "step nr. 34 prediction / true value for lc number 13 0.211502 / 0.224044261234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step nr. 35 prediction / true value for lc number 13 0.211502 / 0.331790747387\n",
      "step nr. 36 prediction / true value for lc number 13 0.211502 / 0.246981897524\n",
      "step nr. 37 prediction / true value for lc number 13 0.211502 / 0.214084506035\n",
      "step nr. 38 prediction / true value for lc number 13 0.211502 / 0.214084501777\n",
      "step nr. 39 prediction / true value for lc number 13 0.211502 / 0.205030181578\n",
      "validate on 30 steps, mse on train / validation data: 0.00048 / 0.00101\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04854  0.01058  0.00153  0.00081] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04451  0.0149   0.00263  0.00105]\n",
      " [ 0.06447  0.01171  0.00082  0.00036]\n",
      " [ 0.03663  0.00513  0.00113  0.00101]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.04393  0.01114  0.00172  0.00049] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.0508   0.01723  0.00177  0.00049]\n",
      " [ 0.04719  0.01064  0.00247  0.0005 ]\n",
      " [ 0.03379  0.00554  0.00093  0.00048]]\n",
      "results validation data \n",
      " [ 0.04854  0.01058  0.00153  0.00081]\n",
      "results training data\n",
      " [ 0.04393  0.01114  0.00172  0.00049]\n"
     ]
    }
   ],
   "source": [
    "# task 3.4 \n",
    "cfg = {'maxdepth': 10, 'lr': 0.08119864140758115, 'gamma': 0.007833441242813044, 'cols_bt': 0.9376450587145334, \n",
    "       'n_estimators': 1000, 'subsample': 0.7946631901813815}\n",
    "res_train, res_val = m.eval_cv('xgb_next', [configs, lcs], Y, steps=(0,[5,10,20,30]), cfg=cfg)\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    running hyperparameter optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139861376198400\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.00975 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.01163 -0.00698 -0.01064]\n",
      "hyperband obj crossval results 0.00975\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0221 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.03018 -0.01989 -0.01623]\n",
      "hyperband obj crossval results 0.0221\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01826 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02461 -0.01537 -0.0148 ]\n",
      "hyperband obj crossval results 0.01826\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.04739 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.0518  -0.04517 -0.0452 ]\n",
      "hyperband obj crossval results 0.04739\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "/home/jochen/tensorflow/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  **self._backend_args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n",
      "MSE on validation data on [5] steps: means over folds: *** 0.0611 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.06532 -0.05967 -0.0583 ]\n",
      "hyperband obj crossval results 0.0611\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on validation data on [5] steps: means over folds: *** 0.01885 ***\n",
      "Results validation data of all Folds: \n",
      "[-0.02477 -0.01592 -0.01586]\n",
      "hyperband obj crossval results 0.01885\n",
      "traj {'losses': [-0.00975, -0.04739, -0.0611], 'budgets': [10.0, 10.0, 10.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2)], 'time_finished': [0.1408522129058838, 0.5603299140930176, 0.603823184967041]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(0, 0, 0): {'config_info': {}, 'config': {'gamma': 0.07063600373860912, 'cols_bt': 0.9217170129716793, 'lr': 0.1050934646426613, 'n_estimators': 279, 'maxdepth': 3, 'subsample': 0.34897607207545855}}, (0, 0, 2): {'config_info': {}, 'config': {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}}, (1, 0, 0): {'config_info': {}, 'config': {'gamma': 0.7207615034783531, 'cols_bt': 0.7812935702980904, 'lr': 0.07416811991915133, 'n_estimators': 151, 'maxdepth': 4, 'subsample': 0.7113184460373105}}, (0, 0, 1): {'config_info': {}, 'config': {'gamma': 0.31305732316547696, 'cols_bt': 0.6343377810392044, 'lr': 0.0012906266637879222, 'n_estimators': 192, 'maxdepth': 9, 'subsample': 0.7704980400242586}}, (0, 0, 3): {'config_info': {}, 'config': {'gamma': 0.495817992217395, 'cols_bt': 0.23061156444509176, 'lr': 0.09241553780580979, 'n_estimators': 171, 'maxdepth': 7, 'subsample': 0.14650611279360481}}, (1, 0, 1): {'config_info': {}, 'config': {'gamma': 0.7589483101395147, 'cols_bt': 0.9768386904658107, 'lr': 0.7304905829910666, 'n_estimators': 182, 'maxdepth': 9, 'subsample': 0.3177857840924941}}}\n",
      "return best config:  {'gamma': 0.856382480028685, 'cols_bt': 0.3850090031280587, 'lr': 0.0012115364873114829, 'n_estimators': 49, 'maxdepth': 3, 'subsample': 0.5221361806440016}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='xgb', min_budget = 10, max_budget=40, run_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140187071411968\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.008436577394848653, 'batch_size': 19}\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015971832060604426, 'batch_size': 25}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 169us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 138us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 215us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.08455] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.10933]\n",
      " [ 0.07648]\n",
      " [ 0.06783]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.08455\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 108us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 168us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02906] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04199]\n",
      " [ 0.02392]\n",
      " [ 0.02129]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02906\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 193us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 191us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 167us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03437] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04792]\n",
      " [ 0.02945]\n",
      " [ 0.02575]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03437\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.009586457136379657, 'batch_size': 27}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 176us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 238us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03834] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.05514]\n",
      " [ 0.03206]\n",
      " [ 0.02784]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03834\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.06945643081756749, 'batch_size': 42}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 153us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 217us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 186us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03485] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04871]\n",
      " [ 0.02974]\n",
      " [ 0.02609]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03485\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 168us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 166us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 193us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03232] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04512]\n",
      " [ 0.02714]\n",
      " [ 0.02468]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03232\n",
      "cross validate 12 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 141us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 299us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03407] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04797]\n",
      " [ 0.02893]\n",
      " [ 0.02531]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03407\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 175us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 463us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 98us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02954] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04222]\n",
      " [ 0.02424]\n",
      " [ 0.02215]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02954\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.021337462797969355, 'batch_size': 40}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 164us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 116us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 156us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03475] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04788]\n",
      " [ 0.02994]\n",
      " [ 0.02643]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03475\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 116us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 115us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01829] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02596]\n",
      " [ 0.01484]\n",
      " [ 0.01408]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01829\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 132us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 152us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 175us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03005] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0427 ]\n",
      " [ 0.02536]\n",
      " [ 0.0221 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03005\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.016073094701798084, 'batch_size': 19}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 143us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 144us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 179us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03387] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04688]\n",
      " [ 0.0292 ]\n",
      " [ 0.02552]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03387\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013227285301914478, 'batch_size': 23}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 180us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 174us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0697] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.09383]\n",
      " [ 0.06129]\n",
      " [ 0.05398]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0697\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 159us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 123us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00868] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00941]\n",
      " [ 0.00721]\n",
      " [ 0.00941]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00868\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07388122579837485, 'batch_size': 33}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 142us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 150us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 149us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02134] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03079]\n",
      " [ 0.01677]\n",
      " [ 0.01646]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02134\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 141us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 189us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.0307] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0426 ]\n",
      " [ 0.02644]\n",
      " [ 0.02307]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.0307\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 110us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 111us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 122us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00811]\n",
      " [ 0.007  ]\n",
      " [ 0.00624]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 25 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0013453747549283328, 'batch_size': 54}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 89us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 142us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 140us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.09446] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.12028]\n",
      " [ 0.08502]\n",
      " [ 0.07807]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.09446\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 167us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 136us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 146us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00976] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.01218]\n",
      " [ 0.00842]\n",
      " [ 0.00868]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00976\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 155us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 145us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 132us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02208] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03183]\n",
      " [ 0.01757]\n",
      " [ 0.01684]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02208\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.04580026289754379, 'batch_size': 43}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 126us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 180us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 155us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02839] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.0411 ]\n",
      " [ 0.02349]\n",
      " [ 0.02059]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02839\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 100us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 124us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 89us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.03028] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04305]\n",
      " [ 0.02544]\n",
      " [ 0.02235]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.03028\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07769230011012973, 'batch_size': 41}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 161us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 194us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 137us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.01432] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.02139]\n",
      " [ 0.01098]\n",
      " [ 0.0106 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.01432\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.0015944533862384857, 'batch_size': 42}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 184us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 160us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 130us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.05398] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.07588]\n",
      " [ 0.046  ]\n",
      " [ 0.04005]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.05398\n",
      "cross validate 50 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.002410925627224085, 'batch_size': 52}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 278us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 128us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 190us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.04653] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.06616]\n",
      " [ 0.03931]\n",
      " [ 0.03413]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.04653\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.164477508973482, 'batch_size': 30}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 122us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 118us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 181us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00712] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00846]\n",
      " [ 0.00592]\n",
      " [ 0.00698]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.00712\n",
      "cross validate 100 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.07673276670965525, 'batch_size': 62}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "89/89 [==============================] - 0s 329us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 246us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "88/88 [==============================] - 0s 218us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.02235] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.03238]\n",
      " [ 0.01768]\n",
      " [ 0.01699]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n",
      "hyperband obj crossval results 0.02235\n",
      "traj {'time_finished': [2.2007694244384766, 3.744567394256592, 21.17214035987854, 31.383424997329712, 38.972952127456665], 'budgets': [12.5, 12.5, 25.0, 50.0, 100.0], 'losses': [0.08455, 0.02906, 0.01829, 0.00868, 0.00712], 'config_ids': [(0, 0, 1), (0, 0, 2), (0, 0, 2), (0, 0, 2), (0, 0, 2)]}\n",
      "best_cfg_id (0, 0, 2)\n",
      "all_configs {(1, 0, 3): {'config': {'lr': 0.0013453747549283328, 'batch_size': 54}, 'config_info': {}}, (0, 0, 7): {'config': {'lr': 0.021337462797969355, 'batch_size': 40}, 'config_info': {}}, (2, 0, 3): {'config': {'lr': 0.002410925627224085, 'batch_size': 52}, 'config_info': {}}, (0, 0, 2): {'config': {'lr': 0.37364476899503524, 'batch_size': 48}, 'config_info': {}}, (1, 0, 0): {'config': {'lr': 0.07769230011012973, 'batch_size': 41}, 'config_info': {}}, (0, 0, 6): {'config': {'lr': 0.07388122579837485, 'batch_size': 33}, 'config_info': {}}, (2, 0, 2): {'config': {'lr': 0.0015944533862384857, 'batch_size': 42}, 'config_info': {}}, (0, 0, 1): {'config': {'lr': 0.0015971832060604426, 'batch_size': 25}, 'config_info': {}}, (1, 0, 1): {'config': {'lr': 0.0013227285301914478, 'batch_size': 23}, 'config_info': {}}, (0, 0, 5): {'config': {'lr': 0.06945643081756749, 'batch_size': 42}, 'config_info': {}}, (0, 0, 0): {'config': {'lr': 0.008436577394848653, 'batch_size': 19}, 'config_info': {}}, (2, 0, 1): {'config': {'lr': 0.07673276670965525, 'batch_size': 62}, 'config_info': {}}, (0, 0, 4): {'config': {'lr': 0.009586457136379657, 'batch_size': 27}, 'config_info': {}}, (1, 0, 2): {'config': {'lr': 0.04580026289754379, 'batch_size': 43}, 'config_info': {}}, (2, 0, 0): {'config': {'lr': 0.164477508973482, 'batch_size': 30}, 'config_info': {}}, (0, 0, 3): {'config': {'lr': 0.016073094701798084, 'batch_size': 19}, 'config_info': {}}}\n",
      "return best config:  {'lr': 0.37364476899503524, 'batch_size': 48}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize(configs,Y,model_type='mlp', min_budget = 10, max_budget=100, \n",
    "                       run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'lr': 0.37364476899503524, 'batch_size': 48}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05283, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05283 to 0.04822, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04822 to 0.04465, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.04470, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04465 to 0.04354, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04354 to 0.04342, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 0.04631, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04342 to 0.04184, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04184 to 0.03921, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.04097, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03921 to 0.03690, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03690 to 0.03614, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03614 to 0.03320, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03320 to 0.03232, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03232 to 0.03137, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03137 to 0.02900, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.03021, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02900 to 0.02758, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02758 to 0.02378, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02378 to 0.02372, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02372 to 0.02174, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02174 to 0.02093, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02093 to 0.01898, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01898 to 0.01830, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.02128, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01830 to 0.01674, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01674 to 0.01596, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01726, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01596 to 0.01556, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss is 0.01675, did not improve\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01556 to 0.01350, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.01493, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01350 to 0.01340, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01340 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.01316 to 0.01282, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss is 0.01392, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.01335, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.01282 to 0.01170, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.01170 to 0.01109, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss is 0.01383, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.01208, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.01155, did not improve\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01109 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss is 0.01021, did not improve\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00966 to 0.00964, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss is 0.00969, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.01048, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00964 to 0.00926, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00926 to 0.00921, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss is 0.01064, did not improve\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00921 to 0.00887, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00887 to 0.00854, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00944, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00854 to 0.00832, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.01001, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01047, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00989, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00857, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00832 to 0.00812, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00862, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00812 to 0.00790, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00790 to 0.00775, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00835, did not improve\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00775 to 0.00770, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00792, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00770 to 0.00767, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss is 0.00903, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00767 to 0.00758, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss is 0.00767, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00793, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00952, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00893, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00829, did not improve\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00758 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss is 0.00776, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00941, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00813, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00949, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00865, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00753 to 0.00753, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00890, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00851, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00772, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00753 to 0.00751, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00987, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00810, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00822, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00836, did not improve\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00751 to 0.00745, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00745 to 0.00742, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00742 to 0.00741, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00940, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00815, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00741 to 0.00740, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss is 0.00751, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00740 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00733 to 0.00733, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.00733 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00732 to 0.00728, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00885, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00754, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.01038, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00737, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00762, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.01099, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00796, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.01269, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00764, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00831, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00728 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00727 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00791, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00746, did not improve\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00724 to 0.00719, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00719 to 0.00715, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00715 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00709 to 0.00709, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00742, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00721, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00869, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00726, did not improve\n",
      "\n",
      "Epoch 00247: val_loss is 0.00935, did not improve\n",
      "\n",
      "Epoch 00248: val_loss is 0.00823, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00953, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00709 to 0.00700, storing weights.\n",
      "\n",
      "Epoch 00256: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00268: val_loss improved from 0.00700 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00269: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.00699 to 0.00699, storing weights.\n",
      "\n",
      "Epoch 00279: val_loss is 0.00771, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00759, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.01133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00293: val_loss improved from 0.00699 to 0.00696, storing weights.\n",
      "\n",
      "Epoch 00294: val_loss is 0.00703, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00295: val_loss is 0.00723, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00733, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00298: val_loss improved from 0.00696 to 0.00695, storing weights.\n",
      "\n",
      "Epoch 00299: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00301: val_loss improved from 0.00695 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00302: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00773, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00911, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00766, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00895, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00689 to 0.00689, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00323: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00324: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00325: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00326: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00327: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00328: val_loss is 0.00755, did not improve\n",
      "\n",
      "Epoch 00329: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00330: val_loss is 0.00725, did not improve\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00689 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00333: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00334: val_loss is 0.00774, did not improve\n",
      "\n",
      "Epoch 00335: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00336: val_loss is 0.00756, did not improve\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00680 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00339: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00340: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00341: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00342: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00343: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00344: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00345: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00346: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00347: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00348: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00349: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00350: val_loss is 0.00921, did not improve\n",
      "\n",
      "Epoch 00351: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00352: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00353: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00354: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00355: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00356: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00357: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00358: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00359: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00360: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00361: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00362: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00363: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00364: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00365: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00366: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00367: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00368: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00369: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00370: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00371: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00679 to 0.00679, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00374: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00375: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00376: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00377: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00378: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00679 to 0.00669, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss is 0.00689, did not improve\n",
      "\n",
      "Epoch 00381: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00382: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00383: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00674, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00757, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00678, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00825, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00706, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00707, did not improve\n",
      "\n",
      "Epoch 00405: val_loss is 0.00722, did not improve\n",
      "\n",
      "Epoch 00406: val_loss improved from 0.00669 to 0.00666, storing weights.\n",
      "\n",
      "Epoch 00407: val_loss is 0.00775, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.00666 to 0.00664, storing weights.\n",
      "\n",
      "Epoch 00413: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.00664 to 0.00658, storing weights.\n",
      "\n",
      "Epoch 00418: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00423: val_loss improved from 0.00658 to 0.00657, storing weights.\n",
      "\n",
      "Epoch 00424: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00669, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.00657 to 0.00652, storing weights.\n",
      "\n",
      "Epoch 00428: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00430: val_loss improved from 0.00652 to 0.00647, storing weights.\n",
      "\n",
      "Epoch 00431: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.00647 to 0.00645, storing weights.\n",
      "\n",
      "Epoch 00433: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00434: val_loss is 0.00720, did not improve\n",
      "\n",
      "Epoch 00435: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00436: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00437: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00438: val_loss is 0.00681, did not improve\n",
      "\n",
      "Epoch 00439: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00440: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00441: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00442: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00443: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00444: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00445: val_loss is 0.00838, did not improve\n",
      "\n",
      "Epoch 00446: val_loss improved from 0.00645 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00447: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00448: val_loss improved from 0.00642 to 0.00638, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00449: val_loss is 0.00826, did not improve\n",
      "\n",
      "Epoch 00450: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00451: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00452: val_loss is 0.00788, did not improve\n",
      "\n",
      "Epoch 00453: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00454: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00455: val_loss is 0.00682, did not improve\n",
      "\n",
      "Epoch 00456: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00457: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00458: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00459: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00460: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00461: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00462: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00463: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00464: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00465: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00466: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00467: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00468: val_loss is 0.00665, did not improve\n",
      "\n",
      "Epoch 00469: val_loss is 0.00881, did not improve\n",
      "\n",
      "Epoch 00470: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00471: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00472: val_loss is 0.00699, did not improve\n",
      "\n",
      "Epoch 00473: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00474: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00475: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00476: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00477: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00478: val_loss is 0.00743, did not improve\n",
      "\n",
      "Epoch 00479: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00480: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00481: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00482: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00483: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00484: val_loss is 0.00691, did not improve\n",
      "\n",
      "Epoch 00485: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00486: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00487: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00488: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00638 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00490: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00491: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00492: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00493: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00494: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00495: val_loss is 0.00758, did not improve\n",
      "\n",
      "Epoch 00496: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00497: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00498: val_loss is 0.00752, did not improve\n",
      "\n",
      "Epoch 00499: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00500: val_loss is 0.00685, did not improve\n",
      "\n",
      "Epoch 00501: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00502: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00503: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00504: val_loss improved from 0.00634 to 0.00624, storing weights.\n",
      "\n",
      "Epoch 00505: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00506: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00507: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00508: val_loss is 0.00750, did not improve\n",
      "\n",
      "Epoch 00509: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00510: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00511: val_loss is 0.00675, did not improve\n",
      "\n",
      "Epoch 00512: val_loss is 0.00625, did not improve\n",
      "\n",
      "Epoch 00513: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00514: val_loss improved from 0.00624 to 0.00623, storing weights.\n",
      "\n",
      "Epoch 00515: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00516: val_loss is 0.00705, did not improve\n",
      "\n",
      "Epoch 00517: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00518: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00519: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00520: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00521: val_loss is 0.00664, did not improve\n",
      "\n",
      "Epoch 00522: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00523: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00524: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00525: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00526: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00527: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00528: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00529: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00530: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00531: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00532: val_loss is 0.00714, did not improve\n",
      "\n",
      "Epoch 00533: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00534: val_loss is 0.00710, did not improve\n",
      "\n",
      "Epoch 00535: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00536: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00537: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00538: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00539: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00540: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00541: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00542: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00543: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00544: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00545: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00546: val_loss is 0.00635, did not improve\n",
      "\n",
      "Epoch 00547: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00548: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00549: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00550: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00551: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00552: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00553: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00554: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00555: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00556: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00557: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00558: val_loss is 0.00680, did not improve\n",
      "\n",
      "Epoch 00559: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00560: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00561: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00562: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00563: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00564: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00565: val_loss is 0.00626, did not improve\n",
      "\n",
      "Epoch 00566: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00567: val_loss is 0.00694, did not improve\n",
      "\n",
      "Epoch 00568: val_loss improved from 0.00623 to 0.00620, storing weights.\n",
      "\n",
      "Epoch 00569: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00570: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00571: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00572: val_loss improved from 0.00620 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00573: val_loss improved from 0.00618 to 0.00617, storing weights.\n",
      "\n",
      "Epoch 00574: val_loss is 0.00629, did not improve\n",
      "\n",
      "Epoch 00575: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00576: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00577: val_loss is 0.00837, did not improve\n",
      "\n",
      "Epoch 00578: val_loss is 0.00638, did not improve\n",
      "\n",
      "Epoch 00579: val_loss is 0.00656, did not improve\n",
      "\n",
      "Epoch 00580: val_loss is 0.00647, did not improve\n",
      "\n",
      "Epoch 00581: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00582: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00583: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00584: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00585: val_loss is 0.00728, did not improve\n",
      "\n",
      "Epoch 00586: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00587: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00588: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00589: val_loss is 0.00713, did not improve\n",
      "Epoch 00589: early stopping\n",
      "Using epoch 00573 with val_loss: 0.00617\n",
      "89/89 [==============================] - 0s 147us/step\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03010, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.03055, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03010 to 0.02792, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02792 to 0.02726, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02965, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02726 to 0.02655, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02655 to 0.02478, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02478 to 0.02292, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02292 to 0.02190, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02190 to 0.02047, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02047 to 0.02037, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.02037 to 0.01899, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01899 to 0.01775, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01775 to 0.01697, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01697 to 0.01573, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01636, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01573 to 0.01439, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.01468, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01439 to 0.01316, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.01386, did not improve\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01316 to 0.01254, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01254 to 0.01225, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01225 to 0.01106, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01106 to 0.01068, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01068 to 0.01052, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01052 to 0.01034, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01034 to 0.00997, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00997 to 0.00945, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00945 to 0.00922, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00922 to 0.00892, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00892 to 0.00824, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00824 to 0.00786, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00789, did not improve\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00786 to 0.00732, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00732 to 0.00731, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00731 to 0.00727, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00731, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00727 to 0.00697, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00979, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00697 to 0.00668, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00712, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00668 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.01095, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00634 to 0.00626, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00695, did not improve\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00626 to 0.00612, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00612 to 0.00589, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.01025, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00747, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00679, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00954, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00592, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00589 to 0.00582, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00606, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00582 to 0.00569, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00623, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00608, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00612, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00655, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00700, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00569 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00556 to 0.00551, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00570, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00551 to 0.00549, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00806, did not improve\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00549 to 0.00547, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss is 0.00811, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00637, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00547 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00579, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00653, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00539 to 0.00539, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00558, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00595, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00768, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00583, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00580, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00560, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00556, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00651, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00553, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00631, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00821, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00844, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00540, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00548, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00634, did not improve\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00539 to 0.00533, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00692, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00538, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00169: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00632, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00547, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00781, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00578, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.01005, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00565, did not improve\n",
      "Epoch 00179: early stopping\n",
      "Using epoch 00164 with val_loss: 0.00533\n",
      "88/88 [==============================] - 0s 133us/step\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02599, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02599 to 0.02511, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 0.02629, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 0.02653, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02511 to 0.02334, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02334 to 0.02258, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02258 to 0.02167, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02167 to 0.02067, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02067 to 0.01985, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01985 to 0.01933, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01933 to 0.01810, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01810 to 0.01763, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01763 to 0.01690, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.01720, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01690 to 0.01602, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.01670, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01602 to 0.01461, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01461 to 0.01421, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.01462, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01421 to 0.01341, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01341 to 0.01294, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01294 to 0.01205, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01205 to 0.01202, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01202 to 0.01154, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss is 0.01522, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01154 to 0.01115, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01115 to 0.01074, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss is 0.01362, did not improve\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01074 to 0.01013, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01013 to 0.00966, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.01408, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.01051, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00966 to 0.00914, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00914 to 0.00879, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00879 to 0.00853, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00898, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00853 to 0.00821, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00924, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00856, did not improve\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00821 to 0.00762, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss is 0.00960, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00945, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00926, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00782, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00797, did not improve\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00762 to 0.00724, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss is 0.00871, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00748, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00816, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00763, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00761, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.01037, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00855, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00939, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00860, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00736, did not improve\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00724 to 0.00684, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00845, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00701, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00995, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00913, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00784, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01107, did not improve\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00684 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss is 0.00741, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00680 to 0.00661, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss is 0.00713, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.01168, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00790, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00807, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00715, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00711, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00778, did not improve\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00661 to 0.00655, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss is 0.00666, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00849, did not improve\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00655 to 0.00642, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.01441, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.01015, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00779, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00819, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00745, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.01430, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.01199, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00868, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00828, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00830, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00648, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00985, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00663, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00683, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00642 to 0.00636, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss is 0.00973, did not improve\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00636 to 0.00618, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00661, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00730, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00704, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00734, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00618 to 0.00609, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss is 0.00800, did not improve\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00609 to 0.00600, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00614, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.01702, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00600 to 0.00599, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00609, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00804, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00611, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00650, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00794, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_loss improved from 0.00599 to 0.00558, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss is 0.00633, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00686, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00841, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00619, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00645, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00597, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00709, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00558 to 0.00556, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00610, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00556 to 0.00548, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss is 0.00738, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00552, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00853, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00568, did not improve\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00548 to 0.00534, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00644, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00886, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00740, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00880, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00673, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00684, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00660, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00575, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00571, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00587, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00719, did not improve\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00534 to 0.00517, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00591, did not improve\n",
      "\n",
      "Epoch 00185: val_loss is 0.00716, did not improve\n",
      "\n",
      "Epoch 00186: val_loss is 0.00877, did not improve\n",
      "\n",
      "Epoch 00187: val_loss is 0.00798, did not improve\n",
      "\n",
      "Epoch 00188: val_loss is 0.00910, did not improve\n",
      "\n",
      "Epoch 00189: val_loss is 0.00760, did not improve\n",
      "\n",
      "Epoch 00190: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00191: val_loss is 0.00794, did not improve\n",
      "\n",
      "Epoch 00192: val_loss is 0.00641, did not improve\n",
      "\n",
      "Epoch 00193: val_loss is 0.00642, did not improve\n",
      "\n",
      "Epoch 00194: val_loss is 0.00708, did not improve\n",
      "\n",
      "Epoch 00195: val_loss is 0.00573, did not improve\n",
      "\n",
      "Epoch 00196: val_loss is 0.00809, did not improve\n",
      "\n",
      "Epoch 00197: val_loss is 0.00732, did not improve\n",
      "\n",
      "Epoch 00198: val_loss is 0.00967, did not improve\n",
      "\n",
      "Epoch 00199: val_loss is 0.00698, did not improve\n",
      "\n",
      "Epoch 00200: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00201: val_loss is 0.00727, did not improve\n",
      "\n",
      "Epoch 00202: val_loss is 0.00589, did not improve\n",
      "\n",
      "Epoch 00203: val_loss is 0.00785, did not improve\n",
      "\n",
      "Epoch 00204: val_loss is 0.00603, did not improve\n",
      "\n",
      "Epoch 00205: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00206: val_loss is 0.00598, did not improve\n",
      "\n",
      "Epoch 00207: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00208: val_loss is 0.00662, did not improve\n",
      "\n",
      "Epoch 00209: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00210: val_loss is 0.00572, did not improve\n",
      "\n",
      "Epoch 00211: val_loss is 0.00658, did not improve\n",
      "\n",
      "Epoch 00212: val_loss is 0.00627, did not improve\n",
      "\n",
      "Epoch 00213: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00214: val_loss is 0.00690, did not improve\n",
      "\n",
      "Epoch 00215: val_loss is 0.00519, did not improve\n",
      "\n",
      "Epoch 00216: val_loss is 0.00729, did not improve\n",
      "\n",
      "Epoch 00217: val_loss is 0.00667, did not improve\n",
      "\n",
      "Epoch 00218: val_loss is 0.00649, did not improve\n",
      "\n",
      "Epoch 00219: val_loss is 0.00839, did not improve\n",
      "\n",
      "Epoch 00220: val_loss is 0.00676, did not improve\n",
      "\n",
      "Epoch 00221: val_loss is 0.00799, did not improve\n",
      "\n",
      "Epoch 00222: val_loss is 0.00539, did not improve\n",
      "\n",
      "Epoch 00223: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00224: val_loss is 0.00615, did not improve\n",
      "\n",
      "Epoch 00225: val_loss is 0.00820, did not improve\n",
      "\n",
      "Epoch 00226: val_loss is 0.00657, did not improve\n",
      "\n",
      "Epoch 00227: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00228: val_loss is 0.00780, did not improve\n",
      "\n",
      "Epoch 00229: val_loss is 0.00677, did not improve\n",
      "\n",
      "Epoch 00230: val_loss is 0.00687, did not improve\n",
      "\n",
      "Epoch 00231: val_loss is 0.00565, did not improve\n",
      "\n",
      "Epoch 00232: val_loss is 0.00717, did not improve\n",
      "\n",
      "Epoch 00233: val_loss is 0.00992, did not improve\n",
      "\n",
      "Epoch 00234: val_loss is 0.00787, did not improve\n",
      "\n",
      "Epoch 00235: val_loss is 0.00542, did not improve\n",
      "\n",
      "Epoch 00236: val_loss is 0.00541, did not improve\n",
      "\n",
      "Epoch 00237: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00238: val_loss is 0.00693, did not improve\n",
      "\n",
      "Epoch 00239: val_loss is 0.00584, did not improve\n",
      "\n",
      "Epoch 00240: val_loss is 0.00724, did not improve\n",
      "\n",
      "Epoch 00241: val_loss is 0.00703, did not improve\n",
      "\n",
      "Epoch 00242: val_loss is 0.00744, did not improve\n",
      "\n",
      "Epoch 00243: val_loss is 0.00607, did not improve\n",
      "\n",
      "Epoch 00244: val_loss is 0.00753, did not improve\n",
      "\n",
      "Epoch 00245: val_loss is 0.00643, did not improve\n",
      "\n",
      "Epoch 00246: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00517 to 0.00492, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss is 0.00718, did not improve\n",
      "\n",
      "Epoch 00249: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00250: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00545, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00546, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00621, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00812, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00659, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00636, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00834, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00576, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00564, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00616, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00555, did not improve\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.00492 to 0.00490, storing weights.\n",
      "\n",
      "Epoch 00263: val_loss is 0.00562, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00596, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00858, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00599, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00961, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00846, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00550, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00557, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00735, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00696, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00544, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00500, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00508, did not improve\n",
      "\n",
      "Epoch 00279: val_loss is 0.00668, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00702, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00620, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00613, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00770, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00551, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00554, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00618, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00588, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00739, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00574, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00585, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00543, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00529, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00652, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00600, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00511, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00516, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00658, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00298: val_loss is 0.00582, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00639, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00630, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00525, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00569, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00527, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00590, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00988, did not improve\n",
      "\n",
      "Epoch 00306: val_loss is 0.00496, did not improve\n",
      "\n",
      "Epoch 00307: val_loss is 0.00986, did not improve\n",
      "\n",
      "Epoch 00308: val_loss is 0.00955, did not improve\n",
      "\n",
      "Epoch 00309: val_loss is 0.00561, did not improve\n",
      "\n",
      "Epoch 00310: val_loss is 0.00605, did not improve\n",
      "\n",
      "Epoch 00311: val_loss is 0.00531, did not improve\n",
      "\n",
      "Epoch 00312: val_loss is 0.00594, did not improve\n",
      "\n",
      "Epoch 00313: val_loss is 0.00624, did not improve\n",
      "\n",
      "Epoch 00314: val_loss is 0.00646, did not improve\n",
      "\n",
      "Epoch 00315: val_loss is 0.00559, did not improve\n",
      "\n",
      "Epoch 00316: val_loss is 0.00688, did not improve\n",
      "\n",
      "Epoch 00317: val_loss is 0.00640, did not improve\n",
      "\n",
      "Epoch 00318: val_loss is 0.00697, did not improve\n",
      "\n",
      "Epoch 00319: val_loss is 0.00566, did not improve\n",
      "\n",
      "Epoch 00320: val_loss is 0.00769, did not improve\n",
      "\n",
      "Epoch 00321: val_loss is 0.00563, did not improve\n",
      "\n",
      "Epoch 00322: val_loss is 0.00578, did not improve\n",
      "Epoch 00322: early stopping\n",
      "Using epoch 00262 with val_loss: 0.00490\n",
      "88/88 [==============================] - 0s 154us/step\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.00547] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.00617]\n",
      " [ 0.00533]\n",
      " [ 0.0049 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [] ***\n",
      "Results training data of all Folds: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "best_config = {'lr': 0.37364476899503524, 'batch_size': 48}\n",
    "results = m.eval_cv('mlp', configs, Y, cfg=best_cfg, epochs=1000, splits = 3, earlystop=True, \n",
    "                    dropout=False, lr_exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139639964378880\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.1874018718283377}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 57305.17739 / 49320.43900\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.35682 / 1.32998\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.63600 / 1.60417\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16441.12438] ***\n",
      "Results validation data of all Folds: \n",
      "[[  4.93204390e+04]\n",
      " [  1.32998000e+00]\n",
      " [  1.60417000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 19102.7234] ***\n",
      "Results training data of all Folds: \n",
      "[[  5.73051774e+04]\n",
      " [  1.35682000e+00]\n",
      " [  1.63600000e+00]]\n",
      "hyperband obj crossval results 16441.12438\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.58379 / 6.78356\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.17556 / 1.32706\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.66769 / 5.89443\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.66835] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 6.78356]\n",
      " [ 1.32706]\n",
      " [ 5.89443]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.47568] ***\n",
      "Results training data of all Folds: \n",
      "[[ 6.58379]\n",
      " [ 1.17556]\n",
      " [ 5.66769]]\n",
      "hyperband obj crossval results 4.66835\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.06374 / 0.08929\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.94452 / 2.82466\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.88062 / 3.28236\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.06544] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.08929]\n",
      " [ 2.82466]\n",
      " [ 3.28236]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96296] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.06374]\n",
      " [ 2.94452]\n",
      " [ 2.88062]]\n",
      "hyperband obj crossval results 2.06544\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 39.15438 / 36.29156\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.85508 / 2.74689\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.94695 / 0.96315\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.33386] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 36.29156]\n",
      " [  2.74689]\n",
      " [  0.96315]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 14.3188] ***\n",
      "Results training data of all Folds: \n",
      "[[ 39.15438]\n",
      " [  2.85508]\n",
      " [  0.94695]]\n",
      "hyperband obj crossval results 13.33386\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 25, 'lr': 0.3835397084896682}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 173541.34126 / 202132.37460\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1651.36476 / 1902.48543\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.59737 / 6.48495\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 68013.78166] ***\n",
      "Results validation data of all Folds: \n",
      "[[  2.02132375e+05]\n",
      " [  1.90248543e+03]\n",
      " [  6.48495000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 58399.7678] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.73541341e+05]\n",
      " [  1.65136476e+03]\n",
      " [  6.59737000e+00]]\n",
      "hyperband obj crossval results 68013.78166\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.17632548175046464}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1985.91012 / 1775.64284\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 34.40116 / 34.60897\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.14322 / 0.18056\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 603.47746] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.77564284e+03]\n",
      " [  3.46089700e+01]\n",
      " [  1.80560000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 673.48483] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98591012e+03]\n",
      " [  3.44011600e+01]\n",
      " [  1.43220000e-01]]\n",
      "hyperband obj crossval results 603.47746\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.45587 / 2.55588\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.94175 / 6.95478\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.55024 / 3.54978\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 4.35348] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.55588]\n",
      " [ 6.95478]\n",
      " [ 3.54978]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 4.31595] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.45587]\n",
      " [ 6.94175]\n",
      " [ 3.55024]]\n",
      "hyperband obj crossval results 4.35348\n",
      "cross validate 5 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 19, 'lr': 0.003469030372105559}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.83829 / 16.07803\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.03017 / 5.83519\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 25.34513 / 25.21779\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 15.71033] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 16.07803]\n",
      " [  5.83519]\n",
      " [ 25.21779]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.73786] ***\n",
      "Results training data of all Folds: \n",
      "[[ 13.83829]\n",
      " [  8.03017]\n",
      " [ 25.34513]]\n",
      "hyperband obj crossval results 15.71033\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 23, 'lr': 0.28430749054335513}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3472.04396 / 3523.06327\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 0.63654 / 0.69261\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.49294 / 0.53654\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1174.76414] ***\n",
      "Results validation data of all Folds: \n",
      "[[  3.52306327e+03]\n",
      " [  6.92610000e-01]\n",
      " [  5.36540000e-01]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1157.72448] ***\n",
      "Results training data of all Folds: \n",
      "[[  3.47204396e+03]\n",
      " [  6.36540000e-01]\n",
      " [  4.92940000e-01]]\n",
      "hyperband obj crossval results 1174.76414\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 3.07994 / 5.19139\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98066 / 1.25787\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.47991 / 2.00694\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 2.81873] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19139]\n",
      " [ 1.25787]\n",
      " [ 2.00694]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.51351] ***\n",
      "Results training data of all Folds: \n",
      "[[ 3.07994]\n",
      " [ 1.98066]\n",
      " [ 2.47991]]\n",
      "hyperband obj crossval results 2.81873\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 22, 'lr': 0.0010424757895254506}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 53.63178 / 49.70385\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 27.54559 / 24.83507\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.28858 / 1.20521\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 25.24804] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 49.70385]\n",
      " [ 24.83507]\n",
      " [  1.20521]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 27.48865] ***\n",
      "Results training data of all Folds: \n",
      "[[ 53.63178]\n",
      " [ 27.54559]\n",
      " [  1.28858]]\n",
      "hyperband obj crossval results 25.24804\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.78026 / 2.92213\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05201 / 0.04299\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.33999 / 1.72137\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.56216] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 2.92213]\n",
      " [ 0.04299]\n",
      " [ 1.72137]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.39075] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.78026]\n",
      " [ 0.05201]\n",
      " [ 2.33999]]\n",
      "hyperband obj crossval results 1.56216\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.001921554114714757}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 37.93051 / 35.21599\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 14.08750 / 11.14776\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 10.54999 / 8.86799\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 18.41058] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 35.21599]\n",
      " [ 11.14776]\n",
      " [  8.86799]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 20.856] ***\n",
      "Results training data of all Folds: \n",
      "[[ 37.93051]\n",
      " [ 14.0875 ]\n",
      " [ 10.54999]]\n",
      "hyperband obj crossval results 18.41058\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 44, 'lr': 0.4913996332060787}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1575585.33856 / 1698166.62906\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1294.57183 / 1544.12482\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.51372 / 2.53258\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 566571.09549] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.69816663e+06]\n",
      " [  1.54412482e+03]\n",
      " [  2.53258000e+00]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 525627.4747] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.57558534e+06]\n",
      " [  1.29457183e+03]\n",
      " [  2.51372000e+00]]\n",
      "hyperband obj crossval results 566571.09549\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.95017 / 1.32579\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04080 / 0.03309\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04376 / 0.02759\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.46215] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.32579]\n",
      " [ 0.03309]\n",
      " [ 0.02759]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34491] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.95017]\n",
      " [ 0.0408 ]\n",
      " [ 0.04376]]\n",
      "hyperband obj crossval results 0.46215\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.007725817805663781}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.62112 / 4.18508\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04926 / 0.04060\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04391 / 0.02767\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.41778] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 4.18508]\n",
      " [ 0.0406 ]\n",
      " [ 0.02767]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.90476] ***\n",
      "Results training data of all Folds: \n",
      "[[ 2.62112]\n",
      " [ 0.04926]\n",
      " [ 0.04391]]\n",
      "hyperband obj crossval results 1.41778\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.30579 / 17.29121\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.52870 / 9.34835\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.08090 / 13.25739\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 13.29898] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 17.29121]\n",
      " [  9.34835]\n",
      " [ 13.25739]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 13.63846] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.30579]\n",
      " [ 11.5287 ]\n",
      " [ 13.0809 ]]\n",
      "hyperband obj crossval results 13.29898\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 18, 'lr': 0.0076099528036695455}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.37079 / 0.41493\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04794 / 0.03834\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.68127 / 4.39918\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.61748] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.41493]\n",
      " [ 0.03834]\n",
      " [ 4.39918]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 2.03333] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.37079]\n",
      " [ 0.04794]\n",
      " [ 5.68127]]\n",
      "hyperband obj crossval results 1.61748\n",
      "cross validate 10 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 853.34644 / 905.47774\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.08386 / 0.08086\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05171 / 0.05630\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 301.87164] ***\n",
      "Results validation data of all Folds: \n",
      "[[  9.05477740e+02]\n",
      " [  8.08600000e-02]\n",
      " [  5.63000000e-02]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 284.494] ***\n",
      "Results training data of all Folds: \n",
      "[[  8.53346440e+02]\n",
      " [  8.38600000e-02]\n",
      " [  5.17100000e-02]]\n",
      "hyperband obj crossval results 301.87164\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.45195 / 0.61088\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04009 / 0.03219\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.13532 / 0.25532\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.29946] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.61088]\n",
      " [ 0.03219]\n",
      " [ 0.25532]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.20912] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.45195]\n",
      " [ 0.04009]\n",
      " [ 0.13532]]\n",
      "hyperband obj crossval results 0.29946\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 5.79849 / 5.19062\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04337 / 0.04290\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.04351 / 0.04174\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.75842] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 5.19062]\n",
      " [ 0.0429 ]\n",
      " [ 0.04174]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.96179] ***\n",
      "Results training data of all Folds: \n",
      "[[ 5.79849]\n",
      " [ 0.04337]\n",
      " [ 0.04351]]\n",
      "hyperband obj crossval results 1.75842\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 40, 'lr': 0.002370731368911436}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 16.55753 / 20.22860\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 8.72184 / 6.69817\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.85380 / 4.63014\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 10.51897] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 20.2286 ]\n",
      " [  6.69817]\n",
      " [  4.63014]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 10.71106] ***\n",
      "Results training data of all Folds: \n",
      "[[ 16.55753]\n",
      " [  8.72184]\n",
      " [  6.8538 ]]\n",
      "hyperband obj crossval results 10.51897\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 17, 'lr': 0.0012756729206255175}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 88.28766 / 88.52939\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 9.23875 / 6.21818\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 15.96105 / 11.43095\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 35.39284] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 88.52939]\n",
      " [  6.21818]\n",
      " [ 11.43095]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 37.82915] ***\n",
      "Results training data of all Folds: \n",
      "[[ 88.28766]\n",
      " [  9.23875]\n",
      " [ 15.96105]]\n",
      "hyperband obj crossval results 35.39284\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 26, 'lr': 0.1579507660887153}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 38.78400 / 41.18006\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 2.46152 / 2.47339\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 4.26055 / 4.35291\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 16.00212] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 41.18006]\n",
      " [  2.47339]\n",
      " [  4.35291]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 15.16869] ***\n",
      "Results training data of all Folds: \n",
      "[[ 38.784  ]\n",
      " [  2.46152]\n",
      " [  4.26055]]\n",
      "hyperband obj crossval results 16.00212\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.001051445164047007}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 50.41666 / 48.16330\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 13.99796 / 9.56694\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 11.71480 / 8.87737\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 22.20254] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 48.1633 ]\n",
      " [  9.56694]\n",
      " [  8.87737]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 25.37647] ***\n",
      "Results training data of all Folds: \n",
      "[[ 50.41666]\n",
      " [ 13.99796]\n",
      " [ 11.7148 ]]\n",
      "hyperband obj crossval results 22.20254\n",
      "cross validate 20 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.84544 / 1.95491\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.26675 / 0.27446\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate on 5 steps, mse on train / validation data: 1.42394 / 1.42303\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 1.21747] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.95491]\n",
      " [ 0.27446]\n",
      " [ 1.42303]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 1.17871] ***\n",
      "Results training data of all Folds: \n",
      "[[ 1.84544]\n",
      " [ 0.26675]\n",
      " [ 1.42394]]\n",
      "hyperband obj crossval results 1.21747\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 36, 'lr': 0.06641032495399549}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 1.98941 / 1.91761\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 6.48118 / 6.42872\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 18.31786 / 18.51387\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 8.9534] ***\n",
      "Results validation data of all Folds: \n",
      "[[  1.91761]\n",
      " [  6.42872]\n",
      " [ 18.51387]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 8.92948] ***\n",
      "Results training data of all Folds: \n",
      "[[  1.98941]\n",
      " [  6.48118]\n",
      " [ 18.31786]]\n",
      "hyperband obj crossval results 8.9534\n",
      "cross validate 40 epochs, train on 5 steps, validate on [5] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.80275 / 1.05999\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.05447 / 0.04670\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "validate on 5 steps, mse on train / validation data: 0.19026 / 0.20350\n",
      "MSE on validation data on [5] steps: means over folds: *** [ 0.43673] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 1.05999]\n",
      " [ 0.0467 ]\n",
      " [ 0.2035 ]]\n",
      "MSE on train data on [5] steps: means over folds: *** [ 0.34916] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.80275]\n",
      " [ 0.05447]\n",
      " [ 0.19026]]\n",
      "hyperband obj crossval results 0.43673\n",
      "traj {'time_finished': [22.488330125808716, 60.9959557056427, 91.6520767211914, 313.71562457084656, 390.30302476882935, 627.3354201316833], 'losses': [16441.12438, 4.66835, 2.06544, 1.56216, 0.46215, 0.29946], 'budgets': [5.0, 5.0, 5.0, 10.0, 20.0, 20.0], 'config_ids': [(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 1), (0, 0, 2), (2, 0, 0)]}\n",
      "best_cfg_id (2, 0, 0)\n",
      "all_configs {(1, 0, 3): {'config_info': {}, 'config': {'batch_size': 26, 'lr': 0.1579507660887153}}, (0, 0, 7): {'config_info': {}, 'config': {'batch_size': 19, 'lr': 0.003469030372105559}}, (2, 0, 3): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.06641032495399549}}, (0, 0, 2): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.0076099528036695455}}, (1, 0, 0): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.28430749054335513}}, (0, 0, 6): {'config_info': {}, 'config': {'batch_size': 22, 'lr': 0.0010424757895254506}}, (2, 0, 2): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.001051445164047007}}, (0, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.007725817805663781}}, (1, 0, 1): {'config_info': {}, 'config': {'batch_size': 44, 'lr': 0.4913996332060787}}, (0, 0, 5): {'config_info': {}, 'config': {'batch_size': 23, 'lr': 0.17632548175046464}}, (0, 0, 0): {'config_info': {}, 'config': {'batch_size': 36, 'lr': 0.1874018718283377}}, (2, 0, 1): {'config_info': {}, 'config': {'batch_size': 17, 'lr': 0.0012756729206255175}}, (0, 0, 4): {'config_info': {}, 'config': {'batch_size': 25, 'lr': 0.3835397084896682}}, (1, 0, 2): {'config_info': {}, 'config': {'batch_size': 40, 'lr': 0.002370731368911436}}, (2, 0, 0): {'config_info': {}, 'config': {'batch_size': 21, 'lr': 0.023123758972112808}}, (0, 0, 3): {'config_info': {}, 'config': {'batch_size': 18, 'lr': 0.001921554114714757}}}\n",
      "return best config:  {'batch_size': 21, 'lr': 0.023123758972112808}\n"
     ]
    }
   ],
   "source": [
    "best_cfg = hp.optimize([configs,lcs], Y, model_type='multi_lstm', \n",
    "                       min_budget = 4, max_budget=40, run_name='', earlystop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validate 1000 epochs, train on 5 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15458, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15458 to 0.09678, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09678 to 0.02604, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss is 0.03820, did not improve\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02604 to 0.01416, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01416 to 0.01231, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01231 to 0.00266, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.00400, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 0.00438, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00266 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00301, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00153 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss is 0.00336, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00170, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00143 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00287, did not improve\n",
      "\n",
      "Epoch 00017: val_loss is 0.00330, did not improve\n",
      "\n",
      "Epoch 00018: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00189, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00281, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00298, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00192, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00232, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00246, did not improve\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss is 0.00183, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00204, did not improve\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00132 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss is 0.00166, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00126 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00122 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss is 0.00148, did not improve\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00116 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00108 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00105 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00102 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00099 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00095 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00087 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00079 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00073 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00069 to 0.00066, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00066 to 0.00065, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00065 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00062 to 0.00062, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00062 to 0.00061, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00061 to 0.00059, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00059 to 0.00058, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00058 to 0.00056, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00056 to 0.00055, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00055 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00054 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00053 to 0.00053, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00053 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00052 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00048 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00049, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00051, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00059, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00134: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00135: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00064, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00137: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00066, did not improve\n",
      "Epoch 00144: early stopping\n",
      "Using epoch 00086 with val_loss: 0.00045\n",
      "validate on 30 steps, mse on train / validation data: 0.17828 / 0.28994\n",
      "train fold 2 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.90476, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 1361.45895, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 510.44960, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 255.13004, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 54.13024, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 21.52839, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.90476 to 4.37317, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.37317 to 1.53281, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.53281 to 0.45816, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45816 to 0.10284, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10284 to 0.10064, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10064 to 0.03229, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03229 to 0.01450, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01450 to 0.00979, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00979 to 0.00822, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00822 to 0.00662, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00662 to 0.00488, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00488 to 0.00424, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00424 to 0.00388, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00388 to 0.00365, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00365 to 0.00335, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00335 to 0.00313, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00313 to 0.00293, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00293 to 0.00274, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00274 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00257 to 0.00243, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00243 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00231 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00220 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00210 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00201 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00193 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00186 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00179 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00172 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00166 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00161 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00157 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00154 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00151 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00148 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00145 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00143 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00140 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00138 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00136 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00134 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00132 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00129 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00126 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00121 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00116 to 0.00115, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00115 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00105 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00099 to 0.00098, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00098 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00095 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00091 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00088 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00086 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00080 to 0.00079, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00136: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00137: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00138: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00139: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00140: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00141: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00142: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00143: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00144: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00145: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00146: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00147: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00148: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00149: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00150: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00151: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00152: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00153: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00154: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00155: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00156: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00157: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00158: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00159: val_loss is 0.00108, did not improve\n",
      "\n",
      "Epoch 00160: val_loss is 0.00110, did not improve\n",
      "\n",
      "Epoch 00161: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00162: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00163: val_loss is 0.00117, did not improve\n",
      "\n",
      "Epoch 00164: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00165: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00166: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00167: val_loss is 0.00125, did not improve\n",
      "\n",
      "Epoch 00168: val_loss is 0.00127, did not improve\n",
      "\n",
      "Epoch 00169: val_loss is 0.00129, did not improve\n",
      "\n",
      "Epoch 00170: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00171: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00172: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00173: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00174: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00175: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00176: val_loss is 0.00143, did not improve\n",
      "\n",
      "Epoch 00177: val_loss is 0.00145, did not improve\n",
      "\n",
      "Epoch 00178: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00179: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00180: val_loss is 0.00147, did not improve\n",
      "\n",
      "Epoch 00181: val_loss is 0.00146, did not improve\n",
      "\n",
      "Epoch 00182: val_loss is 0.00144, did not improve\n",
      "\n",
      "Epoch 00183: val_loss is 0.00141, did not improve\n",
      "\n",
      "Epoch 00184: val_loss is 0.00136, did not improve\n",
      "Epoch 00184: early stopping\n",
      "Using epoch 00134 with val_loss: 0.00069\n",
      "validate on 30 steps, mse on train / validation data: 1.26030 / 1.59993\n",
      "train fold 3 on 5 steps, validation on 5 steps\n",
      "train considering 5 epochs, evaluate with 5 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03170, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.09429, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.03942, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03170 to 0.02370, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.02402, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02370 to 0.02337, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02337 to 0.01970, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01970 to 0.01745, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01745 to 0.01647, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01647 to 0.01517, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01517 to 0.01348, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01348 to 0.01201, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01201 to 0.01066, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01066 to 0.00930, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00930 to 0.00798, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00798 to 0.00680, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00680 to 0.00574, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00574 to 0.00484, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00484 to 0.00409, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00409 to 0.00350, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00350 to 0.00307, storing weights.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00307 to 0.00276, storing weights.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00276 to 0.00257, storing weights.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00257 to 0.00245, storing weights.\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00245 to 0.00239, storing weights.\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00239 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00235 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00234 to 0.00234, storing weights.\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00234 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00233 to 0.00233, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00233 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00232 to 0.00232, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00232 to 0.00231, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00231 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00230 to 0.00230, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00230 to 0.00229, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00229 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00228 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00228 to 0.00227, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00227 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00226 to 0.00226, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00226 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00225 to 0.00225, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00225 to 0.00224, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_loss improved from 0.00224 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00223 to 0.00223, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00223 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00222 to 0.00222, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00222 to 0.00221, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00221 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00220 to 0.00220, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00220 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00219 to 0.00219, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00219 to 0.00218, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00218 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00217 to 0.00217, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00217 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00216 to 0.00216, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00216 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00215 to 0.00215, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00215 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00214 to 0.00214, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00214 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00213 to 0.00213, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00213 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00212 to 0.00212, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00212 to 0.00211, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00211 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00210 to 0.00210, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00210 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00209 to 0.00209, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00209 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00208 to 0.00208, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00208 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00207 to 0.00207, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00207 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00206 to 0.00206, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00206 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00205 to 0.00205, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00205 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00204 to 0.00204, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00204 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00203 to 0.00203, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00203 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00202 to 0.00202, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00202 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00201 to 0.00201, storing weights.\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00201 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00200 to 0.00200, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00200 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00199 to 0.00199, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.00199 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00198 to 0.00198, storing weights.\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.00198 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.00197 to 0.00197, storing weights.\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00197 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00196 to 0.00196, storing weights.\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00196 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00195 to 0.00195, storing weights.\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00195 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00194 to 0.00194, storing weights.\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00194 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00193 to 0.00193, storing weights.\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00193 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00192 to 0.00192, storing weights.\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00192 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00191 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00191 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.00190 to 0.00190, storing weights.\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00190 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.00189 to 0.00189, storing weights.\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00189 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.00188 to 0.00188, storing weights.\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00188 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00187 to 0.00187, storing weights.\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00187 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.00186 to 0.00186, storing weights.\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00186 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00185 to 0.00185, storing weights.\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.00185 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00184 to 0.00184, storing weights.\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00184 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.00183 to 0.00183, storing weights.\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00183 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00182 to 0.00182, storing weights.\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.00182 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00181 to 0.00181, storing weights.\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.00181 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00180 to 0.00180, storing weights.\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00180 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.00179 to 0.00179, storing weights.\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.00179 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.00178 to 0.00178, storing weights.\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00178 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00177 to 0.00177, storing weights.\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.00177 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00176 to 0.00176, storing weights.\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.00176 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00175 to 0.00175, storing weights.\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.00175 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.00174 to 0.00174, storing weights.\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.00174 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00173 to 0.00173, storing weights.\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00173 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00172 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.00172 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.00171 to 0.00171, storing weights.\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.00171 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00170 to 0.00170, storing weights.\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.00170 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.00169 to 0.00169, storing weights.\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00169 to 0.00168, storing weights.\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.00168 to 0.00168, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00159: val_loss improved from 0.00168 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00167 to 0.00167, storing weights.\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00167 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.00166 to 0.00166, storing weights.\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00166 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00165 to 0.00165, storing weights.\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.00165 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00164 to 0.00164, storing weights.\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00164 to 0.00163, storing weights.\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.00163 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00162 to 0.00162, storing weights.\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00162 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.00161 to 0.00161, storing weights.\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.00161 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.00160 to 0.00160, storing weights.\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.00160 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.00159 to 0.00159, storing weights.\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00159 to 0.00158, storing weights.\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.00158 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.00157 to 0.00157, storing weights.\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.00157 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.00156 to 0.00156, storing weights.\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.00156 to 0.00155, storing weights.\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.00155 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.00154 to 0.00154, storing weights.\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.00154 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00153 to 0.00153, storing weights.\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.00153 to 0.00152, storing weights.\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.00152 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.00151 to 0.00151, storing weights.\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00151 to 0.00150, storing weights.\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.00150 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00149 to 0.00149, storing weights.\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.00149 to 0.00148, storing weights.\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.00148 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.00147 to 0.00147, storing weights.\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.00147 to 0.00146, storing weights.\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.00146 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.00145 to 0.00145, storing weights.\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.00145 to 0.00144, storing weights.\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00144 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.00143 to 0.00143, storing weights.\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.00143 to 0.00142, storing weights.\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.00142 to 0.00141, storing weights.\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.00141 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.00140 to 0.00140, storing weights.\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.00140 to 0.00139, storing weights.\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.00139 to 0.00138, storing weights.\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.00138 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.00137 to 0.00137, storing weights.\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.00137 to 0.00136, storing weights.\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.00136 to 0.00135, storing weights.\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.00135 to 0.00134, storing weights.\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.00134 to 0.00133, storing weights.\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00133 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.00132 to 0.00132, storing weights.\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.00132 to 0.00131, storing weights.\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.00131 to 0.00130, storing weights.\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.00130 to 0.00129, storing weights.\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.00129 to 0.00128, storing weights.\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.00128 to 0.00127, storing weights.\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.00127 to 0.00126, storing weights.\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.00126 to 0.00125, storing weights.\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.00125 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.00124 to 0.00124, storing weights.\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00124 to 0.00123, storing weights.\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.00123 to 0.00122, storing weights.\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.00122 to 0.00121, storing weights.\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.00121 to 0.00120, storing weights.\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.00120 to 0.00119, storing weights.\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.00119 to 0.00118, storing weights.\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.00118 to 0.00117, storing weights.\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.00117 to 0.00116, storing weights.\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00116 to 0.00114, storing weights.\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.00114 to 0.00113, storing weights.\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.00113 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.00112 to 0.00111, storing weights.\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00111 to 0.00110, storing weights.\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00110 to 0.00109, storing weights.\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.00109 to 0.00108, storing weights.\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.00108 to 0.00107, storing weights.\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.00107 to 0.00106, storing weights.\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.00106 to 0.00105, storing weights.\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.00105 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.00104 to 0.00104, storing weights.\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.00104 to 0.00103, storing weights.\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00103 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.00102 to 0.00102, storing weights.\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.00102 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.00101 to 0.00101, storing weights.\n",
      "\n",
      "Epoch 00250: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00251: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00252: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00253: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00254: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00255: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00256: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00257: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00258: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00259: val_loss is 0.00114, did not improve\n",
      "\n",
      "Epoch 00260: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00261: val_loss is 0.00119, did not improve\n",
      "\n",
      "Epoch 00262: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00263: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00264: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00265: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00266: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00267: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00268: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00269: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00270: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00271: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00272: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00273: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00274: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00275: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00276: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00277: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00278: val_loss is 0.00139, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00280: val_loss is 0.00139, did not improve\n",
      "\n",
      "Epoch 00281: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00282: val_loss is 0.00138, did not improve\n",
      "\n",
      "Epoch 00283: val_loss is 0.00137, did not improve\n",
      "\n",
      "Epoch 00284: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00285: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00286: val_loss is 0.00134, did not improve\n",
      "\n",
      "Epoch 00287: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00288: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00289: val_loss is 0.00130, did not improve\n",
      "\n",
      "Epoch 00290: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00291: val_loss is 0.00126, did not improve\n",
      "\n",
      "Epoch 00292: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00293: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00294: val_loss is 0.00120, did not improve\n",
      "\n",
      "Epoch 00295: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00296: val_loss is 0.00116, did not improve\n",
      "\n",
      "Epoch 00297: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00298: val_loss is 0.00113, did not improve\n",
      "\n",
      "Epoch 00299: val_loss is 0.00111, did not improve\n",
      "\n",
      "Epoch 00300: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00301: val_loss is 0.00107, did not improve\n",
      "\n",
      "Epoch 00302: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00303: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00304: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00305: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.00101 to 0.00100, storing weights.\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.00100 to 0.00099, storing weights.\n",
      "\n",
      "Epoch 00308: val_loss improved from 0.00099 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.00097 to 0.00096, storing weights.\n",
      "\n",
      "Epoch 00310: val_loss improved from 0.00096 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.00095 to 0.00094, storing weights.\n",
      "\n",
      "Epoch 00312: val_loss improved from 0.00094 to 0.00093, storing weights.\n",
      "\n",
      "Epoch 00313: val_loss improved from 0.00093 to 0.00092, storing weights.\n",
      "\n",
      "Epoch 00314: val_loss improved from 0.00092 to 0.00091, storing weights.\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.00091 to 0.00090, storing weights.\n",
      "\n",
      "Epoch 00316: val_loss improved from 0.00090 to 0.00089, storing weights.\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.00089 to 0.00088, storing weights.\n",
      "\n",
      "Epoch 00318: val_loss improved from 0.00088 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.00087 to 0.00087, storing weights.\n",
      "\n",
      "Epoch 00320: val_loss improved from 0.00087 to 0.00086, storing weights.\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.00086 to 0.00085, storing weights.\n",
      "\n",
      "Epoch 00322: val_loss improved from 0.00085 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.00084 to 0.00084, storing weights.\n",
      "\n",
      "Epoch 00324: val_loss improved from 0.00084 to 0.00083, storing weights.\n",
      "\n",
      "Epoch 00325: val_loss improved from 0.00083 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00326: val_loss improved from 0.00082 to 0.00082, storing weights.\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.00082 to 0.00081, storing weights.\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.00081 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.00080 to 0.00080, storing weights.\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.00080 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00331: val_loss improved from 0.00079 to 0.00079, storing weights.\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.00079 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.00078 to 0.00078, storing weights.\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.00078 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.00077 to 0.00077, storing weights.\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.00077 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00338: val_loss improved from 0.00076 to 0.00076, storing weights.\n",
      "\n",
      "Epoch 00339: val_loss improved from 0.00076 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.00075 to 0.00075, storing weights.\n",
      "\n",
      "Epoch 00341: val_loss improved from 0.00075 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00342: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.00074 to 0.00074, storing weights.\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.00074 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00345: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.00073 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.00073 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00350: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00351: val_loss improved from 0.00072 to 0.00072, storing weights.\n",
      "\n",
      "Epoch 00352: val_loss improved from 0.00072 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00354: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.00071 to 0.00071, storing weights.\n",
      "\n",
      "Epoch 00357: val_loss improved from 0.00071 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00358: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00359: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00361: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00363: val_loss improved from 0.00070 to 0.00070, storing weights.\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.00070 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00365: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00366: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00367: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00368: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00370: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00371: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00376: val_loss improved from 0.00069 to 0.00069, storing weights.\n",
      "\n",
      "Epoch 00377: val_loss improved from 0.00069 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00378: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00379: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00380: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00381: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00382: val_loss improved from 0.00068 to 0.00068, storing weights.\n",
      "\n",
      "Epoch 00383: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00384: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00385: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00386: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00387: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00388: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00389: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00390: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00391: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00392: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00393: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00394: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00395: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00396: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00397: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00398: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00399: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00400: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00401: val_loss is 0.00069, did not improve\n",
      "\n",
      "Epoch 00402: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00403: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00404: val_loss is 0.00070, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00405: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00406: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00407: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00408: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00409: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00410: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00411: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00412: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00413: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00414: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00415: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00416: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00417: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00418: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00419: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00420: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00421: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00422: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00423: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00424: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00425: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00426: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00427: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00428: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00429: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00430: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00431: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00432: val_loss is 0.00077, did not improve\n",
      "Epoch 00432: early stopping\n",
      "Using epoch 00382 with val_loss: 0.00068\n",
      "validate on 30 steps, mse on train / validation data: 0.13800 / 0.08938\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.73713  0.79544  0.92657  0.65975] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.70987  0.61491  0.50921  0.28994]\n",
      " [ 1.45444  1.73461  2.25349  1.59993]\n",
      " [ 0.04709  0.03681  0.01701  0.08938]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.54098  0.66638  0.72073  0.52552] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.52994  0.51729  0.3413   0.17828]\n",
      " [ 1.02344  1.42766  1.79191  1.2603 ]\n",
      " [ 0.06957  0.0542   0.02898  0.138  ]]\n",
      "cross validate 1000 epochs, train on 10 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11671, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12661, did not improve\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11671 to 0.05024, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05024 to 0.02909, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 0.03532, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02909 to 0.00954, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00954 to 0.00902, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00902 to 0.00634, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss is 0.01010, did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00634 to 0.00631, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00654, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00672, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00631 to 0.00496, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00522, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00496 to 0.00286, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00286 to 0.00172, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00172 to 0.00073, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00073 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00054 to 0.00052, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00060, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00100, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00121, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00101, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00081, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00115, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00103, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00064, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00084, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00052 to 0.00047, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00092, did not improve\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00047 to 0.00044, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss is 0.00097, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00089, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00136, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00240, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00386, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00477, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.01023, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.01203, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00506, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00172, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00155, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00061, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00302, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00140, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00194, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00053, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00068, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00046, did not improve\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00044 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00040 to 0.00040, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00040 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.00039 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00093: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00038 to 0.00038, storing weights.\n",
      "\n",
      "Epoch 00095: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.00040, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.00039, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.00040, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.00043, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.00042, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.00044, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.00047, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.00052, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.00058, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.00065, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.00095, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.00162, did not improve\n",
      "\n",
      "Epoch 00130: val_loss is 0.00171, did not improve\n",
      "\n",
      "Epoch 00131: val_loss is 0.00416, did not improve\n",
      "\n",
      "Epoch 00132: val_loss is 0.00269, did not improve\n",
      "\n",
      "Epoch 00133: val_loss is 0.00640, did not improve\n",
      "Epoch 00133: early stopping\n",
      "Using epoch 00094 with val_loss: 0.00038\n",
      "validate on 30 steps, mse on train / validation data: 0.01650 / 0.01693\n",
      "train fold 2 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 378.56074, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 378.56074 to 30.09993, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss is 34.44132, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 30.09993 to 3.21959, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss is 4.52868, did not improve\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.21959 to 0.70476, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss is 1.93981, did not improve\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70476 to 0.47590, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47590 to 0.15103, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.15103 to 0.06758, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06758 to 0.05877, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05877 to 0.03435, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03435 to 0.03089, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.03089 to 0.03066, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03066 to 0.02833, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.02988, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02833 to 0.02818, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.02889, did not improve\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02818 to 0.02816, storing weights.\n",
      "\n",
      "Epoch 00020: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02824, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02825, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.02827, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02828, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02829, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02830, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02830, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00019 with val_loss: 0.02816\n",
      "validate on 30 steps, mse on train / validation data: 0.03174 / 0.02943\n",
      "train fold 3 on 10 steps, validation on 10 steps\n",
      "train considering 10 epochs, evaluate with 10 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04151, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04151 to 0.01769, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01769 to 0.00459, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00459 to 0.00235, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00235 to 0.00191, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00191 to 0.00095, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00095 to 0.00046, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00046 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00039 to 0.00039, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00039 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00012: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00013: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00014: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00033 to 0.00033, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00033 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.00029, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00029, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00029, did not improve\n",
      "Epoch 00085: early stopping\n",
      "Using epoch 00056 with val_loss: 0.00029\n",
      "validate on 30 steps, mse on train / validation data: 0.01208 / 0.00941\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.08369  0.04828  0.03323  0.01859] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.1925   0.08568  0.04499  0.01693]\n",
      " [ 0.02943  0.02943  0.02943  0.02943]\n",
      " [ 0.02916  0.02972  0.02526  0.00941]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.07582  0.05066  0.03704  0.02011] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.14894  0.07332  0.04103  0.0165 ]\n",
      " [ 0.03174  0.03174  0.03174  0.03174]\n",
      " [ 0.04678  0.04693  0.03834  0.01208]]\n",
      "cross validate 1000 epochs, train on 20 steps, validate on [5, 10, 20, 30] steps\n",
      "config {'batch_size': 21, 'lr': 0.023123758972112808}\n",
      "choose min as mode\n",
      "evaluating with early stopping\n",
      "train fold 1 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32145, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32145 to 0.12631, storing weights.\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12631 to 0.10577, storing weights.\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10577 to 0.06052, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06052 to 0.01204, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss is 0.01487, did not improve\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01204 to 0.00730, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss is 0.01194, did not improve\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00730 to 0.00228, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss is 0.00380, did not improve\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00228 to 0.00097, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00164, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00097 to 0.00063, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00063 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00016: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00054 to 0.00048, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00062, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00057, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00048, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00055, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00054, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00071, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00056, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00088, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00087, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00105, did not improve\n",
      "\n",
      "Epoch 00035: val_loss is 0.00118, did not improve\n",
      "\n",
      "Epoch 00036: val_loss is 0.00123, did not improve\n",
      "\n",
      "Epoch 00037: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00038: val_loss is 0.00133, did not improve\n",
      "\n",
      "Epoch 00039: val_loss is 0.00131, did not improve\n",
      "\n",
      "Epoch 00040: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00041: val_loss is 0.00124, did not improve\n",
      "\n",
      "Epoch 00042: val_loss is 0.00122, did not improve\n",
      "\n",
      "Epoch 00043: val_loss is 0.00109, did not improve\n",
      "\n",
      "Epoch 00044: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00045: val_loss is 0.00094, did not improve\n",
      "\n",
      "Epoch 00046: val_loss is 0.00091, did not improve\n",
      "\n",
      "Epoch 00047: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00048: val_loss is 0.00082, did not improve\n",
      "\n",
      "Epoch 00049: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00050: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00051: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00052: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00053: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00054: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00055: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00056: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00057: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00058: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00059: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00060: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00061: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00062: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00063: val_loss is 0.00075, did not improve\n",
      "\n",
      "Epoch 00064: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00065: val_loss is 0.00077, did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: val_loss is 0.00073, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.00080, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.00074, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.00083, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.00086, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.00077, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.00090, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.00093, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.00078, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.00099, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.00076, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.00102, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.00072, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.00104, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.00067, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.00106, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.00063, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.00112, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.00066, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.00128, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.00096, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00173, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00222, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00017 with val_loss: 0.00048\n",
      "validate on 30 steps, mse on train / validation data: 0.00203 / 0.00196\n",
      "train fold 2 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05475, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss is 0.12115, did not improve\n",
      "\n",
      "Epoch 00003: val_loss is 0.08707, did not improve\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.05475 to 0.05174, storing weights.\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05174 to 0.04611, storing weights.\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04611 to 0.02661, storing weights.\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02661 to 0.01495, storing weights.\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01495 to 0.00808, storing weights.\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00808 to 0.00483, storing weights.\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00483 to 0.00260, storing weights.\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00260 to 0.00112, storing weights.\n",
      "\n",
      "Epoch 00012: val_loss is 0.00132, did not improve\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00112 to 0.00054, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss is 0.00135, did not improve\n",
      "\n",
      "Epoch 00015: val_loss is 0.00070, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00054 to 0.00045, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00045 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00018: val_loss is 0.00041, did not improve\n",
      "\n",
      "Epoch 00019: val_loss is 0.00050, did not improve\n",
      "\n",
      "Epoch 00020: val_loss is 0.00045, did not improve\n",
      "\n",
      "Epoch 00021: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.00037, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.00038, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.00036, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.00035, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.00034, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00032: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00033: val_loss is 0.00033, did not improve\n",
      "\n",
      "Epoch 00034: val_loss is 0.00032, did not improve\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00032 to 0.00032, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00032 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00031 to 0.00031, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00031 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00030 to 0.00030, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00030 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00029 to 0.00029, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00029 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00028 to 0.00028, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00028 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00027 to 0.00027, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00027 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00026 to 0.00026, storing weights.\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00026 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.00025 to 0.00025, storing weights.\n",
      "\n",
      "Epoch 00090: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.00025, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.00025, did not improve\n",
      "Epoch 00092: early stopping\n",
      "Using epoch 00089 with val_loss: 0.00025\n",
      "validate on 30 steps, mse on train / validation data: 0.00131 / 0.00129\n",
      "train fold 3 on 20 steps, validation on 20 steps\n",
      "train considering 20 epochs, evaluate with 20 epochs\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51855, storing weights.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51855 to 0.09694, storing weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_loss is 14.75946, did not improve\n",
      "\n",
      "Epoch 00004: val_loss is 275.30684, did not improve\n",
      "\n",
      "Epoch 00005: val_loss is 13.29729, did not improve\n",
      "\n",
      "Epoch 00006: val_loss is 8.29154, did not improve\n",
      "\n",
      "Epoch 00007: val_loss is 0.53958, did not improve\n",
      "\n",
      "Epoch 00008: val_loss is 10.31797, did not improve\n",
      "\n",
      "Epoch 00009: val_loss is 1.31900, did not improve\n",
      "\n",
      "Epoch 00010: val_loss is 0.25212, did not improve\n",
      "\n",
      "Epoch 00011: val_loss is 0.24620, did not improve\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09694 to 0.05898, storing weights.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.05898 to 0.04390, storing weights.\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.04390 to 0.03132, storing weights.\n",
      "\n",
      "Epoch 00015: val_loss is 0.03260, did not improve\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.03132 to 0.02776, storing weights.\n",
      "\n",
      "Epoch 00017: val_loss is 0.02879, did not improve\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02776 to 0.02754, storing weights.\n",
      "\n",
      "Epoch 00019: val_loss is 0.02769, did not improve\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02754 to 0.02724, storing weights.\n",
      "\n",
      "Epoch 00021: val_loss is 0.02757, did not improve\n",
      "\n",
      "Epoch 00022: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00023: val_loss is 0.02750, did not improve\n",
      "\n",
      "Epoch 00024: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00025: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00026: val_loss is 0.02734, did not improve\n",
      "\n",
      "Epoch 00027: val_loss is 0.02735, did not improve\n",
      "\n",
      "Epoch 00028: val_loss is 0.02732, did not improve\n",
      "\n",
      "Epoch 00029: val_loss is 0.02730, did not improve\n",
      "\n",
      "Epoch 00030: val_loss is 0.02727, did not improve\n",
      "\n",
      "Epoch 00031: val_loss is 0.02725, did not improve\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02724 to 0.02722, storing weights.\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02722 to 0.02720, storing weights.\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02720 to 0.02718, storing weights.\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02718 to 0.02716, storing weights.\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.02716 to 0.02714, storing weights.\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.02714 to 0.02711, storing weights.\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02711 to 0.02709, storing weights.\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.02709 to 0.02707, storing weights.\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.02707 to 0.02705, storing weights.\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.02705 to 0.02703, storing weights.\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.02703 to 0.02701, storing weights.\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.02701 to 0.02699, storing weights.\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.02699 to 0.02697, storing weights.\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02697 to 0.02696, storing weights.\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.02696 to 0.02694, storing weights.\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.02694 to 0.02692, storing weights.\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.02692 to 0.02690, storing weights.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.02690 to 0.02689, storing weights.\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.02689 to 0.02687, storing weights.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.02687 to 0.02686, storing weights.\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.02686 to 0.02684, storing weights.\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.02684 to 0.02683, storing weights.\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02683 to 0.02681, storing weights.\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.02681 to 0.02680, storing weights.\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.02680 to 0.02679, storing weights.\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02679 to 0.02678, storing weights.\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02678 to 0.02677, storing weights.\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02677 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02676 to 0.02676, storing weights.\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02676 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02675 to 0.02675, storing weights.\n",
      "\n",
      "Epoch 00065: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00066: val_loss is 0.02675, did not improve\n",
      "\n",
      "Epoch 00067: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00068: val_loss is 0.02676, did not improve\n",
      "\n",
      "Epoch 00069: val_loss is 0.02677, did not improve\n",
      "\n",
      "Epoch 00070: val_loss is 0.02678, did not improve\n",
      "\n",
      "Epoch 00071: val_loss is 0.02680, did not improve\n",
      "\n",
      "Epoch 00072: val_loss is 0.02681, did not improve\n",
      "\n",
      "Epoch 00073: val_loss is 0.02683, did not improve\n",
      "\n",
      "Epoch 00074: val_loss is 0.02686, did not improve\n",
      "\n",
      "Epoch 00075: val_loss is 0.02688, did not improve\n",
      "\n",
      "Epoch 00076: val_loss is 0.02691, did not improve\n",
      "\n",
      "Epoch 00077: val_loss is 0.02695, did not improve\n",
      "\n",
      "Epoch 00078: val_loss is 0.02698, did not improve\n",
      "\n",
      "Epoch 00079: val_loss is 0.02703, did not improve\n",
      "\n",
      "Epoch 00080: val_loss is 0.02707, did not improve\n",
      "\n",
      "Epoch 00081: val_loss is 0.02712, did not improve\n",
      "\n",
      "Epoch 00082: val_loss is 0.02718, did not improve\n",
      "\n",
      "Epoch 00083: val_loss is 0.02724, did not improve\n",
      "\n",
      "Epoch 00084: val_loss is 0.02731, did not improve\n",
      "\n",
      "Epoch 00085: val_loss is 0.02739, did not improve\n",
      "\n",
      "Epoch 00086: val_loss is 0.02747, did not improve\n",
      "\n",
      "Epoch 00087: val_loss is 0.02756, did not improve\n",
      "\n",
      "Epoch 00088: val_loss is 0.02765, did not improve\n",
      "\n",
      "Epoch 00089: val_loss is 0.02775, did not improve\n",
      "\n",
      "Epoch 00090: val_loss is 0.02787, did not improve\n",
      "\n",
      "Epoch 00091: val_loss is 0.02799, did not improve\n",
      "\n",
      "Epoch 00092: val_loss is 0.02812, did not improve\n",
      "\n",
      "Epoch 00093: val_loss is 0.02826, did not improve\n",
      "\n",
      "Epoch 00094: val_loss is 0.02841, did not improve\n",
      "\n",
      "Epoch 00095: val_loss is 0.02857, did not improve\n",
      "\n",
      "Epoch 00096: val_loss is 0.02874, did not improve\n",
      "\n",
      "Epoch 00097: val_loss is 0.02892, did not improve\n",
      "\n",
      "Epoch 00098: val_loss is 0.02912, did not improve\n",
      "\n",
      "Epoch 00099: val_loss is 0.02933, did not improve\n",
      "\n",
      "Epoch 00100: val_loss is 0.02955, did not improve\n",
      "\n",
      "Epoch 00101: val_loss is 0.02979, did not improve\n",
      "\n",
      "Epoch 00102: val_loss is 0.03003, did not improve\n",
      "\n",
      "Epoch 00103: val_loss is 0.03030, did not improve\n",
      "\n",
      "Epoch 00104: val_loss is 0.03058, did not improve\n",
      "\n",
      "Epoch 00105: val_loss is 0.03087, did not improve\n",
      "\n",
      "Epoch 00106: val_loss is 0.03118, did not improve\n",
      "\n",
      "Epoch 00107: val_loss is 0.03151, did not improve\n",
      "\n",
      "Epoch 00108: val_loss is 0.03185, did not improve\n",
      "\n",
      "Epoch 00109: val_loss is 0.03220, did not improve\n",
      "\n",
      "Epoch 00110: val_loss is 0.03257, did not improve\n",
      "\n",
      "Epoch 00111: val_loss is 0.03296, did not improve\n",
      "\n",
      "Epoch 00112: val_loss is 0.03336, did not improve\n",
      "\n",
      "Epoch 00113: val_loss is 0.03377, did not improve\n",
      "\n",
      "Epoch 00114: val_loss is 0.03420, did not improve\n",
      "\n",
      "Epoch 00115: val_loss is 0.03463, did not improve\n",
      "\n",
      "Epoch 00116: val_loss is 0.03508, did not improve\n",
      "\n",
      "Epoch 00117: val_loss is 0.03554, did not improve\n",
      "\n",
      "Epoch 00118: val_loss is 0.03601, did not improve\n",
      "\n",
      "Epoch 00119: val_loss is 0.03648, did not improve\n",
      "\n",
      "Epoch 00120: val_loss is 0.03696, did not improve\n",
      "\n",
      "Epoch 00121: val_loss is 0.03744, did not improve\n",
      "\n",
      "Epoch 00122: val_loss is 0.03792, did not improve\n",
      "\n",
      "Epoch 00123: val_loss is 0.03840, did not improve\n",
      "\n",
      "Epoch 00124: val_loss is 0.03888, did not improve\n",
      "\n",
      "Epoch 00125: val_loss is 0.03936, did not improve\n",
      "\n",
      "Epoch 00126: val_loss is 0.03983, did not improve\n",
      "\n",
      "Epoch 00127: val_loss is 0.04029, did not improve\n",
      "\n",
      "Epoch 00128: val_loss is 0.04074, did not improve\n",
      "\n",
      "Epoch 00129: val_loss is 0.04118, did not improve\n",
      "Epoch 00129: early stopping\n",
      "Using epoch 00064 with val_loss: 0.02675\n",
      "validate on 30 steps, mse on train / validation data: 0.02955 / 0.02495\n",
      "MSE on validation data on [5, 10, 20, 30] steps: means over folds: *** [ 0.02429  0.01109  0.0104   0.0094 ] ***\n",
      "Results validation data of all Folds: \n",
      "[[ 0.04105  0.00511  0.00295  0.00196]\n",
      " [ 0.00689  0.00322  0.00331  0.00129]\n",
      " [ 0.02495  0.02495  0.02495  0.02495]]\n",
      "MSE on train data on [5, 10, 20, 30] steps: means over folds: *** [ 0.0211   0.01247  0.01185  0.01096] ***\n",
      "Results training data of all Folds: \n",
      "[[ 0.02463  0.00444  0.00319  0.00203]\n",
      " [ 0.00913  0.00341  0.00281  0.00131]\n",
      " [ 0.02955  0.02955  0.02955  0.02955]]\n",
      "results validation data \n",
      " [[ 0.73713  0.79544  0.92657  0.65975]\n",
      " [ 0.08369  0.04828  0.03323  0.01859]\n",
      " [ 0.02429  0.01109  0.0104   0.0094 ]]\n",
      "results training data\n",
      " [[ 0.54098  0.66638  0.72073  0.52552]\n",
      " [ 0.07582  0.05066  0.03704  0.02011]\n",
      " [ 0.0211   0.01247  0.01185  0.01096]]\n"
     ]
    }
   ],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = np.zeros((3,4)), np.zeros((3,4))\n",
    "for i, train_steps in enumerate([5,10,20]):\n",
    "    res_train[i], res_val[i] = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                         steps=(train_steps,[5,10,20,30]), \n",
    "                                         cfg=best_cfg, epochs=1000, earlystop=True, \n",
    "                                         mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)\n",
    "# results were far better with lr = 0.002 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = {'batch_size': 21, 'lr': 0.023123758972112808}\n",
    "res_train, res_val = m.eval_cv('multi_lstm', [configs, lcs], Y, \n",
    "                                     steps=(0,[5,10,20,30]), \n",
    "                                     cfg=cfg, epochs=1000, earlystop=True, \n",
    "                                     mode='nextstep')\n",
    "print(\"results validation data \\n\", res_val)  \n",
    "print(\"results training data\\n\", res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling configuration data\n",
      "build lstm with input_dim: 6\n",
      "Train on 200 samples, validate on 65 samples\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.0743 - mean_squared_error: 0.0743 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "65/65 [==============================] - 0s 843us/step\n",
      "mse:  0.00873059442697\n"
     ]
    }
   ],
   "source": [
    "# experiment with concatenating the config to each data point of learning curve\n",
    "timesteps = 5\n",
    "configs,lcs,Y = t.load_lstm_data_concat_cfg(timesteps=timesteps)\n",
    "model_type = 'lstm'\n",
    "model = m.lstm(lcs[0][0].shape[0])\n",
    "m.train_lstm(model, lcs, Y, split=200, batch_size=20, epochs=5)\n",
    "mse = m.eval_lstm(model, lcs, Y, split=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49393387f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_subset=20\n",
    "t_idx = np.arange(1, n_epochs+1)\n",
    "\n",
    "[plt.plot(t_idx, lc) for lc in learning_curves[:n_subset]]\n",
    "plt.title(\"Subset of learning curves\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over the final error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "[ 0.15153689  0.1515819   0.15282866  0.15434343  0.15650082  0.15954709\n",
      "  0.15959183  0.16055092  0.16080048  0.16156045  0.16244856  0.16430366\n",
      "  0.16470223  0.16500051  0.16504065  0.16676737  0.16724419  0.16746314\n",
      "  0.16788982  0.16836734  0.16912972  0.1696      0.17007007  0.17097436\n",
      "  0.17197581  0.17218749  0.17309483  0.17469136  0.17477876  0.17589213\n",
      "  0.1771176   0.17717566  0.18051375  0.18429064  0.18561671  0.18614719\n",
      "  0.18654372  0.1888912   0.18908462  0.18939709  0.19002079  0.19011057\n",
      "  0.19062407  0.19251503  0.19269777  0.19512195  0.19799197  0.20191532\n",
      "  0.2025641   0.20337261  0.20503018  0.20661408  0.2067056   0.20721804\n",
      "  0.20923359  0.20958447  0.21048843  0.21072796  0.21152115  0.21208654\n",
      "  0.21271271  0.21322007  0.21383838  0.21459538  0.2149739   0.21512623\n",
      "  0.21656115  0.2168614   0.2173462   0.21915322  0.22029654  0.22293893\n",
      "  0.22635376  0.22674772  0.22709083  0.23027569  0.23098019  0.23246412\n",
      "  0.23264447  0.23268473  0.23302277  0.23335352  0.23336735  0.23343434\n",
      "  0.23412699  0.23500927  0.23647416  0.23732251  0.2375452   0.23777065\n",
      "  0.23783784  0.24062501  0.24231975  0.24259704  0.24362348  0.24435318\n",
      "  0.24571427  0.24644309  0.2464986   0.24904099  0.24914332  0.25050813\n",
      "  0.25081633  0.25112107  0.25115116  0.25141243  0.25155154  0.25231388\n",
      "  0.25285347  0.25309199  0.25416666  0.2567131   0.25937046  0.25951522\n",
      "  0.26032892  0.26065163  0.26079499  0.26187094  0.2624217   0.26360544\n",
      "  0.26433381  0.26460584  0.26635748  0.26708823  0.26754297  0.26783325\n",
      "  0.26910763  0.26916869  0.26990796  0.26999184  0.27050505  0.27067669\n",
      "  0.27130417  0.27173478  0.27414801  0.27594143  0.27628738  0.27631045\n",
      "  0.27649817  0.27902911  0.27943868  0.27975207  0.28036345  0.28071056\n",
      "  0.2819192   0.28218219  0.28220612  0.28399035  0.28631791  0.2866592\n",
      "  0.28738856  0.28765182  0.28810564  0.28815629  0.28922669  0.29289517\n",
      "  0.29538055  0.29550102  0.29559559  0.2967033   0.297439    0.29830016\n",
      "  0.30056406  0.30062023  0.30236794  0.3029029   0.30427367  0.30525252\n",
      "  0.30558355  0.30674017  0.30742156  0.30936489  0.31012913  0.31084805\n",
      "  0.31117731  0.31134724  0.3121154   0.31726662  0.31868912  0.31876791\n",
      "  0.31962783  0.32151515  0.32217489  0.32231571  0.32381829  0.32454175\n",
      "  0.32527919  0.32738879  0.32871207  0.32940584  0.33004115  0.33357348\n",
      "  0.33494949  0.34071533  0.34141941  0.34167684  0.34295484  0.34467005\n",
      "  0.34560162  0.35090726  0.35261458  0.35325203  0.35509426  0.35739455\n",
      "  0.35766497  0.36118952  0.36355311  0.36651265  0.36814726  0.37251357\n",
      "  0.37533485  0.37769964  0.37815976  0.37994988  0.38020202  0.38165264\n",
      "  0.39293826  0.39895751  0.39897157  0.40262136  0.40277778  0.40509657\n",
      "  0.41080247  0.41186544  0.41676955  0.42396314  0.43043884  0.43374359\n",
      "  0.43489583  0.4712326   0.47561098  0.47720855  0.48276563  0.48861117\n",
      "  0.49195474  0.50221061  0.51912239  0.52155658  0.53157578  0.53413534\n",
      "  0.53465447  0.56480344  0.58298298  0.63064267  0.6986456   0.89738956\n",
      "  0.89742798  0.8975      0.89770334  0.89776127  0.89795918  0.89835934\n",
      "  0.89857724  0.89873029  0.89878788  0.89882447  0.89885726  0.89964912\n",
      "  0.89978547  0.900398    0.90061919  0.90229302  0.90316206  0.90459863\n",
      "  0.90499599]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49357bc908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493579d5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted = np.sort(learning_curves[:, -1])   # sorted list of final val error\n",
    "print(len(sorted))\n",
    "h = plt.hist(sorted, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(len(sorted))/float(len(sorted))   # from 0 to 1 in 265 even steps\n",
    "plt.plot(sorted, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and CDF over all error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4935a3ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f493581f828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_values = np.sort(learning_curves.flatten())\n",
    "\n",
    "h = plt.hist(all_values, bins=20)\n",
    "plt.show()\n",
    "\n",
    "yvals = np.arange(all_values.shape[0])/all_values.shape[0]\n",
    "plt.plot(all_values, yvals)\n",
    "plt.title(\"Empirical CDF\")\n",
    "plt.xlabel(\"y(x, t=40)\")\n",
    "plt.ylabel(\"CDF(y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
